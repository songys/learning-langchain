{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOISA2I64-xt"
   },
   "source": [
    "# 2ì¥. ë°ì´í„° ì¤€ë¹„ì™€ RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7zQI3ol4-xu",
    "outputId": "a4d0aeea-29d5-44d0-f48e-2f0b38bcb917"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.10)\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-postgres in /usr/local/lib/python3.12/dist-packages (0.0.16)\n",
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.7.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.10 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.12)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.46)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.9)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: asyncpg>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain-postgres) (0.31.0)\n",
      "Requirement already satisfied: pgvector<0.4,>=0.2.5 in /usr/local/lib/python3.12/dist-packages (from langchain-postgres) (0.3.6)\n",
      "Requirement already satisfied: psycopg-pool<4,>=3.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain-postgres) (3.3.0)\n",
      "Requirement already satisfied: psycopg<4,>=3 in /usr/local/lib/python3.12/dist-packages (from psycopg[binary]<4,>=3->langchain-postgres) (3.3.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (26.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.14.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: psycopg-binary==3.3.2 in /usr/local/lib/python3.12/dist-packages (from psycopg[binary]<4,>=3->langchain-postgres) (3.3.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain) (1.12.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.3.25 langchain-community==0.3.25 langchain-core==0.3.83 langchain-text-splitters==0.3.11 langchain-postgres==0.0.16 pypdf beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIS5qPb94-xv"
   },
   "source": [
    "---\n",
    "## ì½”ë“œ 2-1 TextLoaderë¡œ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œí•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIM5YgFv4-xv"
   },
   "source": [
    "# TextLoaderë¡œ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œí•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **TextLoader**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ í…ìŠ¤íŠ¸ íŒŒì¼ì„ LangChain Documentë¡œ ë¡œë“œí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## RAG íŒŒì´í”„ë¼ì¸ì—ì„œì˜ ìœ„ì¹˜\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸ“„ Load    â”‚ â”€â”€â–¶ â”‚  âœ‚ï¸ Split   â”‚ â”€â”€â–¶ â”‚  ğŸ”¢ Embed   â”‚ â”€â”€â–¶ â”‚  ğŸ’¾ Store   â”‚\n",
    "â”‚  (ë¡œë“œ)     â”‚     â”‚  (ë¶„í• )     â”‚     â”‚  (ì„ë² ë”©)    â”‚     â”‚  (ì €ì¥)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "      â†‘\n",
    "   í˜„ì¬ ë‹¨ê³„\n",
    "```\n",
    "\n",
    "## Document Loaderë€?\n",
    "\n",
    "Document LoaderëŠ” ë‹¤ì–‘í•œ ì†ŒìŠ¤(íŒŒì¼, ì›¹, DB ë“±)ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ **LangChain Document ê°ì²´**ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "Document(\n",
    "    page_content='í…ìŠ¤íŠ¸ ë‚´ìš©...',  # ì‹¤ì œ í…ìŠ¤íŠ¸\n",
    "    metadata={'source': 'test.txt'}  # ë©”íƒ€ë°ì´í„°\n",
    ")\n",
    "```\n",
    "\n",
    "## ì£¼ìš” Document Loader ì¢…ë¥˜\n",
    "\n",
    "| Loader | ìš©ë„ |\n",
    "|--------|------|\n",
    "| `TextLoader` | í…ìŠ¤íŠ¸ íŒŒì¼ (.txt) |\n",
    "| `PyPDFLoader` | PDF íŒŒì¼ |\n",
    "| `WebBaseLoader` | ì›¹ í˜ì´ì§€ |\n",
    "| `CSVLoader` | CSV íŒŒì¼ |\n",
    "| `JSONLoader` | JSON íŒŒì¼ |\n",
    "\n",
    "---\n",
    "\n",
    "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiUhqlCv4-xv"
   },
   "source": [
    "# 2. í…ŒìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±\n",
    "\n",
    "ì˜ˆì œë¥¼ ìœ„í•œ ìƒ˜í”Œ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5iIc3nDw4-xv",
    "outputId": "31f9fc8b-31a8-4546-a64c-4f9c28245044"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test.txt íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±\n",
    "sample_text = '''LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì£¼ìš” ê¸°ëŠ¥:\n",
    "1. í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ë° ìµœì í™”\n",
    "2. ì²´ì¸ êµ¬ì„±ì„ í†µí•œ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° ìƒì„±\n",
    "3. ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ì˜ ì—°ë™\n",
    "4. ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ í†µí•œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\n",
    "\n",
    "LangChainì„ ì‚¬ìš©í•˜ë©´ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "'''\n",
    "\n",
    "with open('./test.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(\"test.txt íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TICsMQwH4-xv"
   },
   "source": [
    "# 3. TextLoaderë¡œ íŒŒì¼ ë¡œë“œ\n",
    "\n",
    "**ì½”ë“œ ì„¤ëª…:**\n",
    "\n",
    "```python\n",
    "loader = TextLoader('./test.txt', encoding='utf-8')\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "- `TextLoader(íŒŒì¼ê²½ë¡œ, encoding)` - ë¡œë” ìƒì„±\n",
    "- `loader.load()` - íŒŒì¼ì„ ì½ì–´ Document ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "- `encoding='utf-8'` - í•œê¸€ íŒŒì¼ì˜ ê²½ìš° ì¸ì½”ë”© ì§€ì • í•„ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pFA44QT4-xv",
    "outputId": "a53f099a-7add-48c8-a51d-4b343147497e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: 1\n",
      "íƒ€ì…: Document\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# TextLoaderë¡œ íŒŒì¼ ë¡œë“œ\n",
    "loader = TextLoader('./test.txt', encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(docs)}\")\n",
    "print(f\"íƒ€ì…: {type(docs[0]).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Yi6CEQ4-xv"
   },
   "source": [
    "# 4. Document ê°ì²´ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WS1r6Pdf4-xw",
    "outputId": "80ec7350-4825-4cb2-cf09-e841bcfd8c33"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== page_content ===\n",
      "LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì£¼ìš” ê¸°ëŠ¥:\n",
      "1. í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ë° ìµœì í™”\n",
      "2. ì²´ì¸ êµ¬ì„±ì„ í†µí•œ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° ìƒì„±\n",
      "3. ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ì˜ ì—°ë™\n",
      "4. ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ í†µí•œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\n",
      "\n",
      "LangChainì„ ì‚¬ìš©í•˜ë©´ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "\n",
      "=== metadata ===\n",
      "{'source': './test.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Document ë‚´ìš© í™•ì¸\n",
    "doc = docs[0]\n",
    "\n",
    "print(\"=== page_content ===\")\n",
    "print(doc.page_content)\n",
    "\n",
    "print(\"\\n=== metadata ===\")\n",
    "print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-1KoMfq4-xw"
   },
   "source": [
    "---\n",
    "\n",
    "## Document ê°ì²´ êµ¬ì¡°\n",
    "\n",
    "| ì†ì„± | ì„¤ëª… |\n",
    "|------|------|\n",
    "| `page_content` | ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš© |\n",
    "| `metadata` | ì¶œì²˜, í˜ì´ì§€ ë²ˆí˜¸ ë“± ë¶€ê°€ ì •ë³´ |\n",
    "\n",
    "## ì—¬ëŸ¬ íŒŒì¼ ë¡œë“œí•˜ê¸°\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  txt íŒŒì¼ ë¡œë“œ\n",
    "loader = DirectoryLoader('./data/', glob='**/*.txt', loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ë¡œë“œí•œ ë¬¸ì„œëŠ” ë³´í†µ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. **Text Splitter**ë¡œ ì²­í¬ ë¶„í•  (04ë²ˆ ë…¸íŠ¸ë¶)\n",
    "2. **Embeddings**ë¡œ ë²¡í„° ë³€í™˜ (07ë²ˆ ë…¸íŠ¸ë¶)\n",
    "3. **Vector Store**ì— ì €ì¥ (09ë²ˆ ë…¸íŠ¸ë¶)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri6muJvs4-xw"
   },
   "source": [
    "---\n",
    "## ì½”ë“œ 2-2 WebBaseLoaderë¡œ ì›¹ í˜ì´ì§€ ë¡œë“œí•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Xio784Q4-xw"
   },
   "source": [
    "# WebBaseLoaderë¡œ ì›¹ í˜ì´ì§€ ë¡œë“œí•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **WebBaseLoader**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ í˜ì´ì§€ì˜ ë‚´ìš©ì„ LangChain Documentë¡œ ë¡œë“œí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## WebBaseLoaderë€?\n",
    "\n",
    "ì›¹ URLì—ì„œ HTMLì„ ê°€ì ¸ì™€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” Document Loaderì…ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "loader = WebBaseLoader('https://example.com')\n",
    "docs = loader.load()  # ì›¹ í˜ì´ì§€ â†’ Document\n",
    "```\n",
    "\n",
    "## íŠ¹ì§•\n",
    "\n",
    "- **BeautifulSoup** ê¸°ë°˜ HTML íŒŒì‹±\n",
    "- JavaScript ë Œë”ë§ì´ í•„ìš” ì—†ëŠ” ì •ì  í˜ì´ì§€ì— ì í•©\n",
    "- ì—¬ëŸ¬ URLì„ í•œ ë²ˆì— ë¡œë“œ ê°€ëŠ¥\n",
    "\n",
    "## ë‹¤ë¥¸ ì›¹ ë¡œë”ë“¤\n",
    "\n",
    "| Loader | íŠ¹ì§• |\n",
    "|--------|------|\n",
    "| `WebBaseLoader` | ê¸°ë³¸ HTML íŒŒì‹± |\n",
    "| `SeleniumURLLoader` | JavaScript ë Œë”ë§ ì§€ì› |\n",
    "| `PlaywrightURLLoader` | ë¸Œë¼ìš°ì € ìë™í™” |\n",
    "| `UnstructuredURLLoader` | êµ¬ì¡°í™”ëœ ì¶”ì¶œ |\n",
    "\n",
    "---\n",
    "\n",
    "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itwxUrfP4-xw"
   },
   "source": [
    "# 2. WebBaseLoaderë¡œ ì›¹ í˜ì´ì§€ ë¡œë“œ\n",
    "\n",
    "**ì½”ë“œ ì„¤ëª…:**\n",
    "\n",
    "```python\n",
    "loader = WebBaseLoader('https://www.langchain.com/')\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "- URLì„ ì§€ì •í•˜ì—¬ ë¡œë” ìƒì„±\n",
    "- `load()` í˜¸ì¶œ ì‹œ ì›¹ í˜ì´ì§€ë¥¼ ê°€ì ¸ì™€ í…ìŠ¤íŠ¸ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IC09u1G74-xw",
    "outputId": "65d83f3f-c8ec-459d-f79c-74473f549c90"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# ì›¹ í˜ì´ì§€ ë¡œë“œ\n",
    "loader = WebBaseLoader('https://www.langchain.com/')\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgCu1abH4-xw"
   },
   "source": [
    "# 3. Document ë‚´ìš© í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tb2Q8Yaz4-xw",
    "outputId": "c80b911d-21cf-4599-9e76-5ddb8ab5e1d3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== metadata ===\n",
      "{'source': 'https://www.langchain.com/', 'title': 'LangChain: Observe, Evaluate, and Deploy Reliable AI Agents', 'description': 'LangChain provides the engineering platform and open source frameworks developers use to build, test, and deploy reliable AI agents.', 'language': 'en'}\n",
      "\n",
      "=== page_content (ì²˜ìŒ 500ì) ===\n",
      "LangChain: Observe, Evaluate, and Deploy Reliable AI Agents\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Products\n",
      "\n",
      "LangSmithObservabilityDebug and monitor in-depth tracesEvaluationIterate on prompts and modelsDeploymentShip and scale agents in productionAgent BuilderNewBuild no-code agentsOpen Source FrameworksLangChainQuick start agents with any model providerLangGraphBuild custom agents with low-level controlDeep AgentsNewUse planning, memory, and sub-agents for complex, long-running tasksLearn\n",
      "\n",
      "ResourcesBlog2026 S\n"
     ]
    }
   ],
   "source": [
    "doc = docs[0]\n",
    "\n",
    "print(\"=== metadata ===\")\n",
    "print(doc.metadata)\n",
    "\n",
    "print(\"\\n=== page_content (ì²˜ìŒ 500ì) ===\")\n",
    "print(doc.page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4V-LoFCN4-xw"
   },
   "source": [
    "# 4. ì—¬ëŸ¬ URL ë™ì‹œ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vHdvtcm4-xw",
    "outputId": "b0ed7ae9-fb7b-4abc-e305-8b96ca683426"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: 2\n",
      "\n",
      "[1] https://python.langchain.com/docs/introduction/\n",
      "    ê¸¸ì´: 4171ì\n",
      "\n",
      "[2] https://python.langchain.com/docs/tutorials/\n",
      "    ê¸¸ì´: 4171ì\n"
     ]
    }
   ],
   "source": [
    "# ì—¬ëŸ¬ URL ë¡œë“œ\n",
    "urls = [\n",
    "    'https://python.langchain.com/docs/introduction/',\n",
    "    'https://python.langchain.com/docs/tutorials/',\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(urls)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(docs)}\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n[{i+1}] {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"    ê¸¸ì´: {len(doc.page_content)}ì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__GJexpH4-xw"
   },
   "source": [
    "---\n",
    "\n",
    "## ê³ ê¸‰ ì˜µì…˜\n",
    "\n",
    "### íŠ¹ì • ìš”ì†Œë§Œ ì¶”ì¶œ (BeautifulSoup ì„¤ì •)\n",
    "\n",
    "```python\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=['https://example.com'],\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=['article-content', 'main-text']  # íŠ¹ì • í´ë˜ìŠ¤ë§Œ ì¶”ì¶œ\n",
    "        )\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "### í—¤ë” ì„¤ì •\n",
    "\n",
    "```python\n",
    "loader = WebBaseLoader(\n",
    "    'https://example.com',\n",
    "    header_template={'User-Agent': 'MyBot/1.0'}\n",
    ")\n",
    "```\n",
    "\n",
    "## ì£¼ì˜ì‚¬í•­\n",
    "\n",
    "1. **JavaScript ë Œë”ë§**: WebBaseLoaderëŠ” ì •ì  HTMLë§Œ ì²˜ë¦¬. ë™ì  ì½˜í…ì¸ ëŠ” Selenium/Playwright ì‚¬ìš©\n",
    "2. **robots.txt ì¤€ìˆ˜**: ì›¹ í¬ë¡¤ë§ ì‹œ ì‚¬ì´íŠ¸ ì •ì±… í™•ì¸\n",
    "3. **ìš”ì²­ ì œí•œ**: ëŒ€ëŸ‰ í¬ë¡¤ë§ ì‹œ ë”œë ˆì´ ì¶”ê°€ ê¶Œì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykjh-Mdr4-xw"
   },
   "source": [
    "---\n",
    "## ì½”ë“œ 2-3 PyPDFLoaderë¡œ PDF íŒŒì¼ ë¡œë“œí•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip3oGFEZ4-xw"
   },
   "source": [
    "# PyPDFLoaderë¡œ PDF íŒŒì¼ ë¡œë“œí•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **PyPDFLoader**ë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ì„ LangChain Documentë¡œ ë¡œë“œí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## PyPDFLoaderë€?\n",
    "\n",
    "PDF íŒŒì¼ì„ **í˜ì´ì§€ ë‹¨ìœ„**ë¡œ ì½ì–´ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ë¡œë”ì…ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "loader = PyPDFLoader('./document.pdf')\n",
    "pages = loader.load()  # í˜ì´ì§€ë³„ Document ë¦¬ìŠ¤íŠ¸\n",
    "```\n",
    "\n",
    "## PDF Loader ì¢…ë¥˜\n",
    "\n",
    "| Loader | íŠ¹ì§• |\n",
    "|--------|------|\n",
    "| `PyPDFLoader` | ê¸°ë³¸ PDF ë¡œë”, í˜ì´ì§€ë³„ ë¶„ë¦¬ |\n",
    "| `PyMuPDFLoader` | ë¹ ë¥¸ ì²˜ë¦¬, ì´ë¯¸ì§€ ì¶”ì¶œ ì§€ì› |\n",
    "| `PDFPlumberLoader` | í…Œì´ë¸” ì¶”ì¶œì— ê°•í•¨ |\n",
    "| `UnstructuredPDFLoader` | êµ¬ì¡°í™”ëœ ì¶”ì¶œ |\n",
    "\n",
    "---\n",
    "\n",
    "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsCUNjd_4-xw"
   },
   "source": [
    "# 2. í…ŒìŠ¤íŠ¸ìš© PDF íŒŒì¼ ìƒì„±\n",
    "\n",
    "ì˜ˆì œë¥¼ ìœ„í•´ ê°„ë‹¨í•œ PDF íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQh6l0uI4-xw",
    "outputId": "dc51a05d-a778-4a62-a79a-ec6a16286a3a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test.pdf íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. (2í˜ì´ì§€)\n"
     ]
    }
   ],
   "source": [
    "# reportlabìœ¼ë¡œ í…ŒìŠ¤íŠ¸ PDF ìƒì„±\n",
    "!pip install -q reportlab\n",
    "\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "# PDF ìƒì„±\n",
    "c = canvas.Canvas('./test.pdf', pagesize=letter)\n",
    "\n",
    "# í˜ì´ì§€ 1\n",
    "c.drawString(100, 750, 'LangChain Tutorial - Page 1')\n",
    "c.drawString(100, 700, 'LangChain is a framework for developing applications')\n",
    "c.drawString(100, 680, 'powered by large language models (LLMs).')\n",
    "c.showPage()\n",
    "\n",
    "# í˜ì´ì§€ 2\n",
    "c.drawString(100, 750, 'RAG System - Page 2')\n",
    "c.drawString(100, 700, 'RAG stands for Retrieval-Augmented Generation.')\n",
    "c.drawString(100, 680, 'It combines retrieval and generation for better answers.')\n",
    "c.showPage()\n",
    "\n",
    "c.save()\n",
    "print('test.pdf íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. (2í˜ì´ì§€)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIiUj5Vg4-xw"
   },
   "source": [
    "# 3. PyPDFLoaderë¡œ PDF ë¡œë“œ\n",
    "\n",
    "**ì½”ë“œ ì„¤ëª…:**\n",
    "\n",
    "```python\n",
    "loader = PyPDFLoader('./test.pdf')\n",
    "pages = loader.load()\n",
    "```\n",
    "\n",
    "- ê° í˜ì´ì§€ê°€ ë³„ë„ì˜ Documentë¡œ ë°˜í™˜ë¨\n",
    "- `metadata`ì— í˜ì´ì§€ ë²ˆí˜¸(`page`) í¬í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6l80t8l4-xw",
    "outputId": "9aa1a609-b5a3-4789-d06e-fab814a0914b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ë¡œë“œëœ í˜ì´ì§€ ìˆ˜: 2\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF ë¡œë“œ\n",
    "loader = PyPDFLoader('./test.pdf')\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"ë¡œë“œëœ í˜ì´ì§€ ìˆ˜: {len(pages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNBFndcQ4-xw"
   },
   "source": [
    "# 4. í˜ì´ì§€ë³„ ë‚´ìš© í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WejQvmBO4-xw",
    "outputId": "afbcb0af-2e78-4ab0-8c10-ab25ee20cced"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== í˜ì´ì§€ 1 ===\n",
      "metadata: {'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2026-02-13T13:14:41+00:00', 'author': 'anonymous', 'keywords': '', 'moddate': '2026-02-13T13:14:41+00:00', 'subject': 'unspecified', 'title': 'untitled', 'trapped': '/False', 'source': './test.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}\n",
      "content: LangChain Tutorial - Page 1\n",
      "LangChain is a framework for developing applications\n",
      "powered by large language models (LLMs).\n",
      "\n",
      "=== í˜ì´ì§€ 2 ===\n",
      "metadata: {'producer': 'ReportLab PDF Library - (opensource)', 'creator': 'anonymous', 'creationdate': '2026-02-13T13:14:41+00:00', 'author': 'anonymous', 'keywords': '', 'moddate': '2026-02-13T13:14:41+00:00', 'subject': 'unspecified', 'title': 'untitled', 'trapped': '/False', 'source': './test.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}\n",
      "content: RAG System - Page 2\n",
      "RAG stands for Retrieval-Augmented Generation.\n",
      "It combines retrieval and generation for better answers.\n"
     ]
    }
   ],
   "source": [
    "# ê° í˜ì´ì§€ ë‚´ìš© í™•ì¸\n",
    "for i, page in enumerate(pages):\n",
    "    print(f\"\\n=== í˜ì´ì§€ {i+1} ===\")\n",
    "    print(f\"metadata: {page.metadata}\")\n",
    "    print(f\"content: {page.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7cyj5MR4-xw"
   },
   "source": [
    "---\n",
    "\n",
    "## ê³ ê¸‰ ì‚¬ìš©ë²•\n",
    "\n",
    "### íŠ¹ì • í˜ì´ì§€ë§Œ ë¡œë“œ\n",
    "\n",
    "```python\n",
    "# load_and_splitìœ¼ë¡œ íŠ¹ì • í˜ì´ì§€ ì„ íƒ ê°€ëŠ¥\n",
    "pages = loader.load()\n",
    "selected_pages = [pages[0], pages[2]]  # 1, 3í˜ì´ì§€ë§Œ\n",
    "```\n",
    "\n",
    "### í…ìŠ¤íŠ¸ ì¶”ì¶œ ëª¨ë“œ\n",
    "\n",
    "```python\n",
    "loader = PyPDFLoader('./test.pdf', extract_images=True)  # ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "```\n",
    "\n",
    "## ë‹¤ë¥¸ PDF ë¡œë” ì˜ˆì‹œ\n",
    "\n",
    "### PyMuPDFLoader (ë” ë¹ ë¦„)\n",
    "```python\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader('./test.pdf')\n",
    "pages = loader.load()\n",
    "```\n",
    "\n",
    "### PDFPlumberLoader (í…Œì´ë¸” ì¶”ì¶œ)\n",
    "```python\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "loader = PDFPlumberLoader('./test.pdf')\n",
    "pages = loader.load()\n",
    "```\n",
    "\n",
    "## ì£¼ì˜ì‚¬í•­\n",
    "\n",
    "1. **ìŠ¤ìº” PDF**: OCRì´ í•„ìš”í•œ ê²½ìš° `UnstructuredPDFLoader` + Tesseract ì‚¬ìš©\n",
    "2. **ì•”í˜¸í™” PDF**: ë¹„ë°€ë²ˆí˜¸ ì˜µì…˜ í•„ìš”\n",
    "3. **ëŒ€ìš©ëŸ‰ PDF**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì£¼ì˜, lazy_load() ê³ ë ¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvyQrj0H4-xw"
   },
   "source": [
    "---\n",
    "## ì½”ë“œ 2-4 RecursiveCharacterTextSplitterë¡œ ë¬¸ì„œ ë¶„í• í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF3P_JJz4-xw"
   },
   "source": [
    "# RecursiveCharacterTextSplitterë¡œ ë¬¸ì„œ ë¶„í• í•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **RecursiveCharacterTextSplitter**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ì ì ˆí•œ í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## RAG íŒŒì´í”„ë¼ì¸ì—ì„œì˜ ìœ„ì¹˜\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸ“„ Load    â”‚ â”€â”€â–¶ â”‚  âœ‚ï¸ Split   â”‚ â”€â”€â–¶ â”‚  ğŸ”¢ Embed   â”‚ â”€â”€â–¶ â”‚  ğŸ’¾ Store   â”‚\n",
    "â”‚  (ë¡œë“œ)     â”‚     â”‚  (ë¶„í• )     â”‚     â”‚  (ì„ë² ë”©)    â”‚     â”‚  (ì €ì¥)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â†‘\n",
    "                       í˜„ì¬ ë‹¨ê³„\n",
    "```\n",
    "\n",
    "## ì™œ ë¬¸ì„œë¥¼ ë¶„í• í•´ì•¼ í• ê¹Œ?\n",
    "\n",
    "1. **LLM ì»¨í…ìŠ¤íŠ¸ ì œí•œ**: ëª¨ë¸ì˜ ìµœëŒ€ í† í° ìˆ˜ ì œí•œ\n",
    "2. **ê²€ìƒ‰ ì •í™•ë„**: ì‘ì€ ì²­í¬ê°€ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼\n",
    "3. **ë¹„ìš© íš¨ìœ¨**: í•„ìš”í•œ ë¶€ë¶„ë§Œ LLMì— ì „ë‹¬\n",
    "\n",
    "## RecursiveCharacterTextSplitterë€?\n",
    "\n",
    "**ì¬ê·€ì **ìœ¼ë¡œ ì—¬ëŸ¬ êµ¬ë¶„ìë¥¼ ì‹œë„í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê¸°ë³¸ êµ¬ë¶„ì ìˆœì„œ:\n",
    "1. `\\n\\n` (ë‹¨ë½)\n",
    "2. `\\n` (ì¤„ë°”ê¿ˆ)\n",
    "3. ` ` (ê³µë°±)\n",
    "4. `` (ë¬¸ì)\n",
    "\n",
    "---\n",
    "\n",
    "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J17EMTks4-xx"
   },
   "source": [
    "# 2. í…ŒìŠ¤íŠ¸ íŒŒì¼ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUwgfR164-xx",
    "outputId": "b1553167-d931-43a8-a970-f48b65eb5d70"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test.txt ìƒì„± ì™„ë£Œ (ì´ 357ì)\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±\n",
    "sample_text = '''LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì£¼ìš” ê¸°ëŠ¥:\n",
    "1. í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ë° ìµœì í™”\n",
    "2. ì²´ì¸ êµ¬ì„±ì„ í†µí•œ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° ìƒì„±\n",
    "3. ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ì˜ ì—°ë™\n",
    "4. ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ í†µí•œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\n",
    "\n",
    "LangChainì„ ì‚¬ìš©í•˜ë©´ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. RAGëŠ” ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ì‘ë‹µì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "\n",
    "Vector StoreëŠ” ì„ë² ë”©ëœ ë¬¸ì„œë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ Chroma, Pinecone, Weaviate ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "'''\n",
    "\n",
    "with open('./test.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(f\"test.txt ìƒì„± ì™„ë£Œ (ì´ {len(sample_text)}ì)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0SsECtC4-xx"
   },
   "source": [
    "# 3. ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• \n",
    "\n",
    "**í•µì‹¬ íŒŒë¼ë¯¸í„°:**\n",
    "\n",
    "```python\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,    # ì²­í¬ ìµœëŒ€ í¬ê¸° (ë¬¸ì ìˆ˜)\n",
    "    chunk_overlap=200   # ì²­í¬ ê°„ ì¤‘ë³µ (ë¬¸ë§¥ ìœ ì§€)\n",
    ")\n",
    "```\n",
    "\n",
    "| íŒŒë¼ë¯¸í„° | ì„¤ëª… |\n",
    "|----------|------|\n",
    "| `chunk_size` | ê° ì²­í¬ì˜ ìµœëŒ€ ë¬¸ì ìˆ˜ |\n",
    "| `chunk_overlap` | ì¸ì ‘ ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¬¸ì ìˆ˜ (ë¬¸ë§¥ ìœ ì§€ìš©) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R67b8Yb64-xx",
    "outputId": "23099c74-78ea-40af-91a3-ce3fc73ef67f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ì›ë³¸ ë¬¸ì„œ ìˆ˜: 1\n",
      "ì›ë³¸ ë¬¸ì„œ ê¸¸ì´: 357ì\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. ë¬¸ì„œ ë¡œë“œ\n",
    "loader = TextLoader('./test.txt', encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(docs)}\")\n",
    "print(f\"ì›ë³¸ ë¬¸ì„œ ê¸¸ì´: {len(docs[0].page_content)}ì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpoYgGfr4-xx",
    "outputId": "d794b91e-9596-457d-de6e-a40aa6af1076"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: 3\n"
     ]
    }
   ],
   "source": [
    "# 2. Text Splitter ìƒì„±\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,     # ì˜ˆì‹œë¥¼ ìœ„í•´ ì‘ì€ ê°’ ì‚¬ìš©\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# 3. ë¬¸ì„œ ë¶„í• \n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(splitted_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHxrKV3i4-xx"
   },
   "source": [
    "# 4. ë¶„í•  ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-_iZBGQ4-xx",
    "outputId": "3928c0d6-17ff-46be-ea47-c73bd8dc8faf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== ì²­í¬ 1 (149ì) ===\n",
      "LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì£¼ìš” ê¸°ëŠ¥:\n",
      "1. í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ë° ìµœì í™”\n",
      "2. ì²´ì¸ êµ¬ì„±ì„ í†µí•œ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° ìƒì„±\n",
      "3. ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ì˜ ì—°ë™\n",
      "4. ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ í†µí•œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\n",
      "metadata: {'source': './test.txt'}\n",
      "\n",
      "=== ì²­í¬ 2 (110ì) ===\n",
      "LangChainì„ ì‚¬ìš©í•˜ë©´ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. RAGëŠ” ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ì‘ë‹µì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
      "metadata: {'source': './test.txt'}\n",
      "\n",
      "=== ì²­í¬ 3 (93ì) ===\n",
      "Vector StoreëŠ” ì„ë² ë”©ëœ ë¬¸ì„œë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ Chroma, Pinecone, Weaviate ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "metadata: {'source': './test.txt'}\n"
     ]
    }
   ],
   "source": [
    "# ê° ì²­í¬ í™•ì¸\n",
    "for i, doc in enumerate(splitted_docs):\n",
    "    print(f\"\\n=== ì²­í¬ {i+1} ({len(doc.page_content)}ì) ===\")\n",
    "    print(doc.page_content)\n",
    "    print(f\"metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z07c8u_U4-xz"
   },
   "source": [
    "---\n",
    "\n",
    "## chunk_overlapì˜ ì—­í• \n",
    "\n",
    "```\n",
    "ì²­í¬ 1: [    í…ìŠ¤íŠ¸ A    ][ì¤‘ë³µ]\n",
    "ì²­í¬ 2:              [ì¤‘ë³µ][    í…ìŠ¤íŠ¸ B    ]\n",
    "```\n",
    "\n",
    "- ë¬¸ë§¥ì´ ì²­í¬ ê²½ê³„ì—ì„œ ëŠê¸°ëŠ” ê²ƒì„ ë°©ì§€\n",
    "- ê²€ìƒ‰ ì‹œ ê´€ë ¨ ì •ë³´ê°€ ëˆ„ë½ë˜ì§€ ì•Šë„ë¡ ë³´ì¥\n",
    "\n",
    "## ì ì ˆí•œ chunk_size ì„ íƒ\n",
    "\n",
    "| chunk_size | ì¥ì  | ë‹¨ì  |\n",
    "|------------|------|------|\n",
    "| ì‘ìŒ (200-500) | ì •í™•í•œ ê²€ìƒ‰ | ë¬¸ë§¥ ì†ì‹¤ ê°€ëŠ¥ |\n",
    "| ì¤‘ê°„ (500-1000) | ê· í˜• ì¡í˜ | - |\n",
    "| í¼ (1000-2000) | í’ë¶€í•œ ë¬¸ë§¥ | ê²€ìƒ‰ ì •í™•ë„ ì €í•˜ |\n",
    "\n",
    "## ë‹¤ë¥¸ Text Splitter\n",
    "\n",
    "```python\n",
    "# í† í° ê¸°ë°˜ ë¶„í• \n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# ë¬¸ì¥ ê¸°ë°˜ ë¶„í• \n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyov7f_K4-xz"
   },
   "source": [
    "---\n",
    "## ì½”ë“œ 2-5 í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë³„ ì½”ë“œ ë¶„í• í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PO6O6v9Y4-xz"
   },
   "source": [
    "# í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë³„ ì½”ë“œ ë¶„í• í•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **RecursiveCharacterTextSplitter.from_language()**ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œê·¸ë˜ë° ì½”ë“œë¥¼ ì–¸ì–´ë³„ êµ¬ë¬¸ì— ë§ê²Œ ë¶„í• í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## ì½”ë“œ ë¶„í• ì´ íŠ¹ë³„í•œ ì´ìœ \n",
    "\n",
    "ì¼ë°˜ í…ìŠ¤íŠ¸ì™€ ë‹¬ë¦¬ ì½”ë“œëŠ” **ë¬¸ë²•ì  êµ¬ì¡°**ê°€ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "- í•¨ìˆ˜/í´ë˜ìŠ¤ ë‹¨ìœ„\n",
    "- ë“¤ì—¬ì“°ê¸° ë¸”ë¡\n",
    "- ì£¼ì„\n",
    "\n",
    "`from_language()`ëŠ” ì–¸ì–´ë³„ êµ¬ë¶„ìë¥¼ ìë™ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì§€ì› ì–¸ì–´\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import Language\n",
    "\n",
    "# ì£¼ìš” ì§€ì› ì–¸ì–´\n",
    "Language.PYTHON\n",
    "Language.JAVASCRIPT\n",
    "Language.TYPESCRIPT\n",
    "Language.JAVA\n",
    "Language.GO\n",
    "Language.RUST\n",
    "Language.MARKDOWN\n",
    "Language.HTML\n",
    "# ... ë“± 20+ ì–¸ì–´ ì§€ì›\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAOi-2gs4-x0"
   },
   "source": [
    "# 2. Python ì½”ë“œ ë¶„í• \n",
    "\n",
    "**ì½”ë“œ ì„¤ëª…:**\n",
    "\n",
    "```python\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,  # Python êµ¬ë¬¸ ê·œì¹™ ì‚¬ìš©\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "```\n",
    "\n",
    "Pythonì˜ ê¸°ë³¸ êµ¬ë¶„ì:\n",
    "1. í´ë˜ìŠ¤ ì •ì˜ (`class`)\n",
    "2. í•¨ìˆ˜ ì •ì˜ (`def`)\n",
    "3. ì œì–´ë¬¸ (`if`, `for`, `while` ë“±)\n",
    "4. ì¤„ë°”ê¿ˆ, ê³µë°±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gFaBvOsF4-x0",
    "outputId": "3f5fce03-0373-4973-d59a-69c35732c111"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== ì›ë³¸ ì½”ë“œ ===\n",
      "\n",
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "\n",
      "# Call the function\n",
      "hello_world()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "# ìƒ˜í”Œ Python ì½”ë“œ\n",
    "PYTHON_CODE = '''\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "'''\n",
    "\n",
    "print(\"=== ì›ë³¸ ì½”ë“œ ===\")\n",
    "print(PYTHON_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rl2C3644-x0",
    "outputId": "4919086f-a358-4106-b534-0f41762d2de1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ë¶„í• ëœ ì²­í¬ ìˆ˜: 2\n",
      "\n",
      "--- ì²­í¬ 1 ---\n",
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n",
      "\n",
      "--- ì²­í¬ 2 ---\n",
      "# Call the function\n",
      "hello_world()\n"
     ]
    }
   ],
   "source": [
    "# Python ì „ìš© Splitter ìƒì„±\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# ë¬¸ì„œ ìƒì„± ë° ë¶„í• \n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "\n",
    "print(f\"\\në¶„í• ëœ ì²­í¬ ìˆ˜: {len(python_docs)}\")\n",
    "for i, doc in enumerate(python_docs):\n",
    "    print(f\"\\n--- ì²­í¬ {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5OqLie44-x0"
   },
   "source": [
    "# 3. JavaScript ì½”ë“œ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTHioQVy4-x0",
    "outputId": "b8022607-a108-47ce-d565-f86d377b2728"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ë¶„í• ëœ ì²­í¬ ìˆ˜: 3\n",
      "\n",
      "--- ì²­í¬ 1 ---\n",
      "function greet(name) {\n",
      "    console.log(`Hello, ${name}!`);\n",
      "\n",
      "--- ì²­í¬ 2 ---\n",
      "}\n",
      "\n",
      "--- ì²­í¬ 3 ---\n",
      "const add = (a, b) => a + b;\n",
      "\n",
      "greet(\"World\");\n"
     ]
    }
   ],
   "source": [
    "JS_CODE = '''\n",
    "function greet(name) {\n",
    "    console.log(`Hello, ${name}!`);\n",
    "}\n",
    "\n",
    "const add = (a, b) => a + b;\n",
    "\n",
    "greet(\"World\");\n",
    "'''\n",
    "\n",
    "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS,\n",
    "    chunk_size=60,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "js_docs = js_splitter.create_documents([JS_CODE])\n",
    "\n",
    "print(f\"ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(js_docs)}\")\n",
    "for i, doc in enumerate(js_docs):\n",
    "    print(f\"\\n--- ì²­í¬ {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpEZplVi4-x0"
   },
   "source": [
    "# 4. ì–¸ì–´ë³„ êµ¬ë¶„ì í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ez-RVab4-x0",
    "outputId": "cf91bad7-bb41-490b-dba9-54070a95f6fa"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Python êµ¬ë¶„ì ===\n",
      "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']\n",
      "\n",
      "=== JavaScript êµ¬ë¶„ì ===\n",
      "['\\nfunction ', '\\nconst ', '\\nlet ', '\\nvar ', '\\nclass ', '\\nif ', '\\nfor ', '\\nwhile ', '\\nswitch ', '\\ncase ', '\\ndefault ', '\\n\\n', '\\n', ' ', '']\n",
      "\n",
      "=== Markdown êµ¬ë¶„ì ===\n",
      "['\\n#{1,6} ', '```\\n', '\\n\\\\*\\\\*\\\\*+\\n', '\\n---+\\n', '\\n___+\\n', '\\n\\n', '\\n', ' ', '']\n"
     ]
    }
   ],
   "source": [
    "# ê° ì–¸ì–´ì˜ êµ¬ë¶„ì í™•ì¸\n",
    "print(\"=== Python êµ¬ë¶„ì ===\")\n",
    "print(RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON))\n",
    "\n",
    "print(\"\\n=== JavaScript êµ¬ë¶„ì ===\")\n",
    "print(RecursiveCharacterTextSplitter.get_separators_for_language(Language.JS))\n",
    "\n",
    "print(\"\\n=== Markdown êµ¬ë¶„ì ===\")\n",
    "print(RecursiveCharacterTextSplitter.get_separators_for_language(Language.MARKDOWN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6_nSho74-x0"
   },
   "source": [
    "---\n",
    "\n",
    "## ì§€ì› ì–¸ì–´ ëª©ë¡\n",
    "\n",
    "```python\n",
    "# ì „ì²´ ì§€ì› ì–¸ì–´ í™•ì¸\n",
    "print([e.value for e in Language])\n",
    "```\n",
    "\n",
    "ì£¼ìš” ì–¸ì–´:\n",
    "- `Language.PYTHON`\n",
    "- `Language.JS` / `Language.TYPESCRIPT`\n",
    "- `Language.JAVA` / `Language.KOTLIN`\n",
    "- `Language.GO` / `Language.RUST`\n",
    "- `Language.C` / `Language.CPP`\n",
    "- `Language.MARKDOWN` / `Language.HTML`\n",
    "\n",
    "## í™œìš© ì‚¬ë¡€\n",
    "\n",
    "1. **ì½”ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ**: í•¨ìˆ˜/í´ë˜ìŠ¤ ë‹¨ìœ„ë¡œ ê²€ìƒ‰\n",
    "2. **ì½”ë“œ ë¦¬ë·° ë„ìš°ë¯¸**: ê´€ë ¨ ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ\n",
    "3. **ë¬¸ì„œí™” ìë™í™”**: ì½”ë“œì™€ ì£¼ì„ ì—°ê²°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JelC8uy04-x0"
   },
   "source": [
    "---\n",
    "## ì½”ë“œ 2-6 Markdown ë¬¸ì„œ ë¶„í• í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bp_IUfM4-x0"
   },
   "source": [
    "# Markdown ë¬¸ì„œ ë¶„í• í•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **RecursiveCharacterTextSplitter.from_language()**ë¡œ Markdown ë¬¸ì„œë¥¼ êµ¬ì¡°ì— ë§ê²Œ ë¶„í• í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## Markdown ë¶„í•  íŠ¹ì§•\n",
    "\n",
    "Markdownì€ í—¤ë”(`#`, `##`) ê¸°ë°˜ êµ¬ì¡°ë¥¼ ê°€ì§€ë¯€ë¡œ, ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ë©´ ì˜ë¯¸ ìˆëŠ” ì„¹ì…˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "**Markdown êµ¬ë¶„ì ìˆœì„œ:**\n",
    "1. `\\n# ` (H1 í—¤ë”)\n",
    "2. `\\n## ` (H2 í—¤ë”)\n",
    "3. `\\n### ` (H3 í—¤ë”)\n",
    "4. `\\n#### ` (H4 í—¤ë”)\n",
    "5. `\\n- ` (ë¦¬ìŠ¤íŠ¸)\n",
    "6. ` ``` ` (ì½”ë“œ ë¸”ë¡)\n",
    "7. `\\n\\n`, `\\n`, ` `\n",
    "\n",
    "---\n",
    "\n",
    "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8swfTun4-x0"
   },
   "source": [
    "# 2. Markdown ë¬¸ì„œ ë¶„í• \n",
    "\n",
    "**ì½”ë“œ ì„¤ëª…:**\n",
    "\n",
    "```python\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,\n",
    "    chunk_size=60,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# metadata ì¶”ê°€ ê°€ëŠ¥\n",
    "md_docs = md_splitter.create_documents(\n",
    "    [markdown_text],\n",
    "    [{'source': 'https://...'}]  # ê° ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YMl3o9bz4-x0",
    "outputId": "d3ff22db-7159-4da3-e542-58c866742d8b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== ì›ë³¸ Markdown ===\n",
      "\n",
      "# ğŸ¦œğŸ”— LangChain âš¡ Building applications with LLMs through composability âš¡\n",
      "\n",
      "## Quick Install\n",
      "```bash\n",
      "pip install langchain\n",
      "```\n",
      "\n",
      "As an open source project in a rapidly developing field, we are extremely open\n",
      "    to contributions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "# ìƒ˜í”Œ Markdown í…ìŠ¤íŠ¸\n",
    "markdown_text = '''\n",
    "# ğŸ¦œğŸ”— LangChain âš¡ Building applications with LLMs through composability âš¡\n",
    "\n",
    "## Quick Install\n",
    "```bash\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "As an open source project in a rapidly developing field, we are extremely open\n",
    "    to contributions.\n",
    "'''\n",
    "\n",
    "print(\"=== ì›ë³¸ Markdown ===\")\n",
    "print(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNx92mEf4-x0",
    "outputId": "e1087f9f-3b4d-4b8e-91e8-cc5391c18a16"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ë¶„í• ëœ ì²­í¬ ìˆ˜: 7\n"
     ]
    }
   ],
   "source": [
    "# Markdown ì „ìš© Splitter ìƒì„±\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,\n",
    "    chunk_size=60,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë¬¸ì„œ ìƒì„±\n",
    "md_docs = md_splitter.create_documents(\n",
    "    [markdown_text],\n",
    "    [{'source': 'https://www.langchain.com'}]  # ì¶œì²˜ ë©”íƒ€ë°ì´í„°\n",
    ")\n",
    "\n",
    "print(f\"\\në¶„í• ëœ ì²­í¬ ìˆ˜: {len(md_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U9OHSnp4-x0"
   },
   "source": [
    "# 3. ë¶„í•  ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KPK8PPuE4-x0",
    "outputId": "189a338f-0687-40d4-8594-30b6a6d99450"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== ì²­í¬ 1 ===\n",
      "metadata: {'source': 'https://www.langchain.com'}\n",
      "content (56ì):\n",
      "# ğŸ¦œğŸ”— LangChain âš¡ Building applications with LLMs through\n",
      "\n",
      "=== ì²­í¬ 2 ===\n",
      "metadata: {'source': 'https://www.langchain.com'}\n",
      "content (15ì):\n",
      "composability âš¡\n",
      "\n",
      "=== ì²­í¬ 3 ===\n",
      "metadata: {'source': 'https://www.langchain.com'}\n",
      "content (46ì):\n",
      "## Quick Install\n",
      "```bash\n",
      "pip install langchain\n",
      "\n",
      "=== ì²­í¬ 4 ===\n",
      "metadata: {'source': 'https://www.langchain.com'}\n",
      "content (3ì):\n",
      "```\n",
      "\n",
      "=== ì²­í¬ 5 ===\n",
      "metadata: {'source': 'https://www.langchain.com'}\n",
      "content (59ì):\n",
      "As an open source project in a rapidly developing field, we\n",
      "\n",
      "=== ì²­í¬ 6 ===\n",
      "metadata: {'source': 'https://www.langchain.com'}\n",
      "content (18ì):\n",
      "are extremely open\n",
      "\n",
      "=== ì²­í¬ 7 ===\n",
      "metadata: {'source': 'https://www.langchain.com'}\n",
      "content (17ì):\n",
      "to contributions.\n"
     ]
    }
   ],
   "source": [
    "# ê° ì²­í¬ í™•ì¸\n",
    "for i, doc in enumerate(md_docs):\n",
    "    print(f\"\\n=== ì²­í¬ {i+1} ===\")\n",
    "    print(f\"metadata: {doc.metadata}\")\n",
    "    print(f\"content ({len(doc.page_content)}ì):\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvFBccfM4-x0"
   },
   "source": [
    "# 4. í—¤ë” ê¸°ë°˜ ë¶„í•  (MarkdownHeaderTextSplitter)\n",
    "\n",
    "í—¤ë”ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ë³´ì¡´í•˜ë©´ì„œ ë¶„í• í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22VCX_Pv4-x0",
    "outputId": "03fed62e-db12-4bbf-ba88-b5f2a0e67618"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== í—¤ë” ê¸°ë°˜ ë¶„í•  ê²°ê³¼ ===\n",
      "\n",
      "--- ì²­í¬ 1 ---\n",
      "metadata: {'Header 1': 'LangChain ì†Œê°œ'}\n",
      "content: LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "\n",
      "--- ì²­í¬ 2 ---\n",
      "metadata: {'Header 1': 'LangChain ì†Œê°œ', 'Header 2': 'ì„¤ì¹˜ ë°©ë²•'}\n",
      "content: pip install langchain ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "--- ì²­í¬ 3 ---\n",
      "metadata: {'Header 1': 'LangChain ì†Œê°œ', 'Header 2': 'ì£¼ìš” ê¸°ëŠ¥', 'Header 3': 'ì²´ì¸ êµ¬ì„±'}\n",
      "content: LCELë¡œ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
      "\n",
      "--- ì²­í¬ 4 ---\n",
      "metadata: {'Header 1': 'LangChain ì†Œê°œ', 'Header 2': 'ì£¼ìš” ê¸°ëŠ¥', 'Header 3': 'RAG'}\n",
      "content: ê²€ìƒ‰ ì¦ê°• ìƒì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# ë¶„í• í•  í—¤ë” ë ˆë²¨ ì •ì˜\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "md_header_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "# ë” êµ¬ì¡°í™”ëœ Markdown ì˜ˆì‹œ\n",
    "structured_md = '''# LangChain ì†Œê°œ\n",
    "\n",
    "LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "## ì„¤ì¹˜ ë°©ë²•\n",
    "\n",
    "pip install langchain ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” ê¸°ëŠ¥\n",
    "\n",
    "### ì²´ì¸ êµ¬ì„±\n",
    "\n",
    "LCELë¡œ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "### RAG\n",
    "\n",
    "ê²€ìƒ‰ ì¦ê°• ìƒì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "'''\n",
    "\n",
    "header_splits = md_header_splitter.split_text(structured_md)\n",
    "\n",
    "print(\"=== í—¤ë” ê¸°ë°˜ ë¶„í•  ê²°ê³¼ ===\")\n",
    "for i, doc in enumerate(header_splits):\n",
    "    print(f\"\\n--- ì²­í¬ {i+1} ---\")\n",
    "    print(f\"metadata: {doc.metadata}\")\n",
    "    print(f\"content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ny3Vtd8b4-x0"
   },
   "source": [
    "---\n",
    "\n",
    "## ë‘ Splitter ë¹„êµ\n",
    "\n",
    "| Splitter | íŠ¹ì§• |\n",
    "|----------|------|\n",
    "| `RecursiveCharacterTextSplitter.from_language(MARKDOWN)` | chunk_size ê¸°ë°˜ ë¶„í•  |\n",
    "| `MarkdownHeaderTextSplitter` | í—¤ë” ê¸°ë°˜ ë¶„í• , í—¤ë”ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ë³´ì¡´ |\n",
    "\n",
    "## ì¡°í•© ì‚¬ìš© (ê¶Œì¥)\n",
    "\n",
    "```python\n",
    "# 1. í—¤ë”ë¡œ ë¨¼ì € ë¶„í• \n",
    "header_splits = md_header_splitter.split_text(markdown)\n",
    "\n",
    "# 2. í° ì„¹ì…˜ì€ ì¶”ê°€ë¡œ ë¶„í• \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "final_splits = text_splitter.split_documents(header_splits)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q58xqfuh4-x0"
   },
   "source": [
    "---\n",
    "## ì½”ë“œ 2-7 Embeddingsë¡œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJbrFBss4-x0"
   },
   "source": [
    "# Embeddingsë¡œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **Embeddings**ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°(ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## RAG íŒŒì´í”„ë¼ì¸ì—ì„œì˜ ìœ„ì¹˜\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸ“„ Load    â”‚ â”€â”€â–¶ â”‚  âœ‚ï¸ Split   â”‚ â”€â”€â–¶ â”‚  ğŸ”¢ Embed   â”‚ â”€â”€â–¶ â”‚  ğŸ’¾ Store   â”‚\n",
    "â”‚  (ë¡œë“œ)     â”‚     â”‚  (ë¶„í• )     â”‚     â”‚  (ì„ë² ë”©)    â”‚     â”‚  (ì €ì¥)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                              â†‘\n",
    "                                           í˜„ì¬ ë‹¨ê³„\n",
    "```\n",
    "\n",
    "## Embeddingì´ë€?\n",
    "\n",
    "í…ìŠ¤íŠ¸ì˜ **ì˜ë¯¸**ë¥¼ ìˆ«ì ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "\"ì•ˆë…•í•˜ì„¸ìš”\" â†’ [0.12, -0.34, 0.56, ..., 0.78]  (1536ì°¨ì›)\n",
    "```\n",
    "\n",
    "**íŠ¹ì§•:**\n",
    "- ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ í…ìŠ¤íŠ¸ â†’ ë²¡í„°ê°€ ê°€ê¹Œì›€\n",
    "- ë²¡í„° ê°„ ê±°ë¦¬/ìœ ì‚¬ë„ë¡œ ê²€ìƒ‰ ê°€ëŠ¥\n",
    "\n",
    "## ì£¼ìš” Embedding ëª¨ë¸\n",
    "\n",
    "| ëª¨ë¸ | íŠ¹ì§• |\n",
    "|------|------|\n",
    "| OpenAI `text-embedding-3-small` | ë¹ ë¥´ê³  ì €ë ´ |\n",
    "| OpenAI `text-embedding-3-large` | ë†’ì€ ì •í™•ë„ |\n",
    "| Ollama (nomic-embed-text) | ë¡œì»¬, ë¬´ë£Œ |\n",
    "| HuggingFace | ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ |\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Ollama ì„¤ì¹˜ ë° ì„œë²„ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FhYC8jy4-x0",
    "outputId": "083bb3a6-d4fc-4afa-db6d-05b7e36cee43"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n",
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKV_1x7G4-x0"
   },
   "source": [
    "# 2. ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ & íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAUI0iK_4-x0",
    "outputId": "837c5c95-f3be-4ed0-ef60-798716756b91"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# nomic-embed-text: Ollamaì˜ ì„ë² ë”© ëª¨ë¸\n",
    "!ollama pull nomic-embed-text\n",
    "!pip install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKLYfY9e4-x0"
   },
   "source": [
    "# 3. Embeddings ìƒì„±\n",
    "\n",
    "**ì½”ë“œ ì„¤ëª…:**\n",
    "\n",
    "```python\n",
    "model = OllamaEmbeddings(model='nomic-embed-text')\n",
    "embeddings = model.embed_documents(['í…ìŠ¤íŠ¸1', 'í…ìŠ¤íŠ¸2', ...])\n",
    "```\n",
    "\n",
    "- `embed_documents()` - ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
    "- `embed_query()` - ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wcn84MY4-x0",
    "outputId": "346f0f1c-1ca8-4dc5-928e-26ce97d36aee"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ì„ë² ë”©ëœ ë¬¸ì„œ ìˆ˜: 5\n",
      "ê° ë²¡í„°ì˜ ì°¨ì›: 768\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ìƒì„±\n",
    "model = OllamaEmbeddings(model='nomic-embed-text')\n",
    "\n",
    "# ì—¬ëŸ¬ ë¬¸ì„œ ì„ë² ë”©\n",
    "texts = [\n",
    "    'Hi there!',\n",
    "    'Oh, hello!',\n",
    "    \"What's your name?\",\n",
    "    'My friends call me World',\n",
    "    'Hello World!'\n",
    "]\n",
    "\n",
    "embeddings = model.embed_documents(texts)\n",
    "\n",
    "print(f\"ì„ë² ë”©ëœ ë¬¸ì„œ ìˆ˜: {len(embeddings)}\")\n",
    "print(f\"ê° ë²¡í„°ì˜ ì°¨ì›: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_q4DCq_4-x0"
   },
   "source": [
    "# 4. ì„ë² ë”© ë²¡í„° í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19wEeN6w4-x1",
    "outputId": "b612fe3e-88d1-4fcb-e986-fe1db5350441"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "í…ìŠ¤íŠ¸: 'Hi there!'\n",
      "ë²¡í„° (ì²˜ìŒ 10ê°œ): [0.020429805, -0.010229979, -0.177349, -0.0413164, 0.0508709, -0.0045323865, -0.030966198, 0.034292486, -0.016935103, -0.07356401]\n",
      "...\n",
      "ë²¡í„° (ë§ˆì§€ë§‰ 5ê°œ): [0.0132998945, -0.034147747, -0.029394977, -0.0061468175, -0.014125981]\n"
     ]
    }
   ],
   "source": [
    "# ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì˜ ë²¡í„° (ì¼ë¶€ë§Œ ì¶œë ¥)\n",
    "print(f\"í…ìŠ¤íŠ¸: '{texts[0]}'\")\n",
    "print(f\"ë²¡í„° (ì²˜ìŒ 10ê°œ): {embeddings[0][:10]}\")\n",
    "print(f\"...\")\n",
    "print(f\"ë²¡í„° (ë§ˆì§€ë§‰ 5ê°œ): {embeddings[0][-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMT528Se4-x1"
   },
   "source": [
    "# 5. ìœ ì‚¬ë„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSw8jN9q4-x1",
    "outputId": "4eaeba04-757a-43ee-d761-606da1a1202a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ===\n",
      "'Hi there!' vs 'Oh, hello!': 0.7517\n",
      "'Hi there!' vs 'What's your name?': 0.5900\n",
      "'Hi there!' vs 'My friends call me World': 0.5899\n",
      "'Hi there!' vs 'Hello World!': 0.8568\n",
      "'Oh, hello!' vs 'What's your name?': 0.6113\n",
      "'Oh, hello!' vs 'My friends call me World': 0.5965\n",
      "'Oh, hello!' vs 'Hello World!': 0.7607\n",
      "'What's your name?' vs 'My friends call me World': 0.6831\n",
      "'What's your name?' vs 'Hello World!': 0.5879\n",
      "'My friends call me World' vs 'Hello World!': 0.6176\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# ê° í…ìŠ¤íŠ¸ ìŒì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "print(\"=== í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ===\")\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i+1, len(texts)):\n",
    "        sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        print(f\"'{texts[i]}' vs '{texts[j]}': {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ4QnVll4-x1"
   },
   "source": [
    "# 6. ì¿¼ë¦¬ ì„ë² ë”© (ê²€ìƒ‰ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyPDOF7X4-x1",
    "outputId": "7461b62d-4424-4abc-c041-cd4e34a97c3b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ì¿¼ë¦¬: 'greeting'\n",
      "ì¿¼ë¦¬ ë²¡í„° ì°¨ì›: 768\n",
      "\n",
      "=== ì¿¼ë¦¬ì™€ ë¬¸ì„œ ìœ ì‚¬ë„ ===\n",
      "'Hi there!': 0.4996\n",
      "'Oh, hello!': 0.7707\n",
      "'What's your name?': 0.4498\n",
      "'My friends call me World': 0.4937\n",
      "'Hello World!': 0.4925\n"
     ]
    }
   ],
   "source": [
    "# ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”©\n",
    "query = \"greeting\"\n",
    "query_embedding = model.embed_query(query)\n",
    "\n",
    "print(f\"ì¿¼ë¦¬: '{query}'\")\n",
    "print(f\"ì¿¼ë¦¬ ë²¡í„° ì°¨ì›: {len(query_embedding)}\")\n",
    "\n",
    "# ì¿¼ë¦¬ì™€ ê° ë¬¸ì„œì˜ ìœ ì‚¬ë„\n",
    "print(\"\\n=== ì¿¼ë¦¬ì™€ ë¬¸ì„œ ìœ ì‚¬ë„ ===\")\n",
    "for i, text in enumerate(texts):\n",
    "    sim = cosine_similarity(query_embedding, embeddings[i])\n",
    "    print(f\"'{text}': {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeVU2fp14-x1"
   },
   "source": [
    "---\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸ (OpenAI)\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# ë³€ê²½ (Ollama)\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "model = OllamaEmbeddings(model='nomic-embed-text')\n",
    "```\n",
    "\n",
    "## Embedding ë©”ì„œë“œ\n",
    "\n",
    "| ë©”ì„œë“œ | ìš©ë„ |\n",
    "|--------|------|\n",
    "| `embed_documents([texts])` | ë¬¸ì„œë“¤ì„ ë²¡í„°ë¡œ ë³€í™˜ (ì €ì¥ìš©) |\n",
    "| `embed_query(text)` | ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ |\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ì„ë² ë”©ëœ ë²¡í„°ëŠ” **Vector Store**ì— ì €ì¥í•˜ì—¬ ìœ ì‚¬ë„ ê²€ìƒ‰ì— ì‚¬ìš©í•©ë‹ˆë‹¤. (09ë²ˆ ë…¸íŠ¸ë¶)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKLyt-zC4-x1"
   },
   "source": [
    "---\n",
    "## ì½”ë“œ 2-8 Load â†’ Split â†’ Embed ì „ì²´ íŒŒì´í”„ë¼ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClyIFLLY4-x1"
   },
   "source": [
    "# Load â†’ Split â†’ Embed ì „ì²´ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **ë¬¸ì„œ ë¡œë“œ â†’ ë¶„í•  â†’ ì„ë² ë”©**ê¹Œì§€ì˜ ì „ì²´ íë¦„ì„ ì—°ê²°í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "## RAG ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸ“„ Load    â”‚ â”€â”€â–¶ â”‚  âœ‚ï¸ Split   â”‚ â”€â”€â–¶ â”‚  ğŸ”¢ Embed   â”‚ â”€â”€â–¶ â”‚  ğŸ’¾ Store   â”‚\n",
    "â”‚ TextLoader  â”‚     â”‚ TextSplitterâ”‚     â”‚  Embeddings â”‚     â”‚ VectorStore â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†‘                    â†‘                    â†‘\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              ì´ ë…¸íŠ¸ë¶ì—ì„œ ë‹¤ë£¨ëŠ” ë²”ìœ„\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Ollama ì„¤ì¹˜ ë° ì„œë²„ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnkXRKJJ4-x1",
    "outputId": "b69d4bd5-9b85-47d9-9474-43344a77bee8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n",
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0Dps-jF4-x1"
   },
   "source": [
    "# 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5VTejnY4-x1",
    "outputId": "cad4bed1-9be5-49d5-89e7-8a3e85253d70"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull nomic-embed-text\n# pip installì€ Cell 1ì—ì„œ ë²„ì „ ê³ ì •ìœ¼ë¡œ ì„¤ì¹˜ ì™„ë£Œ\n!pip install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSzuS6bA4-x1"
   },
   "source": [
    "# 3. í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJJAIN2F4-x1",
    "outputId": "38cde701-f8e2-4171-907d-b9f9de5bc1d6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test.txt ìƒì„± ì™„ë£Œ (423ì)\n"
     ]
    }
   ],
   "source": [
    "sample_text = '''LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì£¼ìš” ê¸°ëŠ¥:\n",
    "1. í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ë° ìµœì í™”\n",
    "2. ì²´ì¸ êµ¬ì„±ì„ í†µí•œ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° ìƒì„±\n",
    "3. ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ì˜ ì—°ë™\n",
    "4. ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ í†µí•œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\n",
    "\n",
    "LangChainì„ ì‚¬ìš©í•˜ë©´ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. RAGëŠ” ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ì‘ë‹µì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "\n",
    "Vector StoreëŠ” ì„ë² ë”©ëœ ë¬¸ì„œë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ Chroma, Pinecone, PGVector ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ í…ìŠ¤íŠ¸ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ìˆìŠµë‹ˆë‹¤.\n",
    "'''\n",
    "\n",
    "with open('./test.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(f\"test.txt ìƒì„± ì™„ë£Œ ({len(sample_text)}ì)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpBDPvub4-x1"
   },
   "source": [
    "# 4. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "\n",
    "## Step 1: ë¬¸ì„œ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3k31iKhL4-x1",
    "outputId": "f186bdcf-b32c-4975-b422-6237b09e5f6b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Step 1: ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n",
      "   - ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: 1\n",
      "   - ì›ë³¸ ê¸¸ì´: 423ì\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "loader = TextLoader('./test.txt', encoding='utf-8')\n",
    "doc = loader.load()\n",
    "\n",
    "print(f\"âœ… Step 1: ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"   - ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(doc)}\")\n",
    "print(f\"   - ì›ë³¸ ê¸¸ì´: {len(doc[0].page_content)}ì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6j3mAJb4-x1"
   },
   "source": [
    "## Step 2: ë¬¸ì„œ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QP22AxZs4-x1",
    "outputId": "03b3d0a3-3eb0-4300-bd65-1b87435e88be"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "âœ… Step 2: ë¬¸ì„œ ë¶„í•  ì™„ë£Œ\n",
      "   - ë¶„í• ëœ ì²­í¬ ìˆ˜: 3\n",
      "   - ì²­í¬ í¬ê¸°: [149, 110, 159]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ë¬¸ì„œ ë¶„í• \n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = splitter.split_documents(doc)\n",
    "\n",
    "print(f\"\\nâœ… Step 2: ë¬¸ì„œ ë¶„í•  ì™„ë£Œ\")\n",
    "print(f\"   - ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(chunks)}\")\n",
    "print(f\"   - ì²­í¬ í¬ê¸°: {[len(c.page_content) for c in chunks]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkMY67dt4-x1"
   },
   "source": [
    "## Step 3: ì„ë² ë”© ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hU80j3QE4-x1",
    "outputId": "ad9f2a34-9143-4369-eb97-d53fea22939c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "âœ… Step 3: ì„ë² ë”© ìƒì„± ì™„ë£Œ\n",
      "   - ì„ë² ë”© ìˆ˜: 3\n",
      "   - ë²¡í„° ì°¨ì›: 768\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ìƒì„±\n",
    "embeddings_model = OllamaEmbeddings(model='nomic-embed-text')\n",
    "\n",
    "# ê° ì²­í¬ì˜ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [chunk.page_content for chunk in chunks]\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Step 3: ì„ë² ë”© ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"   - ì„ë² ë”© ìˆ˜: {len(embeddings)}\")\n",
    "print(f\"   - ë²¡í„° ì°¨ì›: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vE0v9to4-x1"
   },
   "source": [
    "# 5. ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlZ9XY1f4-x1",
    "outputId": "e3e20a7a-7b62-492b-f55c-f89190dc4ff9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== ì²­í¬-ì„ë² ë”© ë§¤í•‘ ===\n",
      "\n",
      "[ì²­í¬ 1]\n",
      "  í…ìŠ¤íŠ¸: LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬...\n",
      "  ë²¡í„°: [-0.0124, 0.0023, ..., -0.0694]\n",
      "\n",
      "[ì²­í¬ 2]\n",
      "  í…ìŠ¤íŠ¸: LangChainì„ ì‚¬ìš©í•˜ë©´ RAG(Retrieval-Augmented Generation...\n",
      "  ë²¡í„°: [-0.0096, 0.0637, ..., -0.0509]\n",
      "\n",
      "[ì²­í¬ 3]\n",
      "  í…ìŠ¤íŠ¸: Vector StoreëŠ” ì„ë² ë”©ëœ ë¬¸ì„œë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤...\n",
      "  ë²¡í„°: [0.0232, 0.0238, ..., -0.0396]\n"
     ]
    }
   ],
   "source": [
    "# ì²­í¬ì™€ ì„ë² ë”© ë§¤í•‘ í™•ì¸\n",
    "print(\"=== ì²­í¬-ì„ë² ë”© ë§¤í•‘ ===\")\n",
    "for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):\n",
    "    print(f\"\\n[ì²­í¬ {i+1}]\")\n",
    "    print(f\"  í…ìŠ¤íŠ¸: {chunk.page_content[:50]}...\")\n",
    "    print(f\"  ë²¡í„°: [{emb[0]:.4f}, {emb[1]:.4f}, ..., {emb[-1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzZh_odn4-x1"
   },
   "source": [
    "---\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸ (OpenAI)\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# ë³€ê²½ (Ollama)\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings_model = OllamaEmbeddings(model='nomic-embed-text')\n",
    "```\n",
    "\n",
    "## ì „ì²´ íŒŒì´í”„ë¼ì¸ ìš”ì•½\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# 1. Load\n",
    "docs = TextLoader('./file.txt').load()\n",
    "\n",
    "# 2. Split\n",
    "chunks = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "\n",
    "# 3. Embed\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text').embed_documents(\n",
    "    [c.page_content for c in chunks]\n",
    ")\n",
    "```\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "ìƒì„±ëœ ì„ë² ë”©ì„ **Vector Store**ì— ì €ì¥í•˜ì—¬ ìœ ì‚¬ë„ ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. (09ë²ˆ ë…¸íŠ¸ë¶)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WshbflXa4-x1"
   },
   "source": [
    "---\n",
    "## ì½”ë“œ 2-9~2-12 PGVectorë¡œ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvK7vDO24-x1"
   },
   "source": [
    "# PGVectorë¡œ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•í•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **PGVector**(PostgreSQL + pgvector í™•ì¥)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ êµ¬ì¶•í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## RAG íŒŒì´í”„ë¼ì¸ì—ì„œì˜ ìœ„ì¹˜\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸ“„ Load    â”‚ â”€â”€â–¶ â”‚  âœ‚ï¸ Split   â”‚ â”€â”€â–¶ â”‚  ğŸ”¢ Embed   â”‚ â”€â”€â–¶ â”‚  ğŸ’¾ Store   â”‚\n",
    "â”‚  (ë¡œë“œ)     â”‚     â”‚  (ë¶„í• )     â”‚     â”‚  (ì„ë² ë”©)    â”‚     â”‚  (ì €ì¥)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                                  â†‘\n",
    "                                                               í˜„ì¬ ë‹¨ê³„\n",
    "```\n",
    "\n",
    "## Vector Storeë€?\n",
    "\n",
    "ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  **ìœ ì‚¬ë„ ê²€ìƒ‰**ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” Vector Store\n",
    "\n",
    "| Store | íŠ¹ì§• |\n",
    "|-------|------|\n",
    "| **PGVector** | PostgreSQL ê¸°ë°˜, SQL ì§€ì› |\n",
    "| Chroma | ê²½ëŸ‰, ë¡œì»¬ ê°œë°œì— ì í•© |\n",
    "| Pinecone | í´ë¼ìš°ë“œ ë§¤ë‹ˆì§€ë“œ |\n",
    "| Weaviate | ê·¸ë˜í”„ ê²€ìƒ‰ ì§€ì› |\n",
    "| FAISS | ê³ ì„±ëŠ¥, ë©”ëª¨ë¦¬ ê¸°ë°˜ |\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Dockerë¡œ PGVector ì‹¤í–‰\n",
    "\n",
    "ë¨¼ì € í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ PGVector ì»¨í…Œì´ë„ˆë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "    --name pgvector-container \\\n",
    "    -e POSTGRES_USER=langchain \\\n",
    "    -e POSTGRES_PASSWORD=langchain \\\n",
    "    -e POSTGRES_DB=langchain \\\n",
    "    -p 6024:5432 \\\n",
    "    -d pgvector/pgvector:pg16\n",
    "```\n",
    "\n",
    "> **âš ï¸ Google Colab ì‚¬ìš©ì:** Dockerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ì•„ë˜ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•˜ì—¬ PostgreSQL + pgvectorë¥¼ ì§ì ‘ ì„¤ì¹˜í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vp2UrHtY4-x1",
    "outputId": "5b97f23b-c9a7-423e-d1e6-2ef86356d567"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "fatal: destination path 'pgvector' already exists and is not an empty directory.\n",
      "make: Nothing to be done for 'all'.\n",
      "/bin/mkdir -p '/usr/lib/postgresql/14/lib'\n",
      "/bin/mkdir -p '/usr/share/postgresql/14/extension'\n",
      "/bin/mkdir -p '/usr/share/postgresql/14/extension'\n",
      "/usr/bin/install -c -m 755  vector.so '/usr/lib/postgresql/14/lib/vector.so'\n",
      "/usr/bin/install -c -m 644 .//vector.control '/usr/share/postgresql/14/extension/'\n",
      "/usr/bin/install -c -m 644 .//sql/vector--0.1.0--0.1.1.sql .//sql/vector--0.1.1--0.1.3.sql .//sql/vector--0.1.3--0.1.4.sql .//sql/vector--0.1.4--0.1.5.sql .//sql/vector--0.1.5--0.1.6.sql .//sql/vector--0.1.6--0.1.7.sql .//sql/vector--0.1.7--0.1.8.sql .//sql/vector--0.1.8--0.2.0.sql .//sql/vector--0.2.0--0.2.1.sql .//sql/vector--0.2.1--0.2.2.sql .//sql/vector--0.2.2--0.2.3.sql .//sql/vector--0.2.3--0.2.4.sql .//sql/vector--0.2.4--0.2.5.sql .//sql/vector--0.2.5--0.2.6.sql .//sql/vector--0.2.6--0.2.7.sql .//sql/vector--0.2.7--0.3.0.sql .//sql/vector--0.3.0--0.3.1.sql .//sql/vector--0.3.1--0.3.2.sql .//sql/vector--0.3.2--0.4.0.sql .//sql/vector--0.4.0--0.4.1.sql .//sql/vector--0.4.1--0.4.2.sql .//sql/vector--0.4.2--0.4.3.sql .//sql/vector--0.4.3--0.4.4.sql .//sql/vector--0.4.4--0.5.0.sql .//sql/vector--0.5.0--0.5.1.sql .//sql/vector--0.5.1--0.6.0.sql .//sql/vector--0.6.0--0.6.1.sql .//sql/vector--0.6.1--0.6.2.sql .//sql/vector--0.6.2--0.7.0.sql .//sql/vector--0.7.0--0.7.1.sql .//sql/vector--0.7.1--0.7.2.sql .//sql/vector--0.7.2--0.7.3.sql .//sql/vector--0.7.3--0.7.4.sql .//sql/vector--0.7.4--0.8.0.sql sql/vector--0.8.0.sql '/usr/share/postgresql/14/extension/'\n",
      "/bin/mkdir -p '/usr/include/postgresql/14/server/extension/vector/'\n",
      "/usr/bin/install -c -m 644   .//src/halfvec.h .//src/sparsevec.h .//src/vector.h '/usr/include/postgresql/14/server/extension/vector/'\n",
      "/bin/mkdir -p '/usr/lib/postgresql/14/lib/bitcode/vector'\n",
      "/bin/mkdir -p '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/bitutils.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/bitvec.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/halfutils.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/halfvec.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/hnsw.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/hnswbuild.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/hnswinsert.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/hnswscan.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/hnswutils.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/hnswvacuum.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/ivfbuild.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/ivfflat.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/ivfinsert.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/ivfkmeans.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/ivfscan.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/ivfutils.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/ivfvacuum.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/sparsevec.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "/usr/bin/install -c -m 644 src/vector.bc '/usr/lib/postgresql/14/lib/bitcode'/vector/src/\n",
      "cd '/usr/lib/postgresql/14/lib/bitcode' && /usr/lib/llvm-14/bin/llvm-lto -thinlto -thinlto-action=thinlink -o vector.index.bc vector/src/bitutils.bc vector/src/bitvec.bc vector/src/halfutils.bc vector/src/halfvec.bc vector/src/hnsw.bc vector/src/hnswbuild.bc vector/src/hnswinsert.bc vector/src/hnswscan.bc vector/src/hnswutils.bc vector/src/hnswvacuum.bc vector/src/ivfbuild.bc vector/src/ivfflat.bc vector/src/ivfinsert.bc vector/src/ivfkmeans.bc vector/src/ivfscan.bc vector/src/ivfutils.bc vector/src/ivfvacuum.bc vector/src/sparsevec.bc vector/src/vector.bc\n",
      " * Starting PostgreSQL 14 database server\n",
      "   ...done.\n",
      "ERROR:  role \"langchain\" already exists\n",
      "ERROR:  database \"langchain\" already exists\n",
      "NOTICE:  extension \"vector\" already exists, skipping\n",
      "CREATE EXTENSION\n",
      "ALTER ROLE\n",
      " * Restarting PostgreSQL 14 database server\n",
      "   ...done.\n",
      "             status             \n",
      "--------------------------------\n",
      " PostgreSQL ready on port 6024!\n",
      "(1 row)\n",
      "\n",
      "âœ… PostgreSQL + pgvector ì„¤ì¹˜ ë° ì‹¤í–‰ ì™„ë£Œ (í¬íŠ¸: 6024)\n"
     ]
    }
   ],
   "source": [
    "# [ì¶”ê°€] Colabì—ì„œ PostgreSQL + pgvector ì„¤ì¹˜ ë° ì‹¤í–‰ (Docker ëŒ€ì‹ )\n",
    "# ë¡œì»¬ì—ì„œ Dockerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì´ ì…€ì„ ê±´ë„ˆë›°ì„¸ìš”.\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# PostgreSQL ì„¤ì¹˜\n",
    "!sudo apt-get -qq update\n",
    "!sudo apt-get -qq install -y postgresql postgresql-server-dev-all\n",
    "\n",
    "# pgvector í™•ì¥ ì„¤ì¹˜ (ì†ŒìŠ¤ì—ì„œ ë¹Œë“œ)\n",
    "!cd /tmp && git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git\n",
    "!cd /tmp/pgvector && make && sudo make install\n",
    "\n",
    "# PostgreSQL ì‹œì‘\n",
    "!sudo service postgresql start\n",
    "\n",
    "# langchain ì‚¬ìš©ì ë° DB ìƒì„±\n",
    "!sudo -u postgres psql -c \"CREATE USER langchain WITH PASSWORD 'langchain';\"\n",
    "!sudo -u postgres psql -c \"CREATE DATABASE langchain OWNER langchain;\"\n",
    "!sudo -u postgres psql -d langchain -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n",
    "!sudo -u postgres psql -c \"ALTER USER langchain WITH SUPERUSER;\"\n",
    "\n",
    "# í¬íŠ¸ë¥¼ 6024ë¡œ ë³€ê²½í•˜ê³  PostgreSQL ì¬ì‹œì‘\n",
    "!sudo sed -i \"s/^port = .*/port = 6024/\" /etc/postgresql/*/main/postgresql.conf\n",
    "\n",
    "# pg_hba.confì—ì„œ ë¡œì»¬ ì ‘ì† í—ˆìš©\n",
    "!sudo bash -c 'echo \"host all all 127.0.0.1/32 md5\" >> /etc/postgresql/*/main/pg_hba.conf'\n",
    "!sudo bash -c 'echo \"host all all ::1/128 md5\" >> /etc/postgresql/*/main/pg_hba.conf'\n",
    "\n",
    "# PostgreSQL ì¬ì‹œì‘\n",
    "!sudo service postgresql restart\n",
    "\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "# ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "!PGPASSWORD=langchain psql -h localhost -p 6024 -U langchain -d langchain -c \"SELECT 'PostgreSQL ready on port 6024!' AS status;\"\n",
    "print('âœ… PostgreSQL + pgvector ì„¤ì¹˜ ë° ì‹¤í–‰ ì™„ë£Œ (í¬íŠ¸: 6024)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9xUO2dw4-x1"
   },
   "source": [
    "# 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAkwrNhy4-x1"
   },
   "source": [
    "# 3. Ollama ì„¤ì¹˜ ë° ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3Z_wyTp4-x1",
    "outputId": "f340b471-e70a-425c-f4b1-11b0f8ce543f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n",
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tk968-0p4-x1"
   },
   "source": [
    "# 4. í…ŒìŠ¤íŠ¸ íŒŒì¼ ì¤€ë¹„ ë° ë¬¸ì„œ ë¡œë“œ/ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbN_6sls4-x2",
    "outputId": "91baad13-5739-4d70-8375-4e9dc6533830"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test.txt ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±\n",
    "sample_text = '''LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "RAGëŠ” ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ì‘ë‹µì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "Vector StoreëŠ” ì„ë² ë”©ëœ ë¬¸ì„œë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "'''\n",
    "\n",
    "with open('./test.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(\"test.txt ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EAk37FJr4-x2",
    "outputId": "5dfc615b-189e-406e-c46a-776492bde14a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: 2\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• \n",
    "raw_documents = TextLoader('./test.txt', encoding='utf-8').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "print(f\"ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayBdYmmP4-x2"
   },
   "source": [
    "# 5. PGVectorì— ë¬¸ì„œ ì €ì¥\n",
    "\n",
    "**ì½”ë“œ ì„¤ëª…:**\n",
    "\n",
    "```python\n",
    "db = PGVector.from_documents(\n",
    "    documents,         # ì €ì¥í•  ë¬¸ì„œë“¤\n",
    "    embeddings_model,  # ì„ë² ë”© ëª¨ë¸\n",
    "    connection=connection  # DB ì—°ê²° ë¬¸ìì—´\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upVBO1Ow4-x2",
    "outputId": "87dd6c36-471a-40c4-98f7-22b90b46a559"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… ë¬¸ì„œê°€ PGVectorì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "# DB ì—°ê²° ì„¤ì •\n",
    "connection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸\n",
    "embeddings_model = OllamaEmbeddings(model='nomic-embed-text')\n",
    "\n",
    "# PGVectorì— ë¬¸ì„œ ì €ì¥\n",
    "db = PGVector.from_documents(\n",
    "    documents,\n",
    "    embeddings_model,\n",
    "    connection=connection\n",
    ")\n",
    "\n",
    "print(\"âœ… ë¬¸ì„œê°€ PGVectorì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KbJ2Bkc4-x2"
   },
   "source": [
    "# 6. ìœ ì‚¬ë„ ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWprs1h54-x2",
    "outputId": "21fcfc84-6b6f-4255-a283-72e077fdbed9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ì¿¼ë¦¬: 'RAGê°€ ë­”ê°€ìš”?'\n",
      "\n",
      "=== ê²€ìƒ‰ ê²°ê³¼ ===\n",
      "\n",
      "[1] Vector StoreëŠ” ì„ë² ë”©ëœ ë¬¸ì„œë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "    metadata: {'source': './test.txt'}\n",
      "\n",
      "[2] Vector StoreëŠ” ì„ë² ë”©ëœ ë¬¸ì„œë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "    metadata: {'source': './test.txt'}\n"
     ]
    }
   ],
   "source": [
    "# ìœ ì‚¬ë„ ê²€ìƒ‰\n",
    "query = 'RAGê°€ ë­”ê°€ìš”?'\n",
    "results = db.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"ì¿¼ë¦¬: '{query}'\\n\")\n",
    "print(\"=== ê²€ìƒ‰ ê²°ê³¼ ===\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] {doc.page_content}\")\n",
    "    print(f\"    metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxTOLgmS4-x2"
   },
   "source": [
    "# 7. ë¬¸ì„œ ì¶”ê°€ ë° ì‚­ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qobTJBj4-x2",
    "outputId": "2f124ef8-9931-4bbc-8256-cfc362d460c5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… ë¬¸ì„œ 2ê°œ ì¶”ê°€ ì™„ë£Œ\n",
      "   IDs: ['cc94ae19-3270-4df6-b268-2e459becbede', 'c43f11eb-efbd-4d28-9b1c-37c396271d58']\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\nimport uuid\n\n# ìƒˆ ë¬¸ì„œ ì¶”ê°€\nids = [str(uuid.uuid4()), str(uuid.uuid4())]\n\nnew_docs = [\n    Document(\n        page_content='there are cats in the pond',\n        metadata={'location': 'pond', 'topic': 'animals'}\n    ),\n    Document(\n        page_content='ducks are also found in the pond',\n        metadata={'location': 'pond', 'topic': 'animals'}\n    ),\n]\n\ndb.add_documents(new_docs, ids=ids)\nprint(f\"âœ… ë¬¸ì„œ 2ê°œ ì¶”ê°€ ì™„ë£Œ\")\nprint(f\"   IDs: {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSjbaK624-x2",
    "outputId": "261fb0aa-3af1-498f-ee87-178f97c34bdb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ì¡°íšŒëœ ë¬¸ì„œ ìˆ˜: 2\n",
      "\n",
      "âœ… ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ\n",
      "ì‚­ì œ í›„ ì¡°íšŒëœ ë¬¸ì„œ ìˆ˜: 2\n"
     ]
    }
   ],
   "source": [
    "# IDë¡œ ë¬¸ì„œ ì¡°íšŒ\n",
    "retrieved = db.get_by_ids(ids)\n",
    "print(f\"ì¡°íšŒëœ ë¬¸ì„œ ìˆ˜: {len(retrieved)}\")\n",
    "\n",
    "# ë¬¸ì„œ ì‚­ì œ\n",
    "db.delete({'ids': ids})\n",
    "print(f\"\\nâœ… ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ\")\n",
    "\n",
    "# ì‚­ì œ í™•ì¸\n",
    "retrieved_after = db.get_by_ids(ids)\n",
    "print(f\"ì‚­ì œ í›„ ì¡°íšŒëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_after)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTQEzvl84-x2"
   },
   "source": [
    "---\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸ (OpenAI)\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# ë³€ê²½ (Ollama)\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings_model = OllamaEmbeddings(model='nomic-embed-text')\n",
    "```\n",
    "\n",
    "## PGVector ì£¼ìš” ë©”ì„œë“œ\n",
    "\n",
    "| ë©”ì„œë“œ | ì„¤ëª… |\n",
    "|--------|------|\n",
    "| `from_documents()` | ë¬¸ì„œë¡œ ë²¡í„° ì €ì¥ì†Œ ìƒì„± |\n",
    "| `similarity_search()` | ìœ ì‚¬ë„ ê²€ìƒ‰ |\n",
    "| `add_documents()` | ë¬¸ì„œ ì¶”ê°€ |\n",
    "| `delete()` | ë¬¸ì„œ ì‚­ì œ |\n",
    "| `get_by_ids()` | IDë¡œ ë¬¸ì„œ ì¡°íšŒ |\n",
    "\n",
    "## Retrieverë¡œ ë³€í™˜\n",
    "\n",
    "```python\n",
    "# RAG ì²´ì¸ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ Retrieverë¡œ ë³€í™˜\n",
    "retriever = db.as_retriever(search_kwargs={'k': 4})\n",
    "docs = retriever.invoke('query')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpsEScSm4-x2"
   },
   "source": [
    "---\n",
    "## ì½”ë“œ 2-13 SQLRecordManagerë¡œ ë¬¸ì„œ ì¸ë±ì‹± ê´€ë¦¬í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCQtjExQ4-x2"
   },
   "source": [
    "# SQLRecordManagerë¡œ ë¬¸ì„œ ì¸ë±ì‹± ê´€ë¦¬í•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **SQLRecordManager**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì¸ë±ì‹±ì„ ê´€ë¦¬í•˜ê³  **ì¤‘ë³µ ë°©ì§€** ë° **ìë™ ì—…ë°ì´íŠ¸**ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## ì™œ Record Managerê°€ í•„ìš”í•œê°€?\n",
    "\n",
    "Vector Storeì— ë¬¸ì„œë¥¼ ë°˜ë³µ ì €ì¥í•˜ë©´:\n",
    "- ë™ì¼ ë¬¸ì„œê°€ **ì¤‘ë³µ ì €ì¥**ë¨\n",
    "- ìˆ˜ì •ëœ ë¬¸ì„œì˜ **ì´ì „ ë²„ì „ì´ ë‚¨ìŒ**\n",
    "- ì‚­ì œëœ ë¬¸ì„œê°€ **ì—¬ì „íˆ ê²€ìƒ‰**ë¨\n",
    "\n",
    "**Record Manager**ëŠ” ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤:\n",
    "- ë¬¸ì„œ í•´ì‹œë¡œ ì¤‘ë³µ ê°ì§€\n",
    "- ë³€ê²½ëœ ë¬¸ì„œë§Œ ì—…ë°ì´íŠ¸\n",
    "- ì‚­ì œëœ ë¬¸ì„œ ìë™ ì •ë¦¬\n",
    "\n",
    "## cleanup ëª¨ë“œ\n",
    "\n",
    "| ëª¨ë“œ | ë™ì‘ |\n",
    "|------|------|\n",
    "| `None` | ì¤‘ë³µ í—ˆìš©, ì •ë¦¬ ì•ˆí•¨ |\n",
    "| `incremental` | ë™ì¼ sourceì˜ ì´ì „ ë²„ì „ ì‚­ì œ |\n",
    "| `full` | í˜„ì¬ ë°°ì¹˜ì— ì—†ëŠ” ëª¨ë“  ë¬¸ì„œ ì‚­ì œ |\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Dockerë¡œ PGVector ì‹¤í–‰\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "    --name pgvector-container \\\n",
    "    -e POSTGRES_USER=langchain \\\n",
    "    -e POSTGRES_PASSWORD=langchain \\\n",
    "    -e POSTGRES_DB=langchain \\\n",
    "    -p 6024:5432 \\\n",
    "    -d pgvector/pgvector:pg16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqlpkeSd4-x2"
   },
   "source": [
    "# 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° Ollama ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmqWPi1t4-x2",
    "outputId": "a7a2c1ab-1c83-41bb-dc5f-7b8c4a3a7554"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n",
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9ysyi2I4-x2"
   },
   "source": [
    "# 3. Vector Store ë° Record Manager ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzPfXuk54-x2",
    "outputId": "3327cef7-5eff-4611-9273-1ef06cbf604c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.10)\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.12)\n",
      "Requirement already satisfied: langchain-postgres in /usr/local/lib/python3.12/dist-packages (0.0.16)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.46)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.9)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (26.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.14.0)\n",
      "Requirement already satisfied: asyncpg>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain-postgres) (0.31.0)\n",
      "Requirement already satisfied: pgvector<0.4,>=0.2.5 in /usr/local/lib/python3.12/dist-packages (from langchain-postgres) (0.3.6)\n",
      "Requirement already satisfied: psycopg-pool<4,>=3.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain-postgres) (3.3.0)\n",
      "Requirement already satisfied: psycopg<4,>=3 in /usr/local/lib/python3.12/dist-packages (from psycopg[binary]<4,>=3->langchain-postgres) (3.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: psycopg-binary==3.3.2 in /usr/local/lib/python3.12/dist-packages (from psycopg[binary]<4,>=3->langchain-postgres) (3.3.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain) (1.12.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "âœ… Vector Storeì™€ Record Manager ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# pip installì€ Cell 1ì—ì„œ ë²„ì „ ê³ ì •ìœ¼ë¡œ ì„¤ì¹˜ ì™„ë£Œ\n\nfrom langchain.indexes import SQLRecordManager\nfrom langchain_core.indexing import index\nfrom langchain_postgres.vectorstores import PGVector\nfrom langchain_ollama import OllamaEmbeddings\nfrom langchain.docstore.document import Document\n\n# ì„¤ì •\nconnection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\ncollection_name = 'my_docs'\nnamespace = 'my_docs_namespace'\n\n# ì„ë² ë”© ëª¨ë¸\nembeddings_model = OllamaEmbeddings(model='nomic-embed-text')\n\n# Vector Store\nvectorstore = PGVector(\n    embeddings=embeddings_model,\n    collection_name=collection_name,\n    connection=connection,\n    use_jsonb=True,\n)\n\n# Record Manager\nrecord_manager = SQLRecordManager(\n    namespace,\n    db_url=connection,\n)\n\n# ìŠ¤í‚¤ë§ˆ ìƒì„±\nrecord_manager.create_schema()\n\nprint(\"âœ… Vector Storeì™€ Record Manager ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmJVGdQ64-x2"
   },
   "source": [
    "# 4. ë¬¸ì„œ ì¸ë±ì‹± (1íšŒì°¨)\n",
    "\n",
    "**ì½”ë“œ ì„¤ëª…:**\n",
    "\n",
    "```python\n",
    "index(\n",
    "    docs,              # ì¸ë±ì‹±í•  ë¬¸ì„œë“¤\n",
    "    record_manager,    # ë ˆì½”ë“œ ê´€ë¦¬ì\n",
    "    vectorstore,       # ë²¡í„° ì €ì¥ì†Œ\n",
    "    cleanup='incremental',  # ì •ë¦¬ ëª¨ë“œ\n",
    "    source_id_key='source', # ì¶œì²˜ ì‹ë³„ í‚¤\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "du-0MWrP4-x2",
    "outputId": "5d02acab-c3ed-46c1-a530-8cad5399c38f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== ì¸ë±ì‹± 1íšŒì°¨ ===\n",
      "{'num_added': 1, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/langchain_core/indexing/api.py:403: UserWarning: Using SHA-1 for document hashing. SHA-1 is *not* collision-resistant; a motivated attacker can construct distinct inputs that map to the same fingerprint. If this matters in your threat model, switch to a stronger algorithm such as 'blake2b', 'sha256', or 'sha512' by specifying  `key_encoder` parameter in the `index` or `aindex` function. \n",
      "  _warn_about_sha1()\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ ìƒì„±\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content='there are cats in the pond',\n",
    "        metadata={'id': 1, 'source': 'cats.txt'}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content='ducks are also found in the pond',\n",
    "        metadata={'id': 2, 'source': 'ducks.txt'}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 1íšŒì°¨ ì¸ë±ì‹±\n",
    "result_1 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup='incremental',\n",
    "    source_id_key='source',\n",
    ")\n",
    "\n",
    "print(\"=== ì¸ë±ì‹± 1íšŒì°¨ ===\")\n",
    "print(result_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFk0il604-x2"
   },
   "source": [
    "# 5. ë™ì¼ ë¬¸ì„œ ì¬ì¸ë±ì‹± (2íšŒì°¨ - ì¤‘ë³µ ë°©ì§€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "IfmnU7Qu4-x2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1c6eb04b-6527-47d7-d232-ee81d869c79a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== ì¸ë±ì‹± 2íšŒì°¨ (ë™ì¼ ë¬¸ì„œ) ===\n",
      "{'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n",
      "\n",
      "â†’ num_added=0: ì¤‘ë³µì´ ë°©ì§€ë¨!\n"
     ]
    }
   ],
   "source": [
    "# 2íšŒì°¨ ì¸ë±ì‹± - ë™ì¼ ë¬¸ì„œ\n",
    "result_2 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup='incremental',\n",
    "    source_id_key='source',\n",
    ")\n",
    "\n",
    "print(\"=== ì¸ë±ì‹± 2íšŒì°¨ (ë™ì¼ ë¬¸ì„œ) ===\")\n",
    "print(result_2)\n",
    "print(\"\\nâ†’ num_added=0: ì¤‘ë³µì´ ë°©ì§€ë¨!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dC25jLM04-x2"
   },
   "source": [
    "# 6. ë¬¸ì„œ ìˆ˜ì • í›„ ì¬ì¸ë±ì‹± (3íšŒì°¨ - ìë™ ì—…ë°ì´íŠ¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "rAcvSujZ4-x2",
    "outputId": "aafc3a60-b4f5-495f-cef3-ab05214a9b7c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== ì¸ë±ì‹± 3íšŒì°¨ (ìˆ˜ì •ëœ ë¬¸ì„œ) ===\n",
      "{'num_added': 1, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}\n",
      "\n",
      "â†’ num_added=1, num_deleted=1: ìˆ˜ì •ëœ ë¬¸ì„œê°€ ì—…ë°ì´íŠ¸ë¨!\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ ë‚´ìš© ìˆ˜ì •\n",
    "docs[0].page_content = 'I just modified this document!'\n",
    "\n",
    "# 3íšŒì°¨ ì¸ë±ì‹± - ìˆ˜ì •ëœ ë¬¸ì„œ\n",
    "result_3 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup='incremental',\n",
    "    source_id_key='source',\n",
    ")\n",
    "\n",
    "print(\"=== ì¸ë±ì‹± 3íšŒì°¨ (ìˆ˜ì •ëœ ë¬¸ì„œ) ===\")\n",
    "print(result_3)\n",
    "print(\"\\nâ†’ num_added=1, num_deleted=1: ìˆ˜ì •ëœ ë¬¸ì„œê°€ ì—…ë°ì´íŠ¸ë¨!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rFQnDqg4-x2"
   },
   "source": [
    "---\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸ (OpenAI)\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# ë³€ê²½ (Ollama)\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings_model = OllamaEmbeddings(model='nomic-embed-text')\n",
    "```\n",
    "\n",
    "## index() ë°˜í™˜ê°’\n",
    "\n",
    "```python\n",
    "{\n",
    "    'num_added': 2,      # ìƒˆë¡œ ì¶”ê°€ëœ ë¬¸ì„œ ìˆ˜\n",
    "    'num_updated': 0,    # ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œ ìˆ˜\n",
    "    'num_skipped': 0,    # ìŠ¤í‚µëœ ë¬¸ì„œ ìˆ˜ (ì´ë¯¸ ì¡´ì¬)\n",
    "    'num_deleted': 0,    # ì‚­ì œëœ ë¬¸ì„œ ìˆ˜\n",
    "}\n",
    "```\n",
    "\n",
    "## cleanup ëª¨ë“œ ì„ íƒ ê°€ì´ë“œ\n",
    "\n",
    "| ìƒí™© | ì¶”ì²œ ëª¨ë“œ |\n",
    "|------|----------|\n",
    "| ë¬¸ì„œê°€ ìì£¼ ì—…ë°ì´íŠ¸ë¨ | `incremental` |\n",
    "| ì „ì²´ ë¬¸ì„œ ì„¸íŠ¸ë¥¼ êµì²´ | `full` |\n",
    "| ë‹¨ìˆœ ì¶”ê°€ë§Œ í•„ìš” | `None` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHkgZMBj4-x2"
   },
   "source": [
    "---\n",
    "## ì½”ë“œ 2-14 MultiVectorRetrieverë¡œ ìš”ì•½ ê¸°ë°˜ ê²€ìƒ‰ êµ¬í˜„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gof74KB4-x2"
   },
   "source": [
    "# MultiVectorRetrieverë¡œ ìš”ì•½ ê¸°ë°˜ ê²€ìƒ‰ êµ¬í˜„í•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **MultiVectorRetriever**ë¥¼ ì‚¬ìš©í•˜ì—¬ **ìš”ì•½ìœ¼ë¡œ ê²€ìƒ‰**í•˜ê³  **ì›ë³¸ ë¬¸ì„œë¥¼ ë°˜í™˜**í•˜ëŠ” ê³ ê¸‰ RAG íŒ¨í„´ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì¼ë°˜ ê²€ìƒ‰ vs MultiVector ê²€ìƒ‰\n",
    "\n",
    "| ë°©ì‹ | ê²€ìƒ‰ ëŒ€ìƒ | ë°˜í™˜ ê²°ê³¼ |\n",
    "|------|----------|----------|\n",
    "| **ì¼ë°˜** | ì›ë³¸ ì²­í¬ | ì›ë³¸ ì²­í¬ |\n",
    "| **MultiVector** | ìš”ì•½/ì„ë² ë”© | ì›ë³¸ ë¬¸ì„œ |\n",
    "\n",
    "## MultiVectorRetrieverì˜ ì¥ì \n",
    "\n",
    "1. **ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ**: ìš”ì•½ì´ í•µì‹¬ ë‚´ìš©ì„ ë‹´ì•„ ê²€ìƒ‰ í’ˆì§ˆ ê°œì„ \n",
    "2. **í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸**: ê²€ìƒ‰ì€ ìš”ì•½ìœ¼ë¡œ, ì‘ë‹µì€ ì „ì²´ ë¬¸ì„œë¡œ\n",
    "3. **ìœ ì—°í•œ ì €ì¥**: ë²¡í„° ì €ì¥ì†Œ + ë¬¸ì„œ ì €ì¥ì†Œ ë¶„ë¦¬\n",
    "\n",
    "## ì•„í‚¤í…ì²˜\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    MultiVectorRetriever                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚     Vector Store        â”‚         Document Store           â”‚\n",
    "â”‚   (ìš”ì•½ ì„ë² ë”© ì €ì¥)      â”‚       (ì›ë³¸ ë¬¸ì„œ ì €ì¥)            â”‚\n",
    "â”‚                         â”‚                                   â”‚\n",
    "â”‚   ìš”ì•½1 â†’ [ë²¡í„°]         â”‚   doc_id_1 â†’ ì›ë³¸ ë¬¸ì„œ 1          â”‚\n",
    "â”‚   ìš”ì•½2 â†’ [ë²¡í„°]         â”‚   doc_id_2 â†’ ì›ë³¸ ë¬¸ì„œ 2          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "            â”‚                           â”‚\n",
    "            â”‚  1. ì¿¼ë¦¬ë¡œ ìš”ì•½ ê²€ìƒ‰        â”‚  2. doc_idë¡œ ì›ë³¸ ì¡°íšŒ\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Dockerë¡œ PGVector ì‹¤í–‰\n",
    "\n",
    "```bash\n",
    "docker run --name pgvector-container \\\n",
    "    -e POSTGRES_USER=langchain -e POSTGRES_PASSWORD=langchain \\\n",
    "    -e POSTGRES_DB=langchain -p 6024:5432 -d pgvector/pgvector:pg16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HRhtROy4-x2"
   },
   "source": [
    "# 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° Ollama ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "UUHit8oV4-x2",
    "outputId": "e335a09e-b051-41cd-dc84-efae54568341",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n",
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull llama3.2\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgdRgigw4-x2"
   },
   "source": [
    "# 3. í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ekdoeFHw4-x2",
    "outputId": "de535a5e-75c3-4f23-ad15-4efd866818fb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test.txt ìƒì„± ì™„ë£Œ (336ì)\n"
     ]
    }
   ],
   "source": [
    "sample_text = '''LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "í”„ë¡¬í”„íŠ¸ ê´€ë¦¬, ì²´ì¸ êµ¬ì„±, ë°ì´í„° ì—°ë™ ë“± ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ë¡œ, ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ì‘ë‹µì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n",
    "Vector Storeì— ë¬¸ì„œë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ í†µí•´ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "\n",
    "Embeddingì€ í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ í…ìŠ¤íŠ¸ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ìˆìŠµë‹ˆë‹¤.\n",
    "OpenAI, Ollama ë“± ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "'''\n",
    "\n",
    "with open('./test.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(f\"test.txt ìƒì„± ì™„ë£Œ ({len(sample_text)}ì)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqk_m2yW4-x3"
   },
   "source": [
    "# 4. ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ixsqBmLe4-x3",
    "outputId": "b72143ae-f761-4059-c8a0-d55d02131f4e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ì›ë³¸ ë¬¸ì„œ ê¸¸ì´: 336ì\n",
      "ë¶„í• ëœ ì²­í¬ ìˆ˜: 3\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "loader = TextLoader('./test.txt', encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "# ë¬¸ì„œ ë¶„í• \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"ì›ë³¸ ë¬¸ì„œ ê¸¸ì´: {len(docs[0].page_content)}ì\")\n",
    "print(f\"ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRlN0Rr-4-x3"
   },
   "source": [
    "# 5. ê° ì²­í¬ì˜ ìš”ì•½ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "_gq2I7iV4-x3",
    "outputId": "1484a831-38c2-4723-c467-6fb90caf2679",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== ìƒì„±ëœ ìš”ì•½ ===\n",
      "\n",
      "[ì²­í¬ 1] LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ë¡œ, í”„ë¡¬í”„íŠ¸ ê´€ë¦¬, ì²´ì¸ êµ¬ì„±, ë°ì´í„° ì—°ë™ ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "[ì²­í¬ 2] RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ë¡œ, ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ Large Language Model (LLM)ì˜ ì‘ë‹µì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
      "\n",
      "[ì²­í¬ 3] Embeddingì€ í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ, ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ í…ìŠ¤íŠ¸ê°€ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ìœ„ì¹˜í•˜ê²Œ ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ìš”ì•½ ì²´ì¸ ìƒì„±\n",
    "prompt = ChatPromptTemplate.from_template('ë‹¤ìŒ ë¬¸ì„œë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:\\n\\n{doc}')\n",
    "llm = ChatOllama(model='llama3.2', temperature=0)\n",
    "\n",
    "summarize_chain = {'doc': lambda x: x.page_content} | prompt | llm | StrOutputParser()\n",
    "\n",
    "# ê° ì²­í¬ ìš”ì•½ ìƒì„±\n",
    "summaries = summarize_chain.batch(chunks, {'max_concurrency': 2})\n",
    "\n",
    "print(\"=== ìƒì„±ëœ ìš”ì•½ ===\")\n",
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"\\n[ì²­í¬ {i+1}] {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FN1Uugk4-x3"
   },
   "source": [
    "# 6. MultiVectorRetriever ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "2bymVJRz4-x3",
    "outputId": "962a6778-7015-4afb-89af-fc75b06b7651",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… MultiVectorRetriever ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\nfrom langchain_postgres.vectorstores import PGVector\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.docstore.document import Document\nimport uuid\n\n# ì„¤ì •\nconnection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\ncollection_name = 'summaries'\nid_key = 'doc_id'\n\n# ì„ë² ë”© ëª¨ë¸\nembeddings_model = OllamaEmbeddings(model='nomic-embed-text')\n\n# Vector Store (ìš”ì•½ ì €ì¥)\nvectorstore = PGVector(\n    embeddings=embeddings_model,\n    collection_name=collection_name,\n    connection=connection,\n    use_jsonb=True,\n)\n\n# Document Store (ì›ë³¸ ì €ì¥)\nstore = InMemoryStore()\n\n# MultiVectorRetriever ìƒì„±\nretriever = MultiVectorRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    id_key=id_key,\n)\n\nprint(\"âœ… MultiVectorRetriever ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ06Ou1W4-x3"
   },
   "source": [
    "# 7. ìš”ì•½ê³¼ ì›ë³¸ ë¬¸ì„œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "pPMAM11Y4-x3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "578e5cb5-2b76-404a-9bf7-6ad7bc82afcc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… ìš”ì•½ 3ê°œ â†’ Vector Store\n",
      "âœ… ì›ë³¸ 3ê°œ â†’ Document Store\n"
     ]
    }
   ],
   "source": [
    "# ê° ì²­í¬ì— ê³ ìœ  ID ë¶€ì—¬\n",
    "doc_ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "\n",
    "# ìš”ì•½ ë¬¸ì„œ ìƒì„± (doc_idë¡œ ì›ë³¸ê³¼ ì—°ê²°)\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Vector Storeì— ìš”ì•½ ì €ì¥\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "\n",
    "# Document Storeì— ì›ë³¸ ì €ì¥\n",
    "retriever.docstore.mset(list(zip(doc_ids, chunks)))\n",
    "\n",
    "print(f\"âœ… ìš”ì•½ {len(summary_docs)}ê°œ â†’ Vector Store\")\n",
    "print(f\"âœ… ì›ë³¸ {len(chunks)}ê°œ â†’ Document Store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfbMuuc94-x3"
   },
   "source": [
    "# 8. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "HHqT7Z3z4-x3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b816f04d-ae32-490e-e49b-c4f6549d217d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Vector Store ê²€ìƒ‰ ê²°ê³¼ (ìš”ì•½) ===\n",
      "[1] RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ë¡œ, ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ Large Language Model (LLM)ì˜ ì‘ë‹µì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
      "    ê¸¸ì´: 107ì\n",
      "[2] RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ë¡œ, ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ Large Language Model (LLM)ì˜ ì‘ë‹µì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
      "    ê¸¸ì´: 107ì\n"
     ]
    }
   ],
   "source": [
    "query = 'RAGê°€ ë­”ê°€ìš”?'\n",
    "\n",
    "# Vector Storeì—ì„œ ìš”ì•½ ê²€ìƒ‰\n",
    "sub_docs = retriever.vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(\"=== Vector Store ê²€ìƒ‰ ê²°ê³¼ (ìš”ì•½) ===\")\n",
    "for i, doc in enumerate(sub_docs):\n",
    "    print(f\"[{i+1}] {doc.page_content}\")\n",
    "    print(f\"    ê¸¸ì´: {len(doc.page_content)}ì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "yhKXqIe84-x3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dcce52be-53cb-4bd3-fd77-9626c1c9d2ad"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== Retriever ê²€ìƒ‰ ê²°ê³¼ (ì›ë³¸) ===\n",
      "\n",
      "[1] RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ë¡œ, ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ì‘ë‹µì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n",
      "Vector Storeì— ë¬¸ì„œë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ í†µí•´ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
      "    ê¸¸ì´: 126ì\n",
      "\n",
      "[2] LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
      "í”„ë¡¬í”„íŠ¸ ê´€ë¦¬, ì²´ì¸ êµ¬ì„±, ë°ì´í„° ì—°ë™ ë“± ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "    ê¸¸ì´: 94ì\n"
     ]
    }
   ],
   "source": [
    "# MultiVectorRetrieverë¡œ ê²€ìƒ‰ (ì›ë³¸ ë°˜í™˜)\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"\\n=== Retriever ê²€ìƒ‰ ê²°ê³¼ (ì›ë³¸) ===\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n[{i+1}] {doc.page_content}\")\n",
    "    print(f\"    ê¸¸ì´: {len(doc.page_content)}ì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSNridUu4-x3"
   },
   "source": [
    "---\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸ (OpenAI)\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-4o-mini')\n",
    "\n",
    "# ë³€ê²½ (Ollama)\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "embeddings_model = OllamaEmbeddings(model='nomic-embed-text')\n",
    "llm = ChatOllama(model='llama3.2', temperature=0)\n",
    "```\n",
    "\n",
    "## MultiVectorRetriever í™œìš© íŒ¨í„´\n",
    "\n",
    "| íŒ¨í„´ | Vector Store ì €ì¥ | Document Store ì €ì¥ |\n",
    "|------|------------------|--------------------|\n",
    "| **ìš”ì•½ ê¸°ë°˜** | ë¬¸ì„œ ìš”ì•½ | ì›ë³¸ ë¬¸ì„œ |\n",
    "| **ì§ˆë¬¸ ê¸°ë°˜** | ê°€ìƒ ì§ˆë¬¸ë“¤ | ì›ë³¸ ë¬¸ì„œ |\n",
    "| **ì‘ì€ ì²­í¬** | ì‘ì€ ì²­í¬ | í° ì²­í¬/ì „ì²´ ë¬¸ì„œ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnnkMP9O4-x3"
   },
   "source": [
    "---\n",
    "## ì½”ë“œ 2-15 RAGatouille + ColBERTë¡œ ê³ ê¸‰ ê²€ìƒ‰ êµ¬í˜„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJvg2Jwe4-x3"
   },
   "source": [
    "# RAGatouille + ColBERTë¡œ ê³ ê¸‰ ê²€ìƒ‰ êµ¬í˜„í•˜ê¸°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **RAGatouille**ê³¼ **ColBERT** ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê³ ê¸‰ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
    "\n",
    "## ColBERTë€?\n",
    "\n",
    "**ColBERT**(Contextualized Late Interaction over BERT)ëŠ” íš¨ìœ¨ì ì¸ ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ ì‹ ê²½ë§ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ì¼ë°˜ ì„ë² ë”© vs ColBERT\n",
    "\n",
    "| ë°©ì‹ | ì„ë² ë”© | ê²€ìƒ‰ ë°©ì‹ |\n",
    "|------|--------|----------|\n",
    "| **ì¼ë°˜** | ë¬¸ì„œ â†’ ë‹¨ì¼ ë²¡í„° | ë²¡í„° ìœ ì‚¬ë„ |\n",
    "| **ColBERT** | ë¬¸ì„œ â†’ í† í°ë³„ ë²¡í„° | Late Interaction |\n",
    "\n",
    "### ColBERTì˜ ì¥ì \n",
    "\n",
    "1. **ë†’ì€ ì •í™•ë„**: í† í° ìˆ˜ì¤€ ë§¤ì¹­ìœ¼ë¡œ ì„¸ë°€í•œ ê²€ìƒ‰\n",
    "2. **íš¨ìœ¨ì„±**: ì‚¬ì „ ê³„ì‚°ëœ ì„ë² ë”©ìœ¼ë¡œ ë¹ ë¥¸ ê²€ìƒ‰\n",
    "3. **ì„¤ëª… ê°€ëŠ¥ì„±**: ì–´ë–¤ í† í°ì´ ë§¤ì¹­ë˜ì—ˆëŠ”ì§€ í™•ì¸ ê°€ëŠ¥\n",
    "\n",
    "## RAGatouilleì´ë€?\n",
    "\n",
    "ColBERTë¥¼ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "```\n",
    "\n",
    "## ì£¼ì˜ì‚¬í•­\n",
    "\n",
    "- **Windows ë¯¸ì§€ì›**\n",
    "- Python ì „ìš©\n",
    "- GPU ê¶Œì¥ (CPUë„ ê°€ëŠ¥í•˜ì§€ë§Œ ëŠë¦¼)\n",
    "\n",
    "---\n",
    "\n",
    "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JG-5dmZ4-x3"
   },
   "source": [
    "# 2. ColBERT ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade ragatouille\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y0PM8SdNMjRj",
    "outputId": "c7812b30-e94e-45ed-f041-e90dd63bedbd"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: ragatouille in /usr/local/lib/python3.12/dist-packages (0.0.9.post2)\n",
      "Requirement already satisfied: llama-index in /usr/local/lib/python3.12/dist-packages (from ragatouille) (0.14.14)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (from ragatouille) (1.13.2)\n",
      "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (from ragatouille) (1.2.12)\n",
      "Requirement already satisfied: colbert-ai>=0.2.19 in /usr/local/lib/python3.12/dist-packages (from ragatouille) (0.2.22)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (from ragatouille) (1.2.10)\n",
      "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (from ragatouille) (1.20.1)\n",
      "Requirement already satisfied: srsly in /usr/local/lib/python3.12/dist-packages (from ragatouille) (2.5.2)\n",
      "Requirement already satisfied: voyager in /usr/local/lib/python3.12/dist-packages (from ragatouille) (2.1.0)\n",
      "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.12/dist-packages (from ragatouille) (2.9.0+cpu)\n",
      "Requirement already satisfied: fast-pytorch-kmeans in /usr/local/lib/python3.12/dist-packages (from ragatouille) (0.2.2)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (from ragatouille) (5.2.2)\n",
      "Requirement already satisfied: bitarray in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (3.8.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (4.0.0)\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (3.1.2)\n",
      "Requirement already satisfied: GitPython in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (3.1.46)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (1.2.1)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (1.13.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (1.16.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (4.67.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (5.0.0)\n",
      "Requirement already satisfied: ujson in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (5.11.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (82.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (2025.3.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu->ragatouille) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu->ragatouille) (26.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langchain->ragatouille) (1.0.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain->ragatouille) (2.12.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille) (0.6.9)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille) (9.1.3)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille) (0.14.0)\n",
      "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.5.3)\n",
      "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.14 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.14.14)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.5.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.9.4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.6.18)\n",
      "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.5.6)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.5.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (3.9.1)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx->ragatouille) (5.29.6)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx->ragatouille) (0.5.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->ragatouille) (1.4.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->ragatouille) (1.6.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from srsly->ragatouille) (2.0.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->ragatouille) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->ragatouille) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->ragatouille) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->ragatouille) (0.21.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core->ragatouille) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->ragatouille) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->ragatouille) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->ragatouille) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain->ragatouille) (3.6.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (0.25.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (3.13.3)\n",
      "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (0.22.1)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (2.4.0)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (1.2.0)\n",
      "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (2.14.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (1.6.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (4.5.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (2.0.46)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (0.12.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (1.17.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index->ragatouille) (2.17.0)\n",
      "Requirement already satisfied: llama-cloud==0.1.35 in /usr/local/lib/python3.12/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille) (0.1.35)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille) (2026.1.4)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (4.13.5)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (0.7.1)\n",
      "Requirement already satisfied: pandas<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (2.2.2)\n",
      "Requirement already satisfied: pypdf<7,>=6.1.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (6.7.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille) (0.6.54)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index->ragatouille) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index->ragatouille) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index->ragatouille) (2025.11.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (0.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13->ragatouille) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille) (0.7.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (0.3.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (0.70.16)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (2.2.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (3.0.3)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (3.1.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from GitPython->colbert-ai>=0.2.19->ragatouille) (4.0.12)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers->ragatouille) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (1.22.0)\n",
      "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (2.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (2.8.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->GitPython->colbert-ai>=0.2.19->ragatouille) (5.0.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->ragatouille) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->ragatouille) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->ragatouille) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers->ragatouille) (0.16.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain->ragatouille) (1.12.2)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (0.4.2)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.54 in /usr/local/lib/python3.12/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille) (0.6.54)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index->ragatouille) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index->ragatouille) (0.13.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index->ragatouille) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (2025.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (3.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (3.26.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (1.17.0)\n",
      "Requirement already satisfied: griffecli==2.0.0 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (2.0.0)\n",
      "Requirement already satisfied: griffelib==2.0.0 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (2.0.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffecli==2.0.0->griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.14->llama-index->ragatouille) (0.4.6)\n",
      "Collecting langchain==0.0.354\n",
      "  Using cached langchain-0.0.354-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.354) (6.0.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.354) (2.0.46)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.354) (3.13.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.354) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.354) (1.33)\n",
      "Collecting langchain-community<0.1,>=0.0.8 (from langchain==0.0.354)\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.5 (from langchain==0.0.354)\n",
      "  Downloading langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain==0.0.354)\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting numpy<2,>=1 (from langchain==0.0.354)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.354) (2.12.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.354) (2.32.5)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.354)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.354) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.354) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.354) (3.0.0)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-community<0.1,>=0.0.8 (from langchain==0.0.354)\n",
      "  Downloading langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n",
      "  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
      "  Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n",
      "  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
      "  Downloading langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-core<0.2,>=0.1.5 (from langchain==0.0.354)\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Downloading langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.2,>=0.1.5->langchain==0.0.354) (4.12.1)\n",
      "  Downloading langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain==0.0.354)\n",
      "  Downloading langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.5->langchain==0.0.354)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.354) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.354) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.354) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.354) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.354) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.354) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.354) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.354) (2026.1.4)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.354) (3.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.354) (1.1.0)\n",
      "Downloading langchain-0.0.354-py3-none-any.whl (803 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.3/803.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tenacity, packaging, numpy, langsmith, langchain-core, langchain-community, langchain\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.1.3\n",
      "    Uninstalling tenacity-9.1.3:\n",
      "      Successfully uninstalled tenacity-9.1.3\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 26.0\n",
      "    Uninstalling packaging-26.0:\n",
      "      Successfully uninstalled packaging-26.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.6.9\n",
      "    Uninstalling langsmith-0.6.9:\n",
      "      Successfully uninstalled langsmith-0.6.9\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 1.2.12\n",
      "    Uninstalling langchain-core-1.2.12:\n",
      "      Successfully uninstalled langchain-core-1.2.12\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.4.1\n",
      "    Uninstalling langchain-community-0.4.1:\n",
      "      Successfully uninstalled langchain-community-0.4.1\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 1.2.10\n",
      "    Uninstalling langchain-1.2.10:\n",
      "      Successfully uninstalled langchain-1.2.10\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-postgres 0.0.16 requires langchain-core<2.0,>=0.2.13, but you have langchain-core 0.1.23 which is incompatible.\n",
      "langchain-classic 1.0.1 requires langchain-core<2.0.0,>=1.2.5, but you have langchain-core 0.1.23 which is incompatible.\n",
      "langchain-classic 1.0.1 requires langsmith<1.0.0,>=0.1.17, but you have langsmith 0.0.87 which is incompatible.\n",
      "langchain-text-splitters 1.1.0 requires langchain-core<2.0.0,>=1.2.0, but you have langchain-core 0.1.23 which is incompatible.\n",
      "langchain-ollama 1.0.1 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.1.23 which is incompatible.\n",
      "db-dtypes 1.5.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
      "wheel 0.46.3 requires packaging>=24.0, but you have packaging 23.2 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "xarray 2025.12.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
      "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
      "langgraph-prebuilt 1.0.7 requires langchain-core>=1.0.0, but you have langchain-core 0.1.23 which is incompatible.\n",
      "opencv-contrib-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "google-cloud-bigquery 3.40.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "langgraph-checkpoint 4.0.0 requires langchain-core>=0.2.38, but you have langchain-core 0.1.23 which is incompatible.\n",
      "google-adk 1.24.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-0.0.354 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 numpy-1.26.4 packaging-23.2 tenacity-8.5.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "langchain",
         "numpy",
         "packaging"
        ]
       },
       "id": "ddf2156b1a964b379f98e576ec772041"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1tL1nqXg4-x3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 776
    },
    "outputId": "27ace47e-01a2-4590-c12b-5f421012cbc5"
   },
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "# ColBERT v2 ëª¨ë¸ ë¡œë“œ\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "print(\"âœ… ColBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nznCY8j24-x3"
   },
   "source": [
    "# 3. ìœ„í‚¤ë°±ê³¼ì—ì„œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nq0mVG_X4-x3"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    ìœ„í‚¤ë°±ê³¼ì˜ í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.\n",
    "\n",
    "    :param title: ìœ„í‚¤ë°±ê³¼ í˜ì´ì§€ì˜ ì œëª©\n",
    "    :return: í˜ì´ì§€ì˜ ì „ì²´ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "# ë¯¸ì•¼ìí‚¤ í•˜ì•¼ì˜¤ ìœ„í‚¤ë°±ê³¼ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")\n",
    "\n",
    "print(f\"ë¬¸ì„œ ê¸¸ì´: {len(full_document)}ì\")\n",
    "print(f\"\\në¯¸ë¦¬ë³´ê¸°:\\n{full_document[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLQG03Fr4-x3"
   },
   "source": [
    "# 4. ì¸ë±ìŠ¤ ìƒì„±\n",
    "\n",
    "**ì½”ë“œ ì„¤ëª…:**\n",
    "\n",
    "```python\n",
    "RAG.index(\n",
    "    collection=[document],     # ì¸ë±ì‹±í•  ë¬¸ì„œë“¤\n",
    "    index_name=\"index_name\",   # ì¸ë±ìŠ¤ ì´ë¦„\n",
    "    max_document_length=180,   # ì²­í¬ ìµœëŒ€ ê¸¸ì´\n",
    "    split_documents=True,      # ìë™ ë¶„í•  ì—¬ë¶€\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2aFnGav4-x3"
   },
   "outputs": [],
   "source": [
    "# ColBERT ì¸ë±ìŠ¤ ìƒì„±\n",
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-index\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")\n",
    "\n",
    "print(\"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QAUGAs34-x3"
   },
   "source": [
    "# 5. ColBERTë¡œ ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFFZCirG4-x3"
   },
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰ ìˆ˜í–‰\n",
    "query = \"What animation studio did Miyazaki found?\"\n",
    "results = RAG.search(query=query, k=3)\n",
    "\n",
    "print(f\"ì¿¼ë¦¬: '{query}'\\n\")\n",
    "print(\"=== ê²€ìƒ‰ ê²°ê³¼ ===\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] Score: {result['score']:.4f}\")\n",
    "    print(f\"    {result['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y14Mq3Gu4-x3"
   },
   "source": [
    "# 6. LangChain Retrieverë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bg5TzdJg4-x3"
   },
   "outputs": [],
   "source": [
    "# LangChain Retrieverë¡œ ë³€í™˜\n",
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "\n",
    "# Retriever ì‚¬ìš©\n",
    "docs = retriever.invoke(\"What animation studio did Miyazaki found?\")\n",
    "\n",
    "print(\"=== LangChain Retriever ê²°ê³¼ ===\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n[{i+1}] {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scAdhDlm4-x3"
   },
   "source": [
    "# 7. RAG ì²´ì¸ì—ì„œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4PGBtNc4-x3"
   },
   "outputs": [],
   "source": [
    "# Ollama ì„¤ì¹˜ (RAG ì²´ì¸ìš©)\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "!ollama pull llama3.2\n",
    "!pip install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9geCwZo4-x3"
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG ì²´ì¸ êµ¬ì„±\n",
    "llm = ChatOllama(model='llama3.2')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''Answer the question based on the context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:''')\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# RAG ì²´ì¸ ì‹¤í–‰\n",
    "answer = rag_chain.invoke(\"What animation studio did Miyazaki found?\")\n",
    "\n",
    "print(\"=== RAG ì‘ë‹µ ===\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54kWhpyn4-x3"
   },
   "source": [
    "---\n",
    "\n",
    "## RAGatouille ì£¼ìš” ë©”ì„œë“œ\n",
    "\n",
    "| ë©”ì„œë“œ | ì„¤ëª… |\n",
    "|--------|------|\n",
    "| `from_pretrained()` | ì‚¬ì „í›ˆë ¨ ëª¨ë¸ ë¡œë“œ |\n",
    "| `index()` | ë¬¸ì„œ ì¸ë±ì‹± |\n",
    "| `search()` | ê²€ìƒ‰ ìˆ˜í–‰ |\n",
    "| `as_langchain_retriever()` | LangChain Retriever ë³€í™˜ |\n",
    "\n",
    "## ì¼ë°˜ ì„ë² ë”© vs ColBERT ë¹„êµ\n",
    "\n",
    "| íŠ¹ì„± | ì¼ë°˜ ì„ë² ë”© | ColBERT |\n",
    "|------|------------|--------|\n",
    "| ì„ë² ë”© ì°¨ì› | ë¬¸ì„œë‹¹ 1ê°œ ë²¡í„° | í† í°ë‹¹ 1ê°œ ë²¡í„° |\n",
    "| ê²€ìƒ‰ ì •í™•ë„ | ë³´í†µ | ë†’ìŒ |\n",
    "| ì¸ë±ìŠ¤ í¬ê¸° | ì‘ìŒ | í¼ |\n",
    "| ê²€ìƒ‰ ì†ë„ | ë¹ ë¦„ | ì•½ê°„ ëŠë¦¼ |\n",
    "\n",
    "## ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤\n",
    "\n",
    "- **ë†’ì€ ì •í™•ë„ í•„ìš”**: ë²•ë¥ , ì˜ë£Œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "- **ì„¸ë°€í•œ ë§¤ì¹­ í•„ìš”**: ê¸°ìˆ  ë¬¸ì„œ, ì½”ë“œ ê²€ìƒ‰\n",
    "- **ì„¤ëª… ê°€ëŠ¥ì„± í•„ìš”**: ê²€ìƒ‰ ê²°ê³¼ ê·¼ê±° ì œì‹œ"
   ]
  }
 ]
}