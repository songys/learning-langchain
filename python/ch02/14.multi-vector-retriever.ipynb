{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiVectorRetriever로 요약 기반 검색 구현하기\n",
    "\n",
    "이 노트북에서는 **MultiVectorRetriever**를 사용하여 **요약으로 검색**하고 **원본 문서를 반환**하는 고급 RAG 패턴을 구현합니다.\n",
    "\n",
    "## 일반 검색 vs MultiVector 검색\n",
    "\n",
    "| 방식 | 검색 대상 | 반환 결과 |\n",
    "|------|----------|----------|\n",
    "| **일반** | 원본 청크 | 원본 청크 |\n",
    "| **MultiVector** | 요약/임베딩 | 원본 문서 |\n",
    "\n",
    "## MultiVectorRetriever의 장점\n",
    "\n",
    "1. **검색 정확도 향상**: 요약이 핵심 내용을 담아 검색 품질 개선\n",
    "2. **풍부한 컨텍스트**: 검색은 요약으로, 응답은 전체 문서로\n",
    "3. **유연한 저장**: 벡터 저장소 + 문서 저장소 분리\n",
    "\n",
    "## 아키텍처\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    MultiVectorRetriever                    │\n",
    "├─────────────────────────┬───────────────────────────────────┤\n",
    "│     Vector Store        │         Document Store           │\n",
    "│   (요약 임베딩 저장)      │       (원본 문서 저장)            │\n",
    "│                         │                                   │\n",
    "│   요약1 → [벡터]         │   doc_id_1 → 원본 문서 1          │\n",
    "│   요약2 → [벡터]         │   doc_id_2 → 원본 문서 2          │\n",
    "└─────────────────────────┴───────────────────────────────────┘\n",
    "            │                           │\n",
    "            │  1. 쿼리로 요약 검색        │  2. doc_id로 원본 조회\n",
    "            └───────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Docker로 PGVector 실행\n",
    "\n",
    "```bash\n",
    "docker run --name pgvector-container \\\n",
    "    -e POSTGRES_USER=langchain -e POSTGRES_PASSWORD=langchain \\\n",
    "    -e POSTGRES_DB=langchain -p 6024:5432 -d pgvector/pgvector:pg16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 패키지 설치 및 Ollama 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-community langchain-postgres langchain-ollama langchain-text-splitters psycopg psycopg-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull llama3.2\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 테스트 문서 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '''LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 프레임워크입니다.\n",
    "프롬프트 관리, 체인 구성, 데이터 연동 등 다양한 기능을 제공합니다.\n",
    "\n",
    "RAG(Retrieval-Augmented Generation)는 검색 증강 생성 기술로, 외부 지식을 검색하여 LLM의 응답을 향상시킵니다.\n",
    "Vector Store에 문서를 저장하고 유사도 검색을 통해 관련 정보를 찾습니다.\n",
    "\n",
    "Embedding은 텍스트를 고차원 벡터로 변환하는 과정입니다. 의미가 비슷한 텍스트는 벡터 공간에서 가까운 위치에 있습니다.\n",
    "OpenAI, Ollama 등 다양한 임베딩 모델을 사용할 수 있습니다.\n",
    "'''\n",
    "\n",
    "with open('./test.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(f\"test.txt 생성 완료 ({len(sample_text)}자)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 문서 로드 및 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 문서 로드\n",
    "loader = TextLoader('./test.txt', encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 분할\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"원본 문서 길이: {len(docs[0].page_content)}자\")\n",
    "print(f\"분할된 청크 수: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 각 청크의 요약 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 요약 체인 생성\n",
    "prompt = ChatPromptTemplate.from_template('다음 문서를 한 문장으로 요약하세요:\\n\\n{doc}')\n",
    "llm = ChatOllama(model='llama3.2', temperature=0)\n",
    "\n",
    "summarize_chain = {'doc': lambda x: x.page_content} | prompt | llm | StrOutputParser()\n",
    "\n",
    "# 각 청크 요약 생성\n",
    "summaries = summarize_chain.batch(chunks, {'max_concurrency': 2})\n",
    "\n",
    "print(\"=== 생성된 요약 ===\")\n",
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"\\n[청크 {i+1}] {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. MultiVectorRetriever 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "import uuid\n",
    "\n",
    "# 설정\n",
    "connection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
    "collection_name = 'summaries'\n",
    "id_key = 'doc_id'\n",
    "\n",
    "# 임베딩 모델\n",
    "embeddings_model = OllamaEmbeddings(model='nomic-embed-text')\n",
    "\n",
    "# Vector Store (요약 저장)\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "# Document Store (원본 저장)\n",
    "store = InMemoryStore()\n",
    "\n",
    "# MultiVectorRetriever 생성\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "print(\"✅ MultiVectorRetriever 설정 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 요약과 원본 문서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 청크에 고유 ID 부여\n",
    "doc_ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "\n",
    "# 요약 문서 생성 (doc_id로 원본과 연결)\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Vector Store에 요약 저장\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "\n",
    "# Document Store에 원본 저장\n",
    "retriever.docstore.mset(list(zip(doc_ids, chunks)))\n",
    "\n",
    "print(f\"✅ 요약 {len(summary_docs)}개 → Vector Store\")\n",
    "print(f\"✅ 원본 {len(chunks)}개 → Document Store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 검색 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'RAG가 뭔가요?'\n",
    "\n",
    "# Vector Store에서 요약 검색\n",
    "sub_docs = retriever.vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(\"=== Vector Store 검색 결과 (요약) ===\")\n",
    "for i, doc in enumerate(sub_docs):\n",
    "    print(f\"[{i+1}] {doc.page_content}\")\n",
    "    print(f\"    길이: {len(doc.page_content)}자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiVectorRetriever로 검색 (원본 반환)\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"\\n=== Retriever 검색 결과 (원본) ===\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n[{i+1}] {doc.page_content}\")\n",
    "    print(f\"    길이: {len(doc.page_content)}자\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 코드 변경점 (OpenAI → Ollama)\n",
    "\n",
    "```python\n",
    "# 원본 (OpenAI)\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-4o-mini')\n",
    "\n",
    "# 변경 (Ollama)\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "embeddings_model = OllamaEmbeddings(model='nomic-embed-text')\n",
    "llm = ChatOllama(model='llama3.2', temperature=0)\n",
    "```\n",
    "\n",
    "## MultiVectorRetriever 활용 패턴\n",
    "\n",
    "| 패턴 | Vector Store 저장 | Document Store 저장 |\n",
    "|------|------------------|--------------------|\n",
    "| **요약 기반** | 문서 요약 | 원본 문서 |\n",
    "| **질문 기반** | 가상 질문들 | 원본 문서 |\n",
    "| **작은 청크** | 작은 청크 | 큰 청크/전체 문서 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
