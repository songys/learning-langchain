{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL 체인에서 스트리밍 사용하기\n",
    "\n",
    "이 노트북에서는 **LCEL 선언적 체인**에서 **스트리밍**을 사용하는 방법을 알아봅니다.\n",
    "\n",
    "## LCEL 스트리밍의 장점\n",
    "\n",
    "LCEL로 구성한 체인은 **자동으로 스트리밍을 지원**합니다.\n",
    "\n",
    "```python\n",
    "chain = template | model\n",
    "\n",
    "# 별도 설정 없이 바로 스트리밍 가능!\n",
    "for chunk in chain.stream(inputs):\n",
    "    print(chunk)\n",
    "```\n",
    "\n",
    "## @chain + yield vs LCEL stream() 비교\n",
    "\n",
    "| 방식 | 코드 | 특징 |\n",
    "|------|------|------|\n",
    "| **@chain + yield** | 함수 내부에 `yield` 작성 | 명시적, 커스텀 가능 |\n",
    "| **LCEL stream()** | `chain.stream()` 호출 | 자동, 간편함 |\n",
    "\n",
    "```python\n",
    "# @chain + yield (12번 노트북)\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token\n",
    "\n",
    "# LCEL stream() (이 노트북)\n",
    "chain = template | model\n",
    "for token in chain.stream(inputs):  # 자동 지원!\n",
    "    print(token)\n",
    "```\n",
    "\n",
    "## 스트리밍 지원 컴포넌트\n",
    "\n",
    "모든 LangChain 컴포넌트가 스트리밍을 지원하는 것은 아닙니다:\n",
    "\n",
    "| 컴포넌트 | 스트리밍 지원 | 동작 |\n",
    "|----------|--------------|------|\n",
    "| ChatModel | O | 토큰 단위 스트리밍 |\n",
    "| LLM | O | 토큰 단위 스트리밍 |\n",
    "| PromptTemplate | X | 최종 결과만 전달 |\n",
    "| OutputParser | 일부 | 파서에 따라 다름 |\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Ollama 설치 및 서버 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# zstd 설치 (Ollama 설치의 사전 요구 사항)\n",
    "!apt-get install -y zstd\n",
    "\n",
    "# Ollama 설치\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# 백그라운드에서 Ollama 서버 실행\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 다운로드 & 패키지 설치\n",
    "\n",
    "- `ollama pull llama3.2` - Llama 3.2 모델 다운로드\n",
    "- `pip install langchain-ollama` - LangChain Ollama 통합 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2\n",
    "!pip install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 구성 요소 준비 및 체인 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Prompt Template\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '당신은 친절한 어시스턴트입니다.'),\n",
    "    ('human', '{question}'),\n",
    "])\n",
    "\n",
    "# Model\n",
    "model = ChatOllama(model='llama3.2')\n",
    "\n",
    "# LCEL로 체인 구성\n",
    "chatbot = template | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LCEL 체인 스트리밍 실행\n",
    "\n",
    "**핵심 코드:**\n",
    "\n",
    "```python\n",
    "for part in chatbot.stream({'question': '...'}):\n",
    "    print(part)\n",
    "```\n",
    "\n",
    "- LCEL 체인은 **자동으로** 스트리밍 지원\n",
    "- 별도의 `yield` 코드 작성 불필요\n",
    "- 각 `part`는 **AIMessageChunk** 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL 체인 스트리밍\n",
    "print(\"=== LCEL 스트리밍 (전체 객체) ===\")\n",
    "\n",
    "for part in chatbot.stream({'question': '거대 언어 모델은 어디서 제공하나요?'}):\n",
    "    print(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 실시간 텍스트 출력\n",
    "\n",
    "ChatGPT처럼 글자가 하나씩 나타나는 효과를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실시간 텍스트 출력\n",
    "print(\"=== 실시간 텍스트 출력 ===\")\n",
    "\n",
    "for part in chatbot.stream({'question': 'Python의 장점 3가지를 알려주세요.'}):\n",
    "    print(part.content, end='', flush=True)\n",
    "\n",
    "print()  # 줄바꿈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 각 chunk 상세 확인\n",
    "\n",
    "스트리밍에서 각 `part`의 구조를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 chunk 상세 분석\n",
    "print(\"=== 각 chunk 상세 ===\")\n",
    "\n",
    "chunks = []\n",
    "for i, part in enumerate(chatbot.stream({'question': '안녕!'})):\n",
    "    chunks.append(part)\n",
    "    if i < 5:  # 처음 5개만 출력\n",
    "        print(f\"[{i}] type: {type(part).__name__}\")\n",
    "        print(f\"    content: '{part.content}'\")\n",
    "        print()\n",
    "\n",
    "print(f\"... 총 {len(chunks)}개 chunks\")\n",
    "print(f\"\\n전체 응답: {''.join([c.content for c in chunks])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. StrOutputParser와 함께 스트리밍\n",
    "\n",
    "`StrOutputParser`를 추가하면 AIMessageChunk 대신 **문자열**로 스트리밍됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# OutputParser 추가 체인\n",
    "chain_with_parser = template | model | StrOutputParser()\n",
    "\n",
    "print(\"=== StrOutputParser 스트리밍 ===\")\n",
    "\n",
    "for part in chain_with_parser.stream({'question': 'Hello!'}):\n",
    "    print(f\"type: {type(part).__name__}, value: '{part}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StrOutputParser로 깔끔한 실시간 출력\n",
    "print(\"=== StrOutputParser 실시간 출력 ===\")\n",
    "\n",
    "for text in chain_with_parser.stream({'question': 'LangChain이 뭔가요?'}):\n",
    "    print(text, end='', flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 비동기 스트리밍 (astream)\n",
    "\n",
    "비동기 환경에서는 `astream()`을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비동기 스트리밍\n",
    "print(\"=== 비동기 스트리밍 (astream) ===\")\n",
    "\n",
    "async for part in chatbot.astream({'question': '짧게 인사해주세요.'}):\n",
    "    print(part.content, end='', flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 코드 변경점 (OpenAI → Ollama)\n",
    "\n",
    "```python\n",
    "# 원본 (OpenAI)\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 변경 (Ollama)\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model='llama3.2')\n",
    "```\n",
    "\n",
    "## 스트리밍 방식 비교 정리\n",
    "\n",
    "| 노트북 | 방식 | 코드 | 특징 |\n",
    "|--------|------|------|------|\n",
    "| 10 | 모델 직접 | `model.stream()` | 가장 기본 |\n",
    "| 12 | @chain + yield | `for token in model.stream(): yield token` | 커스텀 가능 |\n",
    "| **15** | **LCEL stream** | **`chain.stream()`** | **자동, 간편** |\n",
    "\n",
    "## LCEL 스트리밍 추천 패턴\n",
    "\n",
    "### 기본 패턴\n",
    "```python\n",
    "chain = prompt | model\n",
    "for chunk in chain.stream(inputs):\n",
    "    print(chunk.content, end='')\n",
    "```\n",
    "\n",
    "### 문자열로 변환\n",
    "```python\n",
    "chain = prompt | model | StrOutputParser()\n",
    "for text in chain.stream(inputs):\n",
    "    print(text, end='')\n",
    "```\n",
    "\n",
    "### 비동기 스트리밍\n",
    "```python\n",
    "async for chunk in chain.astream(inputs):\n",
    "    print(chunk.content, end='')\n",
    "```\n",
    "\n",
    "## 언제 어떤 방식을 사용할까?\n",
    "\n",
    "| 상황 | 추천 방식 |\n",
    "|------|----------|\n",
    "| 단순 스트리밍 | **LCEL `chain.stream()`** |\n",
    "| 스트리밍 중 토큰 변환 | @chain + yield |\n",
    "| 스트리밍 중 로깅 | @chain + yield |\n",
    "| 웹 서버 (FastAPI) | LCEL `chain.astream()` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
