{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @chain 데코레이터와 스트리밍 결합하기\n",
    "\n",
    "이 노트북에서는 **@chain 데코레이터**와 **스트리밍(yield)**을 함께 사용하여 토큰 단위로 응답을 전달하는 방법을 알아봅니다.\n",
    "\n",
    "## 스트리밍이 필요한 이유\n",
    "\n",
    "LLM 응답은 생성에 시간이 걸립니다. 스트리밍을 사용하면:\n",
    "\n",
    "1. **체감 대기 시간 감소**: 첫 토큰이 생성되면 바로 표시\n",
    "2. **사용자 경험 향상**: ChatGPT처럼 글자가 하나씩 나타나는 효과\n",
    "3. **실시간 피드백**: 응답이 진행 중임을 사용자가 인지\n",
    "\n",
    "## @chain + 스트리밍 원리\n",
    "\n",
    "```python\n",
    "@chain\n",
    "def my_chain(values):\n",
    "    for token in model.stream(prompt):\n",
    "        yield token  # return 대신 yield 사용!\n",
    "```\n",
    "\n",
    "**핵심:**\n",
    "- `return` 대신 **`yield`** 사용 → 제너레이터 함수\n",
    "- `model.stream()`의 각 토큰을 **그대로 전달**\n",
    "- `chatbot.stream()`으로 호출하면 토큰 단위로 수신\n",
    "\n",
    "## 실행 흐름\n",
    "\n",
    "```\n",
    "┌──────────────┐     ┌──────────────┐     ┌──────────────┐     ┌──────────────┐\n",
    "│   입력값      │ ──▶ │   Prompt     │ ──▶ │ model.stream │ ──▶ │    yield     │\n",
    "│  {question}  │     │   생성       │     │   (토큰)     │     │   (토큰)     │\n",
    "└──────────────┘     └──────────────┘     └──────────────┘     └──────────────┘\n",
    "                                                                      │\n",
    "                                                      ┌───────────────┘\n",
    "                                                      ▼\n",
    "                                          chatbot.stream() 호출 측에서\n",
    "                                          토큰을 하나씩 수신\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Ollama 설치 및 서버 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# zstd 설치 (Ollama 설치의 사전 요구 사항)\n",
    "!apt-get install -y zstd\n",
    "\n",
    "# Ollama 설치\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# 백그라운드에서 Ollama 서버 실행\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 다운로드 & 패키지 설치\n",
    "\n",
    "- `ollama pull llama3.2` - Llama 3.2 모델 다운로드\n",
    "- `pip install langchain-ollama` - LangChain Ollama 통합 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2\n",
    "!pip install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 구성 요소 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Model\n",
    "model = ChatOllama(model='llama3.2')\n",
    "\n",
    "# Prompt Template\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant.'),\n",
    "    ('human', '{question}'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. @chain + yield로 스트리밍 체인 구성\n",
    "\n",
    "**코드 설명:**\n",
    "\n",
    "### yield 키워드의 역할\n",
    "```python\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token  # 각 토큰을 즉시 반환\n",
    "```\n",
    "\n",
    "- `yield`는 함수를 **제너레이터**로 만듦\n",
    "- 각 토큰이 생성될 때마다 **즉시** 호출 측으로 전달\n",
    "- 전체 응답을 기다리지 않고 **실시간 처리** 가능\n",
    "\n",
    "### return vs yield 차이\n",
    "\n",
    "| 키워드 | 동작 | 결과 |\n",
    "|--------|------|------|\n",
    "| `return` | 모든 토큰 수집 후 반환 | 전체 응답 한 번에 |\n",
    "| `yield` | 각 토큰 즉시 반환 | 토큰 단위 스트리밍 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "# 스트리밍을 지원하는 체인\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token  # return 대신 yield!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 스트리밍 실행\n",
    "\n",
    "`chatbot.stream()`을 호출하면 토큰이 하나씩 전달됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 실행 - 토큰 단위 출력\n",
    "print(\"=== 스트리밍 출력 ===\")\n",
    "\n",
    "for part in chatbot.stream({'question': '거대 언어 모델은 어디서 제공하나요?'}):\n",
    "    print(part.content, end='', flush=True)\n",
    "\n",
    "print()  # 줄바꿈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 각 토큰(chunk) 상세 확인\n",
    "\n",
    "스트리밍에서 각 `part`가 어떤 구조인지 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 chunk 상세 확인\n",
    "print(\"=== 각 chunk 상세 ===\")\n",
    "\n",
    "chunks = []\n",
    "for i, part in enumerate(chatbot.stream({'question': '안녕!'})):\n",
    "    chunks.append(part.content)\n",
    "    if i < 10:  # 처음 10개만 출력\n",
    "        print(f\"chunk[{i}]: type={type(part).__name__}, content='{part.content}'\")\n",
    "\n",
    "print(f\"...\\n총 {len(chunks)}개 chunk\")\n",
    "print(f\"\\n전체 응답: {''.join(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. invoke vs stream 비교\n",
    "\n",
    "같은 체인에서 `invoke()`와 `stream()` 차이를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "question = {'question': 'Python이 뭔가요? 간단히 설명해주세요.'}\n",
    "\n",
    "# invoke() - 전체 응답 대기\n",
    "print(\"=== invoke() ===\")\n",
    "start = time.time()\n",
    "\n",
    "# yield를 사용하는 함수는 invoke()시 제너레이터를 반환\n",
    "# 전체 결과를 얻으려면 list()로 변환하거나 순회해야 함\n",
    "result = list(chatbot.invoke(question))\n",
    "full_response = ''.join([r.content for r in result])\n",
    "\n",
    "print(f\"전체 응답 시간: {time.time() - start:.2f}초\")\n",
    "print(f\"응답: {full_response[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# stream() - 토큰 단위 수신\n",
    "print(\"=== stream() ===\")\n",
    "start = time.time()\n",
    "first_token_time = None\n",
    "\n",
    "response_parts = []\n",
    "for part in chatbot.stream(question):\n",
    "    if first_token_time is None:\n",
    "        first_token_time = time.time() - start\n",
    "    response_parts.append(part.content)\n",
    "\n",
    "print(f\"첫 토큰 수신: {first_token_time:.2f}초\")\n",
    "print(f\"전체 응답 시간: {time.time() - start:.2f}초\")\n",
    "print(f\"응답: {''.join(response_parts)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 코드 변경점 (OpenAI → Ollama)\n",
    "\n",
    "```python\n",
    "# 원본 (OpenAI)\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "# 변경 (Ollama)\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model='llama3.2')\n",
    "```\n",
    "\n",
    "## 스트리밍 패턴 비교\n",
    "\n",
    "### 1. 직접 model.stream() 사용 (10번 노트북)\n",
    "```python\n",
    "for token in model.stream('질문'):\n",
    "    print(token.content, end='')\n",
    "```\n",
    "\n",
    "### 2. LCEL 체인에서 stream() (권장)\n",
    "```python\n",
    "chain = template | model\n",
    "for token in chain.stream({'question': '질문'}):\n",
    "    print(token.content, end='')\n",
    "```\n",
    "\n",
    "### 3. @chain + yield (이 노트북)\n",
    "```python\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token\n",
    "\n",
    "for token in chatbot.stream({'question': '질문'}):\n",
    "    print(token.content, end='')\n",
    "```\n",
    "\n",
    "## 언제 @chain + yield를 사용할까?\n",
    "\n",
    "| 상황 | 추천 방식 |\n",
    "|------|----------|\n",
    "| 단순 체인 스트리밍 | LCEL `chain.stream()` |\n",
    "| 스트리밍 중 **로깅** 필요 | @chain + yield |\n",
    "| 스트리밍 중 **토큰 변환** 필요 | @chain + yield |\n",
    "| 스트리밍 중 **조건부 처리** 필요 | @chain + yield |\n",
    "\n",
    "## 스트리밍 중 토큰 변환 예시\n",
    "\n",
    "```python\n",
    "@chain\n",
    "def uppercase_chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        # 토큰 내용을 대문자로 변환\n",
    "        token.content = token.content.upper()\n",
    "        yield token\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
