{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @chain과 async/await로 비동기 체인 구성하기\n",
    "\n",
    "이 노트북에서는 **@chain 데코레이터**와 **async/await**를 결합하여 비동기(Asynchronous) 체인을 구성하는 방법을 알아봅니다.\n",
    "\n",
    "## 동기 vs 비동기\n",
    "\n",
    "| 방식 | 메서드 | 특징 |\n",
    "|------|--------|------|\n",
    "| **동기 (Sync)** | `invoke()`, `batch()`, `stream()` | 순차 실행, 완료까지 대기 |\n",
    "| **비동기 (Async)** | `ainvoke()`, `abatch()`, `astream()` | 동시 실행, I/O 대기 중 다른 작업 가능 |\n",
    "\n",
    "## 비동기의 장점\n",
    "\n",
    "1. **높은 처리량**: 여러 요청을 동시에 처리 (웹 서버에서 유용)\n",
    "2. **효율적인 I/O**: 네트워크 대기 시간 동안 다른 작업 수행\n",
    "3. **확장성**: FastAPI, aiohttp 등 비동기 프레임워크와 호환\n",
    "\n",
    "## @chain + async 문법\n",
    "\n",
    "```python\n",
    "@chain\n",
    "async def my_chain(values):         # async def로 정의\n",
    "    prompt = await template.ainvoke(values)  # await + ainvoke()\n",
    "    return await model.ainvoke(prompt)       # await + ainvoke()\n",
    "\n",
    "# 호출 시에도 await + ainvoke()\n",
    "result = await my_chain.ainvoke(inputs)\n",
    "```\n",
    "\n",
    "**핵심 규칙:**\n",
    "- 함수: `async def`\n",
    "- 내부 호출: `await` + `ainvoke()` / `abatch()` / `astream()`\n",
    "- 외부 호출: `await` + `ainvoke()`\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Ollama 설치 및 서버 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# zstd 설치 (Ollama 설치의 사전 요구 사항)\n",
    "!apt-get install -y zstd\n",
    "\n",
    "# Ollama 설치\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# 백그라운드에서 Ollama 서버 실행\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 다운로드 & 패키지 설치\n",
    "\n",
    "- `ollama pull llama3.2` - Llama 3.2 모델 다운로드\n",
    "- `pip install langchain-ollama` - LangChain Ollama 통합 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2\n",
    "!pip install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 구성 요소 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Prompt Template\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant.'),\n",
    "    ('human', '{question}'),\n",
    "])\n",
    "\n",
    "# Model\n",
    "model = ChatOllama(model='llama3.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 비동기 체인 정의\n",
    "\n",
    "**코드 설명:**\n",
    "\n",
    "### async def + @chain\n",
    "```python\n",
    "@chain\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)  # 비동기 프롬프트 생성\n",
    "    return await model.ainvoke(prompt)        # 비동기 모델 호출\n",
    "```\n",
    "\n",
    "**주의사항:**\n",
    "- 동기 메서드(`invoke`)가 아닌 **비동기 메서드(`ainvoke`)** 사용\n",
    "- 각 비동기 호출 앞에 **`await`** 필수\n",
    "- Jupyter 노트북에서는 `await`를 셀에서 직접 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "# 비동기 체인 정의\n",
    "@chain\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)  # await + ainvoke\n",
    "    return await model.ainvoke(prompt)        # await + ainvoke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 비동기 체인 실행\n",
    "\n",
    "Jupyter 노트북에서는 `await`를 직접 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter에서 비동기 실행\n",
    "response = await chatbot.ainvoke({'question': '거대 언어 모델은 어디서 제공하나요?'})\n",
    "\n",
    "print(\"=== 비동기 실행 결과 ===\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 여러 요청 동시 처리 (비동기의 진정한 힘)\n",
    "\n",
    "`asyncio.gather()`를 사용하면 여러 요청을 **동시에** 처리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "questions = [\n",
    "    {'question': 'Python이 뭔가요?'},\n",
    "    {'question': 'JavaScript가 뭔가요?'},\n",
    "    {'question': 'Rust가 뭔가요?'},\n",
    "]\n",
    "\n",
    "# 동시 실행 (asyncio.gather)\n",
    "print(\"=== asyncio.gather()로 동시 실행 ===\")\n",
    "start = time.time()\n",
    "\n",
    "results = await asyncio.gather(*[\n",
    "    chatbot.ainvoke(q) for q in questions\n",
    "])\n",
    "\n",
    "print(f\"총 소요 시간: {time.time() - start:.2f}초\")\n",
    "print(f\"처리된 요청 수: {len(results)}개\\n\")\n",
    "\n",
    "for i, resp in enumerate(results):\n",
    "    print(f\"[{i+1}] {resp.content[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 동기 vs 비동기 성능 비교\n",
    "\n",
    "여러 요청을 처리할 때 동기와 비동기의 차이를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동기 방식으로 순차 실행\n",
    "@chain\n",
    "def sync_chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "\n",
    "print(\"=== 동기 순차 실행 ===\")\n",
    "start = time.time()\n",
    "\n",
    "sync_results = []\n",
    "for q in questions:\n",
    "    sync_results.append(sync_chatbot.invoke(q))\n",
    "\n",
    "sync_time = time.time() - start\n",
    "print(f\"동기 순차 실행 시간: {sync_time:.2f}초\")\n",
    "\n",
    "# 비동기 동시 실행\n",
    "print(\"\\n=== 비동기 동시 실행 ===\")\n",
    "start = time.time()\n",
    "\n",
    "async_results = await asyncio.gather(*[\n",
    "    chatbot.ainvoke(q) for q in questions\n",
    "])\n",
    "\n",
    "async_time = time.time() - start\n",
    "print(f\"비동기 동시 실행 시간: {async_time:.2f}초\")\n",
    "\n",
    "print(f\"\\n성능 향상: {sync_time / async_time:.1f}배 빠름\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 코드 변경점 (OpenAI → Ollama)\n",
    "\n",
    "```python\n",
    "# 원본 (OpenAI)\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "# 변경 (Ollama)\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model='llama3.2')\n",
    "```\n",
    "\n",
    "## 비동기 메서드 정리\n",
    "\n",
    "| 동기 메서드 | 비동기 메서드 | 용도 |\n",
    "|------------|--------------|------|\n",
    "| `invoke()` | `ainvoke()` | 단일 실행 |\n",
    "| `batch()` | `abatch()` | 배치 실행 |\n",
    "| `stream()` | `astream()` | 스트리밍 |\n",
    "\n",
    "## Python 스크립트에서 실행하기\n",
    "\n",
    "Jupyter가 아닌 일반 스크립트에서는 `asyncio.run()`을 사용합니다:\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    result = await chatbot.ainvoke({'question': '안녕!'})\n",
    "    print(result.content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "```\n",
    "\n",
    "## FastAPI에서 사용 예시\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def chat(question: str):\n",
    "    response = await chatbot.ainvoke({'question': question})\n",
    "    return {\"answer\": response.content}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
