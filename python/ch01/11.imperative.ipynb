{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @chain 데코레이터로 명령형 체인 구성하기\n",
    "\n",
    "이 노트북에서는 **@chain 데코레이터**를 사용하여 명령형(Imperative) 방식으로 체인을 구성하는 방법을 알아봅니다.\n",
    "\n",
    "## 선언형 vs 명령형 체인\n",
    "\n",
    "LangChain에서 체인을 구성하는 두 가지 방식이 있습니다:\n",
    "\n",
    "| 방식 | 문법 | 특징 |\n",
    "|------|------|------|\n",
    "| **선언형 (LCEL)** | `chain = a \\| b \\| c` | 파이프 연산자, 간결함 |\n",
    "| **명령형 (Imperative)** | `@chain` 데코레이터 + 함수 | 세밀한 제어, 조건문/반복문 사용 가능 |\n",
    "\n",
    "## @chain 데코레이터란?\n",
    "\n",
    "일반 Python 함수에 **Runnable 인터페이스**를 추가하는 데코레이터입니다.\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def my_chain(inputs):\n",
    "    # 자유로운 Python 코드\n",
    "    return result\n",
    "\n",
    "# Runnable 메서드 사용 가능!\n",
    "my_chain.invoke(inputs)\n",
    "my_chain.batch([inputs1, inputs2])\n",
    "my_chain.stream(inputs)\n",
    "```\n",
    "\n",
    "## 언제 명령형을 사용할까?\n",
    "\n",
    "1. **조건부 로직**: 입력에 따라 다른 처리가 필요할 때\n",
    "2. **복잡한 흐름**: 반복문, 예외 처리 등이 필요할 때\n",
    "3. **중간 처리**: 단계 사이에 데이터 가공이 필요할 때\n",
    "4. **디버깅**: 각 단계를 명시적으로 제어하고 싶을 때\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Ollama 설치 및 서버 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# zstd 설치 (Ollama 설치의 사전 요구 사항)\n",
    "!apt-get install -y zstd\n",
    "\n",
    "# Ollama 설치\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# 백그라운드에서 Ollama 서버 실행\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 다운로드 & 패키지 설치\n",
    "\n",
    "- `ollama pull llama3.2` - Llama 3.2 모델 다운로드\n",
    "- `pip install langchain-ollama` - LangChain Ollama 통합 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2\n",
    "!pip install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 구성 요소 준비\n",
    "\n",
    "체인에서 사용할 **Prompt Template**과 **Model**을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Prompt Template\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', '당신은 친절한 어시스턴트입니다.'),\n",
    "    ('human', '{question}'),\n",
    "])\n",
    "\n",
    "# Model\n",
    "model = ChatOllama(model='llama3.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. @chain 데코레이터로 체인 구성\n",
    "\n",
    "**코드 설명:**\n",
    "\n",
    "### @chain 데코레이터 적용\n",
    "```python\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)  # Step 1: 프롬프트 생성\n",
    "    return model.invoke(prompt)        # Step 2: 모델 호출\n",
    "```\n",
    "\n",
    "**핵심 포인트:**\n",
    "- `@chain` 데코레이터로 일반 함수를 **Runnable**로 변환\n",
    "- 함수 내부에서 **명시적으로** 각 단계 실행\n",
    "- `invoke()`, `batch()`, `stream()` 메서드 자동 지원\n",
    "\n",
    "### LCEL 방식과 비교\n",
    "```python\n",
    "# LCEL (선언형)\n",
    "chain = template | model\n",
    "\n",
    "# @chain (명령형)\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "```\n",
    "\n",
    "결과는 동일하지만, 명령형은 **중간에 자유로운 코드 삽입**이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "# @chain 데코레이터로 Runnable 함수 생성\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)  # Step 1: 프롬프트 생성\n",
    "    return model.invoke(prompt)        # Step 2: 모델 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 체인 실행\n",
    "\n",
    "`@chain`으로 만든 함수는 **Runnable 인터페이스**를 갖습니다.\n",
    "\n",
    "- `chatbot.invoke()` - 단일 실행\n",
    "- `chatbot.batch()` - 배치 실행\n",
    "- `chatbot.stream()` - 스트리밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke() 실행\n",
    "response = chatbot.invoke({'question': '거대 언어 모델은 어디서 제공하나요?'})\n",
    "\n",
    "print(\"=== chatbot.invoke() 결과 ===\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch() 실행\n",
    "responses = chatbot.batch([\n",
    "    {'question': 'Python이 뭔가요?'},\n",
    "    {'question': 'LangChain이 뭔가요?'}\n",
    "])\n",
    "\n",
    "print(\"=== chatbot.batch() 결과 ===\")\n",
    "for i, resp in enumerate(responses):\n",
    "    print(f\"\\n[{i+1}] {resp.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 명령형의 장점: 조건부 로직\n",
    "\n",
    "명령형 방식의 가장 큰 장점은 **조건문, 반복문** 등을 자유롭게 사용할 수 있다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조건부 로직이 포함된 체인\n",
    "@chain\n",
    "def smart_chatbot(values):\n",
    "    question = values.get('question', '')\n",
    "    \n",
    "    # 조건에 따라 다른 시스템 메시지 사용\n",
    "    if '코드' in question or '프로그래밍' in question:\n",
    "        system_msg = '당신은 프로그래밍 전문가입니다. 코드 예시와 함께 설명하세요.'\n",
    "    elif '번역' in question:\n",
    "        system_msg = '당신은 전문 번역가입니다.'\n",
    "    else:\n",
    "        system_msg = '당신은 친절한 어시스턴트입니다.'\n",
    "    \n",
    "    # 동적으로 템플릿 생성\n",
    "    dynamic_template = ChatPromptTemplate.from_messages([\n",
    "        ('system', system_msg),\n",
    "        ('human', '{question}'),\n",
    "    ])\n",
    "    \n",
    "    prompt = dynamic_template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "\n",
    "# 테스트\n",
    "print(\"=== 조건부 체인 테스트 ===\")\n",
    "response = smart_chatbot.invoke({'question': 'Python 코드로 Hello World 출력하는 방법'})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 명령형의 장점: 중간 처리 및 로깅\n",
    "\n",
    "각 단계 사이에 **로깅, 데이터 변환, 검증** 등을 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로깅이 포함된 체인\n",
    "@chain\n",
    "def logged_chatbot(values):\n",
    "    print(f\"[LOG] 입력값: {values}\")\n",
    "    \n",
    "    # Step 1: 프롬프트 생성\n",
    "    prompt = template.invoke(values)\n",
    "    print(f\"[LOG] 프롬프트 생성 완료\")\n",
    "    \n",
    "    # Step 2: 모델 호출\n",
    "    response = model.invoke(prompt)\n",
    "    print(f\"[LOG] 응답 길이: {len(response.content)}자\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 테스트\n",
    "print(\"=== 로깅 체인 테스트 ===\")\n",
    "response = logged_chatbot.invoke({'question': '안녕하세요!'})\n",
    "print(f\"\\n최종 응답: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 코드 변경점 (OpenAI → Ollama)\n",
    "\n",
    "```python\n",
    "# 원본 (OpenAI)\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "# 변경 (Ollama)\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model='llama3.2')\n",
    "```\n",
    "\n",
    "## 선언형 vs 명령형 선택 가이드\n",
    "\n",
    "| 상황 | 추천 방식 |\n",
    "|------|----------|\n",
    "| 단순한 순차 처리 | **LCEL** (`a \\| b \\| c`) |\n",
    "| 조건부 분기 필요 | **@chain** (명령형) |\n",
    "| 복잡한 에러 처리 | **@chain** (명령형) |\n",
    "| 중간 로깅/검증 | **@chain** (명령형) |\n",
    "| 빠른 프로토타이핑 | **LCEL** (간결함) |\n",
    "\n",
    "## @chain 주의사항\n",
    "\n",
    "1. **스트리밍**: 기본적으로 최종 결과만 반환됨. 중간 스트리밍이 필요하면 추가 처리 필요\n",
    "2. **타입 힌트**: 입력/출력 타입을 명시하면 가독성 향상\n",
    "3. **에러 처리**: try-except로 각 단계 에러를 세밀하게 제어 가능\n",
    "\n",
    "```python\n",
    "@chain\n",
    "def safe_chatbot(values: dict) -> str:\n",
    "    try:\n",
    "        prompt = template.invoke(values)\n",
    "        response = model.invoke(prompt)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"오류 발생: {e}\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
