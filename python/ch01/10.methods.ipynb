{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain 핵심 메서드: invoke, batch, stream\n",
    "\n",
    "이 노트북에서는 LangChain의 **세 가지 핵심 실행 메서드**를 알아봅니다.\n",
    "\n",
    "## 메서드 개요\n",
    "\n",
    "| 메서드 | 설명 | 반환값 | 사용 상황 |\n",
    "|--------|------|--------|----------|\n",
    "| `invoke()` | 단일 입력 처리 | 단일 결과 | 일반적인 1회 호출 |\n",
    "| `batch()` | 여러 입력 동시 처리 | 결과 리스트 | 대량 처리, 병렬 실행 |\n",
    "| `stream()` | 스트리밍 출력 | 토큰 제너레이터 | 실시간 응답 표시 |\n",
    "\n",
    "## Runnable 인터페이스\n",
    "\n",
    "LangChain의 모든 컴포넌트(LLM, Prompt, Parser, Chain)는 **Runnable** 인터페이스를 구현합니다.\n",
    "\n",
    "```python\n",
    "# 모든 Runnable 객체는 이 메서드들을 지원\n",
    "runnable.invoke(input)      # 동기 실행\n",
    "runnable.batch([inputs])    # 배치 실행\n",
    "runnable.stream(input)      # 스트리밍\n",
    "\n",
    "# 비동기 버전\n",
    "await runnable.ainvoke(input)\n",
    "await runnable.abatch([inputs])\n",
    "async for chunk in runnable.astream(input)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Ollama 설치 및 서버 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# zstd 설치 (Ollama 설치의 사전 요구 사항)\n",
    "!apt-get install -y zstd\n",
    "\n",
    "# Ollama 설치\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# 백그라운드에서 Ollama 서버 실행\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 다운로드 & 패키지 설치\n",
    "\n",
    "- `ollama pull llama3.2` - Llama 3.2 모델 다운로드\n",
    "- `pip install langchain-ollama` - LangChain Ollama 통합 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2\n",
    "!pip install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model='llama3.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. invoke() - 단일 입력 처리\n",
    "\n",
    "**가장 기본적인 실행 메서드**입니다. 하나의 입력을 받아 하나의 결과를 반환합니다.\n",
    "\n",
    "```python\n",
    "result = model.invoke(input)\n",
    "```\n",
    "\n",
    "**특징:**\n",
    "- 동기(synchronous) 실행\n",
    "- 응답이 완료될 때까지 대기\n",
    "- 가장 간단하고 직관적인 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke() - 단일 입력, 단일 출력\n",
    "response = model.invoke('반가워요!')\n",
    "\n",
    "print(\"=== invoke() 결과 ===\")\n",
    "print(f\"타입: {type(response).__name__}\")\n",
    "print(f\"응답: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. batch() - 여러 입력 동시 처리\n",
    "\n",
    "**여러 입력을 한 번에 처리**하여 결과 리스트를 반환합니다.\n",
    "\n",
    "```python\n",
    "results = model.batch([input1, input2, input3])\n",
    "```\n",
    "\n",
    "**특징:**\n",
    "- 내부적으로 **병렬 처리** (효율적)\n",
    "- 순서대로 결과 반환 (입력 순서 = 출력 순서)\n",
    "- 대량 데이터 처리에 적합\n",
    "\n",
    "**옵션:**\n",
    "```python\n",
    "# 동시 실행 수 제한\n",
    "results = model.batch(inputs, config={\"max_concurrency\": 5})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch() - 여러 입력, 여러 출력\n",
    "responses = model.batch(['반가워요!', '잘 있어요!', '좋은 하루 되세요!'])\n",
    "\n",
    "print(\"=== batch() 결과 ===\")\n",
    "print(f\"타입: {type(responses).__name__}\")\n",
    "print(f\"결과 개수: {len(responses)}\")\n",
    "print()\n",
    "\n",
    "for i, resp in enumerate(responses):\n",
    "    print(f\"[{i+1}] {resp.content[:50]}...\" if len(resp.content) > 50 else f\"[{i+1}] {resp.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. stream() - 스트리밍 출력\n",
    "\n",
    "**응답을 토큰 단위로 실시간 수신**합니다. ChatGPT처럼 글자가 하나씩 나타나는 효과를 구현할 때 사용합니다.\n",
    "\n",
    "```python\n",
    "for chunk in model.stream(input):\n",
    "    print(chunk.content, end='')\n",
    "```\n",
    "\n",
    "**특징:**\n",
    "- **제너레이터(Generator)** 반환\n",
    "- 첫 토큰이 생성되면 바로 수신 시작\n",
    "- 사용자 경험(UX) 향상 - 대기 시간 체감 감소\n",
    "- 긴 응답에서 특히 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream() - 토큰 단위 스트리밍\n",
    "print(\"=== stream() 결과 ===\")\n",
    "print(\"실시간 출력: \", end='')\n",
    "\n",
    "for chunk in model.stream('잘 있어요!'):\n",
    "    print(chunk.content, end='', flush=True)\n",
    "\n",
    "print()  # 줄바꿈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 스트리밍 토큰 상세 확인\n",
    "\n",
    "각 chunk가 어떤 내용을 담고 있는지 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 chunk 상세 확인\n",
    "print(\"=== 각 chunk 내용 ===\")\n",
    "\n",
    "chunks = []\n",
    "for i, chunk in enumerate(model.stream('안녕!')):\n",
    "    chunks.append(chunk.content)\n",
    "    print(f\"chunk[{i}]: '{chunk.content}'\")\n",
    "\n",
    "print(f\"\\n전체 응답: {''.join(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 코드 변경점 (OpenAI → Ollama)\n",
    "\n",
    "```python\n",
    "# 원본 (OpenAI)\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 변경 (Ollama)\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model='llama3.2')\n",
    "```\n",
    "\n",
    "> `invoke()`, `batch()`, `stream()` 메서드는 동일하게 사용됩니다.\n",
    "\n",
    "## 메서드 선택 가이드\n",
    "\n",
    "| 상황 | 추천 메서드 |\n",
    "|------|------------|\n",
    "| 일반적인 단일 요청 | `invoke()` |\n",
    "| 여러 문서 요약, 번역 등 | `batch()` |\n",
    "| 챗봇 UI, 실시간 응답 | `stream()` |\n",
    "| 비동기 웹 서버 (FastAPI 등) | `ainvoke()`, `astream()` |\n",
    "\n",
    "## 체인에서도 동일하게 사용\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic}에 대해 설명해줘\")\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# 체인에서도 세 메서드 모두 사용 가능\n",
    "chain.invoke({\"topic\": \"AI\"})\n",
    "chain.batch([{\"topic\": \"AI\"}, {\"topic\": \"ML\"}])\n",
    "for chunk in chain.stream({\"topic\": \"AI\"}):\n",
    "    print(chunk, end='')\n",
    "```\n",
    "\n",
    "## 비동기 버전 (async)\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    # 비동기 invoke\n",
    "    result = await model.ainvoke('안녕!')\n",
    "    \n",
    "    # 비동기 batch\n",
    "    results = await model.abatch(['안녕!', '반가워!'])\n",
    "    \n",
    "    # 비동기 stream\n",
    "    async for chunk in model.astream('안녕!'):\n",
    "        print(chunk.content, end='')\n",
    "\n",
    "asyncio.run(main())\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
