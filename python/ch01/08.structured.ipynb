{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output으로 정형화된 응답 받기\n",
    "\n",
    "이 노트북에서는 **Structured Output**을 사용하여 LLM 응답을 정해진 스키마(구조)에 맞게 받는 방법을 알아봅니다.\n",
    "\n",
    "## Structured Output이란?\n",
    "\n",
    "LLM의 응답을 **Pydantic 모델**로 정의한 구조에 맞게 강제하는 기능입니다.\n",
    "\n",
    "### 일반 응답 vs Structured Output\n",
    "\n",
    "| 구분 | 일반 응답 | Structured Output |\n",
    "|------|----------|------------------|\n",
    "| 반환 타입 | 문자열 (str) | Pydantic 객체 |\n",
    "| 형식 | 자유 형식 텍스트 | 정의된 필드 구조 |\n",
    "| 파싱 | 수동 파싱 필요 | 자동 파싱 |\n",
    "| 안정성 | 형식 불일치 가능 | 스키마 보장 |\n",
    "\n",
    "## 언제 사용할까?\n",
    "\n",
    "1. **API 응답 생성**: JSON 형식으로 프론트엔드에 전달\n",
    "2. **데이터 추출**: 텍스트에서 특정 정보 추출 (이름, 날짜, 금액 등)\n",
    "3. **의사결정 기록**: 답변과 함께 근거/확신도 저장\n",
    "4. **워크플로우 연결**: 다음 단계에 필요한 데이터 구조화\n",
    "\n",
    "## Pydantic이란?\n",
    "\n",
    "Python의 데이터 검증 라이브러리로, 타입 힌트를 사용해 데이터 모델을 정의합니다.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str      # 필수 문자열\n",
    "    age: int       # 필수 정수\n",
    "    email: str | None = None  # 선택 문자열\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Ollama 설치 및 서버 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# zstd 설치 (Ollama 설치의 사전 요구 사항)\n",
    "!apt-get install -y zstd\n",
    "\n",
    "# Ollama 설치\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# 백그라운드에서 Ollama 서버 실행\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 다운로드 & 패키지 설치\n",
    "\n",
    "- `ollama pull llama3.2` - Llama 3.2 모델 다운로드\n",
    "- `pip install langchain-ollama` - LangChain Ollama 통합 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2\n",
    "!pip install -q langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pydantic 모델 정의\n",
    "\n",
    "**코드 설명:**\n",
    "\n",
    "### Pydantic 모델 구조\n",
    "```python\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''클래스 docstring = 모델에게 전달되는 지시사항'''\n",
    "    \n",
    "    answer: str\n",
    "    '''필드 docstring = 필드 설명 (모델이 참고)'''\n",
    "    \n",
    "    justification: str\n",
    "    '''답변에 대한 근거'''\n",
    "```\n",
    "\n",
    "**핵심 포인트:**\n",
    "- 클래스의 **docstring**은 LLM에게 전달되는 지시사항\n",
    "- 각 필드의 **docstring**은 해당 필드에 무엇을 넣어야 하는지 설명\n",
    "- 타입 힌트(`str`, `int`, `list` 등)로 데이터 타입 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''사용자의 질문에 대한 답변과 그에 대한 근거(justification)를 함께 제공하세요.'''\n",
    "    \n",
    "    answer: str\n",
    "    '''사용자의 질문에 대한 답변'''\n",
    "    \n",
    "    justification: str\n",
    "    '''답변에 대한 근거'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Structured Output 적용 및 실행\n",
    "\n",
    "**코드 설명:**\n",
    "\n",
    "### with_structured_output() 메서드\n",
    "```python\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "```\n",
    "- 기존 LLM을 감싸서 Structured Output을 반환하는 새 객체 생성\n",
    "- Pydantic 모델을 인자로 전달\n",
    "\n",
    "### 실행 및 결과\n",
    "```python\n",
    "result = structured_llm.invoke('질문...')\n",
    "```\n",
    "- `result`는 **Pydantic 객체** (문자열이 아님!)\n",
    "- `result.answer` - 답변 필드 접근\n",
    "- `result.justification` - 근거 필드 접근\n",
    "- `result.model_dump_json()` - JSON 문자열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 기본 LLM 생성\n",
    "llm = ChatOllama(model='llama3.2', temperature=0)\n",
    "\n",
    "# Structured Output 적용\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "# 실행\n",
    "result = structured_llm.invoke('1 킬로그램의 벽돌과 1 킬로그램의 깃털 중 어느 쪽이 더 무겁나요?')\n",
    "\n",
    "# 결과 출력\n",
    "print(\"=== Pydantic 객체 필드 접근 ===\")\n",
    "print(f\"답변: {result.answer}\")\n",
    "print(f\"근거: {result.justification}\")\n",
    "\n",
    "print(\"\\n=== JSON 형식 출력 ===\")\n",
    "print(result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 코드 변경점 (OpenAI → Ollama)\n",
    "\n",
    "```python\n",
    "# 원본 (OpenAI)\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# 변경 (Ollama)\n",
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model='llama3.2', temperature=0)\n",
    "```\n",
    "\n",
    "> `with_structured_output()` 메서드는 동일하게 사용됩니다.\n",
    "\n",
    "## 다양한 Pydantic 모델 예시\n",
    "\n",
    "### 감정 분석\n",
    "```python\n",
    "class SentimentAnalysis(BaseModel):\n",
    "    '''텍스트의 감정을 분석하세요.'''\n",
    "    sentiment: str  # positive, negative, neutral\n",
    "    confidence: float  # 0.0 ~ 1.0\n",
    "    keywords: list[str]  # 핵심 키워드\n",
    "```\n",
    "\n",
    "### 정보 추출\n",
    "```python\n",
    "class PersonInfo(BaseModel):\n",
    "    '''텍스트에서 인물 정보를 추출하세요.'''\n",
    "    name: str\n",
    "    age: int | None = None\n",
    "    occupation: str | None = None\n",
    "```\n",
    "\n",
    "### 의사결정\n",
    "```python\n",
    "class Decision(BaseModel):\n",
    "    '''주어진 상황에 대한 결정을 내리세요.'''\n",
    "    decision: bool  # True/False\n",
    "    reason: str\n",
    "    confidence_level: str  # high, medium, low\n",
    "```\n",
    "\n",
    "## 주의사항\n",
    "\n",
    "1. **모델 호환성**: 모든 모델이 Structured Output을 지원하지는 않음\n",
    "2. **복잡한 스키마**: 너무 복잡한 중첩 구조는 오류 가능성 증가\n",
    "3. **temperature=0**: 일관된 구조 출력을 위해 낮은 temperature 권장\n",
    "4. **필드 설명**: docstring을 자세히 작성할수록 정확도 향상"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
