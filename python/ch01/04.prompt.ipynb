{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain PromptTemplate과 LCEL 체인 구성하기\n",
    "\n",
    "이 노트북에서는 **PromptTemplate**을 사용하여 프롬프트를 구조화하고, **LCEL(LangChain Expression Language)**로 체인을 구성하는 방법을 알아봅니다.\n",
    "\n",
    "## PromptTemplate이란?\n",
    "\n",
    "프롬프트 템플릿은 동적으로 변경되는 값을 포함한 프롬프트를 정의하는 방법입니다.\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                    PromptTemplate 개념                             │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│   일반 문자열:                                                     │\n",
    "│   \"거대 언어 모델에 대해 알려주세요\"                               │\n",
    "│   → 고정된 질문, 재사용 불가                                       │\n",
    "│                                                                    │\n",
    "│   PromptTemplate:                                                  │\n",
    "│   \"{topic}에 대해 알려주세요\"                                     │\n",
    "│   → {topic} 부분만 바꿔서 다양한 질문 생성 가능                    │\n",
    "│                                                                    │\n",
    "│   활용:                                                            │\n",
    "│   • Context + Question 패턴 (RAG의 기본)                           │\n",
    "│   • 역할 지정 + 작업 지시                                          │\n",
    "│   • 입력 형식 표준화                                               │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. 환경 설정 (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# zstd 설치 (Ollama 설치의 사전 요구 사항)\n",
    "!apt-get install -y zstd\n",
    "\n",
    "# Ollama 설치\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# 백그라운드에서 Ollama 서버 실행\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 다운로드 및 패키지 설치\n",
    "!ollama pull llama3.2\n",
    "!pip install -q langchain langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PromptTemplate 단독 사용\n",
    "\n",
    "먼저 PromptTemplate만 사용해서 프롬프트가 어떻게 생성되는지 확인합니다.\n",
    "\n",
    "**원본 Python 파일 (04.prompt.py)의 동작:**\n",
    "- `PromptTemplate.from_template()` - 문자열에서 `{변수명}` 패턴을 자동 감지\n",
    "- `template.invoke({...})` - 변수에 값을 채워서 완성된 프롬프트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 원본 코드와 동일한 템플릿 생성\n",
    "template = PromptTemplate.from_template('''아래 작성한 컨텍스트(Context)를 기반으로\n",
    "    질문(Question)에 대답하세요. 제공된 정보로 대답할 수 없는 질문이라면 \"모르겠어요.\" 라고 답하세요.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: ''')\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트 생성 (LLM 호출 없이)\n",
    "response = template.invoke({\n",
    "    'context': '''거대 언어 모델(LLM)은 자연어 처리(NLP) 분야의 최신 발전을 이끌고 있습니다.\n",
    "    거대 언어 모델은 더 작은 모델보다 우수한 성능을 보이며, NLP 기능을 갖춘 애플리케이션을 개발하는 개발자들에게\n",
    "    매우 중요한 도구가 되었습니다. 개발자들은 Hugging Face의 `transformers` 라이브러리를\n",
    "    활용하거나, `openai` 및 `cohere` 라이브러리를 통해 OpenAI와 Cohere의 서비스를 이용하여\n",
    "    거대 언어 모델을 활용할 수 있습니다.\n",
    "    ''',\n",
    "    'question': '거대 언어 모델은 어디서 제공하나요?'\n",
    "})\n",
    "\n",
    "print(\"=== 생성된 프롬프트 ===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**출력 결과 설명:**\n",
    "\n",
    "위 코드에서 `template.invoke()`는 LLM을 호출하지 않습니다. 단지 `{context}`와 `{question}` 자리에 값을 채워서 완성된 프롬프트 문자열을 반환합니다.\n",
    "\n",
    "이것이 바로 **원본 Python 파일(04.prompt.py)**의 동작입니다.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. PromptTemplate + LLM 체인 (LCEL)\n",
    "\n",
    "실제로 LLM에게 질문하려면 템플릿과 모델을 연결해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 1. 프롬프트 템플릿 생성\n",
    "template = PromptTemplate.from_template('''아래 작성한 컨텍스트(Context)를 기반으로\n",
    "    질문(Question)에 대답하세요. 제공된 정보로 대답할 수 없는 질문이라면 \"모르겠어요.\" 라고 답하세요.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: ''')\n",
    "\n",
    "# 2. Ollama 모델 초기화\n",
    "model = ChatOllama(model='llama3.2', temperature=0)\n",
    "\n",
    "# 3. LCEL 체인 구성: template → model\n",
    "chain = template | model\n",
    "\n",
    "# 4. 체인 실행\n",
    "response = chain.invoke({\n",
    "    'context': '''거대 언어 모델(LLM)은 자연어 처리(NLP) 분야의 최신 발전을 이끌고 있습니다.\n",
    "    거대 언어 모델은 더 작은 모델보다 우수한 성능을 보이며, NLP 기능을 갖춘 애플리케이션을 개발하는 개발자들에게\n",
    "    매우 중요한 도구가 되었습니다. 개발자들은 Hugging Face의 `transformers` 라이브러리를\n",
    "    활용하거나, `openai` 및 `cohere` 라이브러리를 통해 OpenAI와 Cohere의 서비스를 이용하여\n",
    "    거대 언어 모델을 활용할 수 있습니다.\n",
    "    ''',\n",
    "    'question': '거대 언어 모델은 어디서 제공하나요?'\n",
    "})\n",
    "\n",
    "print(\"=== LLM 응답 ===\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 정리: PromptTemplate\n",
    "\n",
    "### 원본 코드 (04.prompt.py) vs 확장 코드\n",
    "\n",
    "| 구분 | 원본 코드 | 확장 코드 |\n",
    "|------|----------|----------|\n",
    "| 목적 | 프롬프트 생성만 | 프롬프트 생성 + LLM 호출 |\n",
    "| 코드 | `template.invoke({...})` | `(template \\| model).invoke({...})` |\n",
    "| 반환값 | StringPromptValue (문자열) | AIMessage (LLM 응답) |\n",
    "\n",
    "### 핵심 개념\n",
    "\n",
    "```python\n",
    "# 1. 템플릿 생성 - {변수명} 자동 감지\n",
    "template = PromptTemplate.from_template(\"Context: {context}\\nQuestion: {question}\")\n",
    "\n",
    "# 2. 템플릿만 사용 (프롬프트 문자열 생성)\n",
    "prompt_string = template.invoke({'context': '...', 'question': '...'})\n",
    "\n",
    "# 3. LCEL 체인 (템플릿 + LLM)\n",
    "chain = template | model\n",
    "response = chain.invoke({'context': '...', 'question': '...'})\n",
    "```\n",
    "\n",
    "### 코드 변경점 (OpenAI → Ollama)\n",
    "\n",
    "```python\n",
    "# 원본 (OpenAI 사용 시)\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "# 변경 (Ollama 사용)\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model='llama3.2')\n",
    "```\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "- **05.prompt-model.py**: 프롬프트와 모델을 LCEL로 연결하는 패턴\n",
    "- **06.chat-prompt.py**: ChatPromptTemplate으로 역할별 메시지 구성"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
