{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interrupt: ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ë‹¨ ë° ì¬ê°œ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **ê·¸ë˜í”„ ì‹¤í–‰ì„ íŠ¹ì • ì§€ì ì—ì„œ ì¤‘ë‹¨í•˜ê³  ì¬ê°œ**í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## Interruptë€?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Interruptì˜ ê°œë…                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   ì¼ë°˜ ì‹¤í–‰:                                                       â”‚\n",
    "â”‚   START â†’ ë…¸ë“œ1 â†’ ë…¸ë“œ2 â†’ ë…¸ë“œ3 â†’ END                             â”‚\n",
    "â”‚   (ì¤‘ê°„ì— ë©ˆì¶œ ìˆ˜ ì—†ìŒ)                                            â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   interrupt_before ì‚¬ìš©:                                           â”‚\n",
    "â”‚   START â†’ ë…¸ë“œ1 â†’ [ì¤‘ë‹¨] â†’ ì‚¬ìš©ì ìŠ¹ì¸ â†’ ë…¸ë“œ2 â†’ ë…¸ë“œ3 â†’ END     â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   ì‚¬ìš© ì‚¬ë¡€:                                                       â”‚\n",
    "â”‚   â€¢ ë„êµ¬ ì‹¤í–‰ ì „ ì‚¬ìš©ì ìŠ¹ì¸                                       â”‚\n",
    "â”‚   â€¢ ê²°ì œ/ì‚­ì œ ë“± ìœ„í—˜í•œ ì‘ì—… ì „ í™•ì¸                               â”‚\n",
    "â”‚   â€¢ Human-in-the-loop ì›Œí¬í”Œë¡œìš°                                   â”‚\n",
    "â”‚   â€¢ ë””ë²„ê¹… ë° ê²€í†                                                  â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ì•„í‚¤í…ì²˜\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Interrupt ë™ì‘ íë¦„                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   1ï¸âƒ£ ì²« ë²ˆì§¸ ì‹¤í–‰                                                 â”‚\n",
    "â”‚   graph.stream(input, config, interrupt_before=[\"model\"])         â”‚\n",
    "â”‚   â†’ select_tools ì‹¤í–‰ â†’ model ì „ì— ì¤‘ë‹¨ â†’ ìƒíƒœ ì €ì¥               â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   2ï¸âƒ£ ì‚¬ìš©ì ê²€í† /ìŠ¹ì¸                                             â”‚\n",
    "â”‚   (ì—¬ê¸°ì„œ ìƒíƒœ í™•ì¸, ìˆ˜ì • ê°€ëŠ¥)                                    â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   3ï¸âƒ£ ë‘ ë²ˆì§¸ ì‹¤í–‰ (ì¬ê°œ)                                          â”‚\n",
    "â”‚   graph.stream(None, config)  # None = ì €ì¥ëœ ìƒíƒœì—ì„œ ì¬ê°œ       â”‚\n",
    "â”‚   â†’ model ì‹¤í–‰ â†’ tools ì‹¤í–‰ â†’ ... â†’ ì™„ë£Œ                          â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-ollama langgraph duckduckgo-search langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull llama3.2\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(query: str) -> str:\n",
    "    '''ê³„ì‚°ê¸°. ìˆ˜ì‹ë§Œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.'''\n",
    "    return ast.literal_eval(query)\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1)\n",
    "\n",
    "tools_retriever = InMemoryVectorStore.from_documents(\n",
    "    [Document(tool.description, metadata={'name': tool.name}) for tool in tools],\n",
    "    embeddings,\n",
    ").as_retriever()\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    selected_tools: list[str]\n",
    "\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    selected_tools = [tool for tool in tools if tool.name in state['selected_tools']]\n",
    "    res = model.bind_tools(selected_tools).invoke(state['messages'])\n",
    "    return {'messages': res}\n",
    "\n",
    "\n",
    "def select_tools(state: State) -> State:\n",
    "    query = state['messages'][-1].content\n",
    "    tool_docs = tools_retriever.invoke(query)\n",
    "    return {'selected_tools': [doc.metadata['name'] for doc in tool_docs]}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node('select_tools', select_tools)\n",
    "builder.add_node('model', model_node)\n",
    "builder.add_node('tools', ToolNode(tools))\n",
    "builder.add_edge(START, 'select_tools')\n",
    "builder.add_edge('select_tools', 'model')\n",
    "builder.add_conditional_edges('model', tools_condition)\n",
    "builder.add_edge('tools', 'model')\n",
    "\n",
    "# MemorySaver í•„ìˆ˜ (ìƒíƒœ ì €ì¥ìš©)\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "print(\"âœ… ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. interrupt_before ì‚¬ìš©\n",
    "\n",
    "íŠ¹ì • ë…¸ë“œ ì‹¤í–‰ **ì „ì—** ì¤‘ë‹¨í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    'messages': [\n",
    "        HumanMessage(\n",
    "            'ë¯¸êµ­ ì œ30ëŒ€ ëŒ€í†µë ¹ì˜ ì‚¬ë§ ë‹¹ì‹œ ë‚˜ì´ëŠ” ëª‡ ì‚´ì´ì—ˆë‚˜ìš”?'\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "print(\"=== ì²« ë²ˆì§¸ ì‹¤í–‰ (model ì „ì— ì¤‘ë‹¨) ===\")\n",
    "print(f\"ì§ˆë¬¸: {input_data['messages'][0].content}\\n\")\n",
    "\n",
    "# interrupt_before=[\"model\"]: model ë…¸ë“œ ì‹¤í–‰ ì „ì— ì¤‘ë‹¨\n",
    "output = graph.stream(input_data, config, interrupt_before=[\"model\"])\n",
    "\n",
    "for chunk in output:\n",
    "    for node_name, node_output in chunk.items():\n",
    "        print(f\"ë…¸ë“œ: {node_name}\")\n",
    "        print(f\"  ì¶œë ¥: {node_output}\")\n",
    "    print()\n",
    "\n",
    "print(\"ğŸ›‘ model ë…¸ë“œ ì „ì— ì¤‘ë‹¨ë¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ì¤‘ë‹¨ëœ ìƒíƒœ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜„ì¬ ì €ì¥ëœ ìƒíƒœ í™•ì¸\n",
    "state = graph.get_state(config)\n",
    "\n",
    "print(\"=== ì¤‘ë‹¨ëœ ìƒíƒœ ===\")\n",
    "print(f\"ë©”ì‹œì§€: {state.values.get('messages', [])}\")\n",
    "print(f\"ì„ íƒëœ ë„êµ¬: {state.values.get('selected_tools', [])}\")\n",
    "print(f\"\\në‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ: {state.next}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ì¬ê°œ (Resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ë‘ ë²ˆì§¸ ì‹¤í–‰ (ì¬ê°œ) ===\")\n",
    "print(\"ì‚¬ìš©ìê°€ ìŠ¹ì¸í–ˆë‹¤ê³  ê°€ì •\\n\")\n",
    "\n",
    "# Noneì„ ì „ë‹¬í•˜ë©´ ì €ì¥ëœ ìƒíƒœì—ì„œ ì¬ê°œ\n",
    "output = graph.stream(None, config)\n",
    "\n",
    "for chunk in output:\n",
    "    for node_name, node_output in chunk.items():\n",
    "        print(f\"ë…¸ë“œ: {node_name}\")\n",
    "        if 'messages' in node_output:\n",
    "            last_msg = node_output['messages'][-1] if isinstance(node_output['messages'], list) else node_output['messages']\n",
    "            if hasattr(last_msg, 'content') and last_msg.content:\n",
    "                print(f\"  ì‘ë‹µ: {str(last_msg.content)[:200]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. ìƒˆ ì…ë ¥ìœ¼ë¡œ ì¬ê°œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆë¡œìš´ ìŠ¤ë ˆë“œë¡œ ì‹œì‘\n",
    "config2 = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "print(\"=== ì²« ë²ˆì§¸ ì‹¤í–‰ (ì¤‘ë‹¨) ===\")\n",
    "output = graph.stream(input_data, config2, interrupt_before=[\"model\"])\n",
    "for chunk in output:\n",
    "    pass  # ì¶œë ¥ ìƒëµ\n",
    "print(\"ğŸ›‘ ì¤‘ë‹¨ë¨\\n\")\n",
    "\n",
    "# ìƒˆ ì…ë ¥ìœ¼ë¡œ ì¬ê°œ (ì§ˆë¬¸ ë³€ê²½)\n",
    "new_input = {\n",
    "    'messages': [\n",
    "        HumanMessage(\n",
    "            'ì˜í™” ë³´ì´í›„ë“œì˜ ì´¬ì˜ ê¸°ê°„ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?'\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=== ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìƒˆ ì…ë ¥ìœ¼ë¡œ ì¬ê°œ) ===\")\n",
    "print(f\"ìƒˆ ì§ˆë¬¸: {new_input['messages'][0].content}\\n\")\n",
    "\n",
    "# ìƒˆ ì…ë ¥ ì „ë‹¬ (ê¸°ì¡´ ìƒíƒœì— ì¶”ê°€ë¨)\n",
    "output = graph.stream(new_input, config2)\n",
    "\n",
    "for chunk in output:\n",
    "    for node_name, node_output in chunk.items():\n",
    "        print(f\"ë…¸ë“œ: {node_name}\")\n",
    "        if 'messages' in node_output:\n",
    "            last_msg = node_output['messages'][-1] if isinstance(node_output['messages'], list) else node_output['messages']\n",
    "            if hasattr(last_msg, 'content') and last_msg.content:\n",
    "                print(f\"  ì‘ë‹µ: {str(last_msg.content)[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì •ë¦¬: Interrupt\n",
    "\n",
    "### í•µì‹¬ ì½”ë“œ\n",
    "\n",
    "```python\n",
    "# 1. checkpointer í•„ìˆ˜\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# 2. interrupt_beforeë¡œ ì¤‘ë‹¨ ì§€ì  ì§€ì •\n",
    "output = graph.stream(input, config, interrupt_before=[\"model\"])\n",
    "\n",
    "# 3. ìƒíƒœ í™•ì¸\n",
    "state = graph.get_state(config)\n",
    "print(state.values)  # í˜„ì¬ ìƒíƒœ\n",
    "print(state.next)    # ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ\n",
    "\n",
    "# 4. ì¬ê°œ (ì €ì¥ëœ ìƒíƒœì—ì„œ)\n",
    "output = graph.stream(None, config)\n",
    "\n",
    "# 5. ìƒˆ ì…ë ¥ìœ¼ë¡œ ì¬ê°œ\n",
    "output = graph.stream(new_input, config)\n",
    "```\n",
    "\n",
    "### interrupt_before vs interrupt_after\n",
    "\n",
    "| ì˜µì…˜ | ì¤‘ë‹¨ ì‹œì  | ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ |\n",
    "|------|----------|---------------|\n",
    "| **interrupt_before** | ë…¸ë“œ ì‹¤í–‰ ì „ | ì‹¤í–‰ ì „ ìŠ¹ì¸ |\n",
    "| **interrupt_after** | ë…¸ë“œ ì‹¤í–‰ í›„ | ê²°ê³¼ ê²€í†  í›„ ì§„í–‰ |\n",
    "\n",
    "### ì‚¬ìš© ì‚¬ë¡€\n",
    "\n",
    "| ì‚¬ë¡€ | ì„¤ëª… |\n",
    "|------|------|\n",
    "| ë„êµ¬ ìŠ¹ì¸ | ìœ„í—˜í•œ ë„êµ¬ ì‹¤í–‰ ì „ í™•ì¸ |\n",
    "| ê²°ì œ í™•ì¸ | ê²°ì œ API í˜¸ì¶œ ì „ ìŠ¹ì¸ |\n",
    "| ë°ì´í„° ì‚­ì œ | ì‚­ì œ ì‘ì—… ì „ í™•ì¸ |\n",
    "| Human-in-the-loop | ì‚¬ëŒì˜ ê²€í†  í•„ìš” |\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸\n",
    "embeddings = OpenAIEmbeddings()\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0.1)\n",
    "\n",
    "# ë³€ê²½\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1)\n",
    "```\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "**ìƒíƒœ ìˆ˜ì •** ë° **íˆìŠ¤í† ë¦¬ì—ì„œ Fork**í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. (07-11ë²ˆ ë…¸íŠ¸ë¶)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
