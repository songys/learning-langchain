{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Streaming: í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **LLM ì¶œë ¥ì„ í† í° ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## í† í° ìŠ¤íŠ¸ë¦¬ë°ì´ë€?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    í† í° ìŠ¤íŠ¸ë¦¬ë°ì˜ ê°œë…                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   ë…¸ë“œ ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°:                                              â”‚\n",
    "â”‚   ë…¸ë“œ ì™„ë£Œ â†’ ì „ì²´ ì‘ë‹µ ë°˜í™˜                                       â”‚\n",
    "â”‚   \"ë¯¸êµ­ì˜ 30ëŒ€ ëŒ€í†µë ¹ì€ ìº˜ë¹ˆ ì¿¨ë¦¬ì§€ì…ë‹ˆë‹¤...\" (ì „ì²´)              â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°:                                              â”‚\n",
    "â”‚   \"ë¯¸êµ­\" â†’ \"ì˜\" â†’ \"30\" â†’ \"ëŒ€\" â†’ \"ëŒ€í†µë ¹\" â†’ \"ì€\" â†’ ...          â”‚\n",
    "â”‚   (ì‹¤ì‹œê°„ìœ¼ë¡œ ê¸€ìê°€ ë‚˜íƒ€ë‚¨)                                       â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   ì¥ì :                                                            â”‚\n",
    "â”‚   â€¢ ChatGPTì²˜ëŸ¼ ì‹¤ì‹œê°„ íƒ€ì´í•‘ íš¨ê³¼                                 â”‚\n",
    "â”‚   â€¢ ì‚¬ìš©ìê°€ ê¸°ë‹¤ë¦¬ëŠ” ëŠë‚Œ ê°ì†Œ                                    â”‚\n",
    "â”‚   â€¢ ê¸´ ì‘ë‹µë„ ì¦‰ì‹œ ì‹œì‘                                            â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-ollama langgraph duckduckgo-search langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(query: str) -> str:\n",
    "    '''ê³„ì‚°ê¸°. ìˆ˜ì‹ë§Œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.'''\n",
    "    return ast.literal_eval(query)\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1).bind_tools(tools)\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    res = model.invoke(state['messages'])\n",
    "    return {'messages': res}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node('model', model_node)\n",
    "builder.add_node('tools', ToolNode(tools))\n",
    "builder.add_edge(START, 'model')\n",
    "builder.add_conditional_edges('model', tools_condition)\n",
    "builder.add_edge('tools', 'model')\n",
    "\n",
    "# checkpointer í•„ìš” (astream_events ì‚¬ìš© ì‹œ)\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "print(\"âœ… ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. í† í° ìŠ¤íŠ¸ë¦¬ë° (astream_events)\n",
    "\n",
    "ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def stream_tokens():\n",
    "    \"\"\"\n",
    "    astream_eventsë¥¼ ì‚¬ìš©í•œ í† í° ìŠ¤íŠ¸ë¦¬ë°\n",
    "    \n",
    "    event['event'] == 'on_chat_model_stream' ì¼ ë•Œ\n",
    "    í† í° ë‹¨ìœ„ë¡œ contentë¥¼ ë°›ì„ ìˆ˜ ìˆìŒ\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        'messages': [\n",
    "            HumanMessage(\n",
    "                'ë¯¸êµ­ ì œ30ëŒ€ ëŒ€í†µë ¹ì˜ ì‚¬ë§ ë‹¹ì‹œ ë‚˜ì´ëŠ” ëª‡ ì‚´ì´ì—ˆë‚˜ìš”?'\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "    config = {'configurable': {'thread_id': '1'}}\n",
    "    \n",
    "    print(\"=== í† í° ìŠ¤íŠ¸ë¦¬ë° ===\")\n",
    "    print(f\"ì§ˆë¬¸: {input_data['messages'][0].content}\\n\")\n",
    "    print(\"ì‘ë‹µ: \", end='', flush=True)\n",
    "    \n",
    "    async for event in graph.astream_events(input_data, config, version=\"v2\"):\n",
    "        # LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ìº¡ì²˜\n",
    "        if event[\"event\"] == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                print(content, end='', flush=True)  # í† í° ë‹¨ìœ„ ì¶œë ¥\n",
    "    \n",
    "    print(\"\\n\\nâœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ\")\n",
    "\n",
    "# ë¹„ë™ê¸° ì‹¤í–‰\n",
    "await stream_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ì´ë²¤íŠ¸ ìœ í˜• í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def show_events():\n",
    "    \"\"\"\n",
    "    ë‹¤ì–‘í•œ ì´ë²¤íŠ¸ ìœ í˜• í™•ì¸\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        'messages': [\n",
    "            HumanMessage('ì•ˆë…•í•˜ì„¸ìš”, ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”.')\n",
    "        ]\n",
    "    }\n",
    "    config = {'configurable': {'thread_id': '2'}}\n",
    "    \n",
    "    print(\"=== ì´ë²¤íŠ¸ ìœ í˜• ===\")\n",
    "    \n",
    "    event_types = set()\n",
    "    \n",
    "    async for event in graph.astream_events(input_data, config, version=\"v2\"):\n",
    "        event_type = event[\"event\"]\n",
    "        if event_type not in event_types:\n",
    "            event_types.add(event_type)\n",
    "            print(f\"ìƒˆ ì´ë²¤íŠ¸ ìœ í˜•: {event_type}\")\n",
    "    \n",
    "    print(f\"\\në°œê²¬ëœ ì´ë²¤íŠ¸ ìœ í˜•: {len(event_types)}ê°œ\")\n",
    "\n",
    "await show_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ì£¼ìš” ì´ë²¤íŠ¸ í•„í„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def filtered_stream():\n",
    "    \"\"\"\n",
    "    ì£¼ìš” ì´ë²¤íŠ¸ë§Œ í•„í„°ë§í•˜ì—¬ í‘œì‹œ\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        'messages': [\n",
    "            HumanMessage('íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”? í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.')\n",
    "        ]\n",
    "    }\n",
    "    config = {'configurable': {'thread_id': '3'}}\n",
    "    \n",
    "    print(\"=== í•„í„°ë§ëœ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ===\")\n",
    "    \n",
    "    async for event in graph.astream_events(input_data, config, version=\"v2\"):\n",
    "        event_type = event[\"event\"]\n",
    "        \n",
    "        if event_type == \"on_chain_start\":\n",
    "            name = event.get(\"name\", \"unknown\")\n",
    "            print(f\"ğŸš€ ì²´ì¸ ì‹œì‘: {name}\")\n",
    "            \n",
    "        elif event_type == \"on_chat_model_start\":\n",
    "            print(f\"ğŸ¤– LLM í˜¸ì¶œ ì‹œì‘\")\n",
    "            \n",
    "        elif event_type == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                print(f\"ğŸ“ í† í°: '{content}'\")\n",
    "                \n",
    "        elif event_type == \"on_chat_model_end\":\n",
    "            print(f\"âœ… LLM í˜¸ì¶œ ì™„ë£Œ\")\n",
    "            \n",
    "        elif event_type == \"on_chain_end\":\n",
    "            name = event.get(\"name\", \"unknown\")\n",
    "            if name == \"LangGraph\":\n",
    "                print(f\"ğŸ ê·¸ë˜í”„ ì™„ë£Œ\")\n",
    "\n",
    "await filtered_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì •ë¦¬: Token Streaming\n",
    "\n",
    "### í•µì‹¬ ì½”ë“œ\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def stream_tokens():\n",
    "    async for event in graph.astream_events(input, config, version=\"v2\"):\n",
    "        if event[\"event\"] == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                print(content, end='', flush=True)\n",
    "\n",
    "# ì‹¤í–‰\n",
    "await stream_tokens()  # ë˜ëŠ” asyncio.run(stream_tokens())\n",
    "```\n",
    "\n",
    "### ì£¼ìš” ì´ë²¤íŠ¸ ìœ í˜•\n",
    "\n",
    "| ì´ë²¤íŠ¸ | ì„¤ëª… | ìš©ë„ |\n",
    "|--------|------|------|\n",
    "| **on_chat_model_stream** | LLM í† í° ì¶œë ¥ | ì‹¤ì‹œê°„ ì‘ë‹µ í‘œì‹œ |\n",
    "| **on_chat_model_start** | LLM í˜¸ì¶œ ì‹œì‘ | ë¡œë”© í‘œì‹œ |\n",
    "| **on_chat_model_end** | LLM í˜¸ì¶œ ì™„ë£Œ | ë¡œë”© ì¢…ë£Œ |\n",
    "| **on_chain_start/end** | ì²´ì¸ ì‹œì‘/ì™„ë£Œ | ì§„í–‰ ìƒí™© ì¶”ì  |\n",
    "| **on_tool_start/end** | ë„êµ¬ ì‹œì‘/ì™„ë£Œ | ë„êµ¬ ì‹¤í–‰ ì¶”ì  |\n",
    "\n",
    "### ì£¼ì˜ ì‚¬í•­\n",
    "\n",
    "- `astream_events`ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ (`async for` í•„ìš”)\n",
    "- `checkpointer`ê°€ í•„ìš”í•¨ (MemorySaver ë“±)\n",
    "- `version=\"v2\"` ê¶Œì¥\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0.1).bind_tools(tools)\n",
    "\n",
    "# ë³€ê²½\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1).bind_tools(tools)\n",
    "```\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "**Interrupt**ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ê³  ì¬ê°œí•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. (05-06ë²ˆ ë…¸íŠ¸ë¶)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
