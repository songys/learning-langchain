{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8ì¥. í”„ë¡œë•ì…˜ ê¸°ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community langchain-openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\nimport os\n\nos.environ['OPENAI_API_KEY']=userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## ì½”ë“œ 8-1~8-2 Structured Output: êµ¬ì¡°í™”ëœ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output: êµ¬ì¡°í™”ëœ ì¶œë ¥\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **LLMì˜ ì¶œë ¥ì„ íŠ¹ì • êµ¬ì¡°ë¡œ ê°•ì œ**í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## Structured Outputì´ë€?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Structured Outputì˜ ê°œë…                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   ì¼ë°˜ LLM ì¶œë ¥:                                                   â”‚\n",
    "â”‚   \"ê³ ì–‘ì´ ë†ë‹´ì´ìš”? ìŒ... ê³ ì–‘ì´ê°€ ì»´í“¨í„°ë¥¼ ì¢‹ì•„í•˜ëŠ” ì´ìœ ëŠ”...\"    â”‚\n",
    "â”‚   â†’ ììœ  í˜•ì‹ í…ìŠ¤íŠ¸ (íŒŒì‹±í•˜ê¸° ì–´ë ¤ì›€)                            â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   Structured Output:                                               â”‚\n",
    "â”‚   {                                                                â”‚\n",
    "â”‚     \"setup\": \"ê³ ì–‘ì´ê°€ ì»´í“¨í„°ë¥¼ ì¢‹ì•„í•˜ëŠ” ì´ìœ ëŠ”?\",               â”‚\n",
    "â”‚     \"punchline\": \"ë§ˆìš°ìŠ¤ê°€ ìˆìœ¼ë‹ˆê¹Œ!\"                            â”‚\n",
    "â”‚   }                                                                â”‚\n",
    "â”‚   â†’ ì •í•´ì§„ êµ¬ì¡°ë¡œ ì¶œë ¥ (íŒŒì‹± ìš©ì´, íƒ€ì… ì•ˆì „)                     â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ì™œ í•„ìš”í• ê¹Œìš”?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Structured Output ì‚¬ìš© ì‚¬ë¡€                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   â€¢ API ì‘ë‹µ ìƒì„±: íŠ¹ì • JSON í˜•ì‹ í•„ìš”                            â”‚\n",
    "â”‚   â€¢ ë°ì´í„° ì¶”ì¶œ: ì´ë¦„, ë‚ ì§œ, ê¸ˆì•¡ ë“± êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ           â”‚\n",
    "â”‚   â€¢ ë¶„ë¥˜ ì‘ì—…: ì •í•´ì§„ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ ì„ íƒ                       â”‚\n",
    "â”‚   â€¢ ì—ì´ì „íŠ¸ ê²°ì •: ë‹¤ìŒ í–‰ë™ ì„ íƒ (Supervisor íŒ¨í„´)               â”‚\n",
    "â”‚   â€¢ í¼ ë°ì´í„° ìƒì„±: ì‚¬ìš©ì ì…ë ¥ ê²€ì¦                              â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pydantic ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "\n",
    "ì¶œë ¥ êµ¬ì¡°ë¥¼ Pydantic ëª¨ë¸ë¡œ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"\n",
    "    ë†ë‹´ êµ¬ì¡°\n",
    "    \n",
    "    setup: ë†ë‹´ì˜ ì„¤ì • (\"ì™œ ê³ ì–‘ì´ê°€...\")\n",
    "    punchline: ë†ë‹´ì˜ í¬ì¸íŠ¸ (\"ë§ˆìš°ìŠ¤ê°€ ìˆìœ¼ë‹ˆê¹Œ!\")\n",
    "    \"\"\"\n",
    "    setup: str = Field(description='ë†ë‹´ì˜ ì„¤ì •')\n",
    "    punchline: str = Field(description='ë†ë‹´ì˜ í¬ì¸íŠ¸')\n",
    "\n",
    "print(\"âœ… Joke ìŠ¤í‚¤ë§ˆ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"   - setup: ë†ë‹´ì˜ ì„¤ì •\")\n",
    "print(\"   - punchline: ë†ë‹´ì˜ í¬ì¸íŠ¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LLMì— Structured Output ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# ê¸°ë³¸ ëª¨ë¸\n",
    "base_model = ChatOllama(model='llama3.2', temperature=0)\n",
    "\n",
    "# Structured Output ì ìš©\n",
    "model = base_model.with_structured_output(Joke)\n",
    "\n",
    "print(\"âœ… Structured Output LLM ì„¤ì • ì™„ë£Œ\")\n",
    "print(\"   í•­ìƒ Joke í˜•íƒœë¡œ ì‘ë‹µí•¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ì‹¤í–‰ ë° ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë†ë‹´ ìƒì„±\n",
    "result = model.invoke('ê³ ì–‘ì´ì— ëŒ€í•œ ë†ë‹´ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.')\n",
    "\n",
    "print(\"=== Structured Output ê²°ê³¼ ===\")\n",
    "print(f\"íƒ€ì…: {type(result)}\")\n",
    "print(f\"\\nê²°ê³¼: {result}\")\n",
    "print(f\"\\nì„¤ì •: {result.setup}\")\n",
    "print(f\"í¬ì¸íŠ¸: {result.punchline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ë‹¤ì–‘í•œ ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ì˜ˆì‹œ 1: ê°ì • ë¶„ì„\n",
    "class SentimentAnalysis(BaseModel):\n",
    "    sentiment: Literal['positive', 'negative', 'neutral'] = Field(\n",
    "        description='í…ìŠ¤íŠ¸ì˜ ê°ì •'\n",
    "    )\n",
    "    confidence: float = Field(\n",
    "        description='ì‹ ë¢°ë„ (0.0 ~ 1.0)',\n",
    "        ge=0.0,\n",
    "        le=1.0\n",
    "    )\n",
    "    reason: str = Field(description='íŒë‹¨ ì´ìœ ')\n",
    "\n",
    "# ì˜ˆì‹œ 2: ê°œì²´ëª… ì¸ì‹\n",
    "class Entity(BaseModel):\n",
    "    name: str = Field(description='ê°œì²´ ì´ë¦„')\n",
    "    type: Literal['person', 'organization', 'location', 'date', 'other'] = Field(\n",
    "        description='ê°œì²´ ìœ í˜•'\n",
    "    )\n",
    "\n",
    "class EntityExtraction(BaseModel):\n",
    "    entities: list[Entity] = Field(description='ì¶”ì¶œëœ ê°œì²´ ëª©ë¡')\n",
    "\n",
    "# ì˜ˆì‹œ 3: ì‘ì—… ë¶„ë¥˜\n",
    "class TaskClassification(BaseModel):\n",
    "    category: Literal['question', 'request', 'complaint', 'feedback'] = Field(\n",
    "        description='ì‘ì—… ì¹´í…Œê³ ë¦¬'\n",
    "    )\n",
    "    priority: Literal['low', 'medium', 'high', 'urgent'] = Field(\n",
    "        description='ìš°ì„ ìˆœìœ„'\n",
    "    )\n",
    "    summary: str = Field(description='ìš”ì•½')\n",
    "\n",
    "print(\"âœ… ë‹¤ì–‘í•œ ìŠ¤í‚¤ë§ˆ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ì • ë¶„ì„ í…ŒìŠ¤íŠ¸\n",
    "sentiment_model = base_model.with_structured_output(SentimentAnalysis)\n",
    "\n",
    "text = \"ì´ ì œí’ˆ ì •ë§ ìµœê³ ì˜ˆìš”! ë°°ì†¡ë„ ë¹ ë¥´ê³  í’ˆì§ˆë„ ì¢‹ìŠµë‹ˆë‹¤.\"\n",
    "result = sentiment_model.invoke(f\"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”: {text}\")\n",
    "\n",
    "print(\"=== ê°ì • ë¶„ì„ ê²°ê³¼ ===\")\n",
    "print(f\"í…ìŠ¤íŠ¸: {text}\")\n",
    "print(f\"\\nê°ì •: {result.sentiment}\")\n",
    "print(f\"ì‹ ë¢°ë„: {result.confidence}\")\n",
    "print(f\"ì´ìœ : {result.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°œì²´ëª… ì¸ì‹ í…ŒìŠ¤íŠ¸\n",
    "entity_model = base_model.with_structured_output(EntityExtraction)\n",
    "\n",
    "text = \"ì‚¼ì„±ì „ìì˜ ì´ì¬ìš© íšŒì¥ì´ 2024ë…„ 1ì›” ì„œìš¸ì—ì„œ ê¸°ìíšŒê²¬ì„ ì—´ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "result = entity_model.invoke(f\"ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°œì²´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”: {text}\")\n",
    "\n",
    "print(\"=== ê°œì²´ëª… ì¸ì‹ ê²°ê³¼ ===\")\n",
    "print(f\"í…ìŠ¤íŠ¸: {text}\")\n",
    "print(f\"\\nì¶”ì¶œëœ ê°œì²´:\")\n",
    "for entity in result.entities:\n",
    "    print(f\"  - {entity.name} ({entity.type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì •ë¦¬: Structured Output\n",
    "\n",
    "### í•µì‹¬ ì½”ë“œ\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 1. ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "class MyOutput(BaseModel):\n",
    "    field1: str = Field(description='ì„¤ëª…')\n",
    "    field2: int = Field(description='ì„¤ëª…')\n",
    "\n",
    "# 2. LLMì— ì ìš©\n",
    "model = base_model.with_structured_output(MyOutput)\n",
    "\n",
    "# 3. ì‹¤í–‰ (ê²°ê³¼ê°€ MyOutput íƒ€ì…)\n",
    "result = model.invoke('í”„ë¡¬í”„íŠ¸')\n",
    "print(result.field1)  # íƒ€ì… ì•ˆì „\n",
    "```\n",
    "\n",
    "### Field ì˜µì…˜\n",
    "\n",
    "| ì˜µì…˜ | ì„¤ëª… | ì˜ˆì‹œ |\n",
    "|------|------|------|\n",
    "| **description** | í•„ë“œ ì„¤ëª… | `Field(description='ì´ë¦„')` |\n",
    "| **default** | ê¸°ë³¸ê°’ | `Field(default='ê¸°ë³¸')` |\n",
    "| **ge/le** | ìˆ«ì ë²”ìœ„ | `Field(ge=0, le=100)` |\n",
    "| **min_length** | ë¬¸ìì—´ ìµœì†Œ ê¸¸ì´ | `Field(min_length=1)` |\n",
    "\n",
    "### Literalë¡œ ì„ íƒì§€ ì œí•œ\n",
    "\n",
    "```python\n",
    "from typing import Literal\n",
    "\n",
    "class Decision(BaseModel):\n",
    "    choice: Literal['option1', 'option2', 'option3']  # ì´ ì¤‘ í•˜ë‚˜ë§Œ ê°€ëŠ¥\n",
    "```\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "model = model.with_structured_output(Joke)\n",
    "\n",
    "# ë³€ê²½\n",
    "model = ChatOllama(model='llama3.2', temperature=0)\n",
    "model = model.with_structured_output(Joke)\n",
    "```\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "**Streaming**ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥ì„ ë°›ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. (03ë²ˆ ë…¸íŠ¸ë¶)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## ì½”ë“œ 8-3 Streaming Output: ë…¸ë“œë³„ ìŠ¤íŠ¸ë¦¬ë°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Output: ë…¸ë“œë³„ ìŠ¤íŠ¸ë¦¬ë°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ë…¸ë“œë³„ë¡œ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°**í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## Streamingì´ë€?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Streamingì˜ ê°œë…                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   ì¼ë°˜ ì‹¤í–‰ (invoke):                                              â”‚\n",
    "â”‚   ì…ë ¥ â†’ [ì²˜ë¦¬ ì¤‘...] â†’ ì „ì²´ ê²°ê³¼ ë°˜í™˜                             â”‚\n",
    "â”‚   (ëª¨ë“  ì²˜ë¦¬ê°€ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼)                                 â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (stream):                                          â”‚\n",
    "â”‚   ì…ë ¥ â†’ ë…¸ë“œ1 ê²°ê³¼ â†’ ë…¸ë“œ2 ê²°ê³¼ â†’ ë…¸ë“œ3 ê²°ê³¼ â†’ ...               â”‚\n",
    "â”‚   (ê° ë…¸ë“œ ì™„ë£Œ ì‹œë§ˆë‹¤ ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜)                             â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   ì¥ì :                                                            â”‚\n",
    "â”‚   â€¢ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í™•ì¸                                          â”‚\n",
    "â”‚   â€¢ ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ (ì‘ë‹µì„±)                                      â”‚\n",
    "â”‚   â€¢ ë””ë²„ê¹… ìš©ì´                                                    â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## stream_mode ì˜µì…˜\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    stream_mode ë¹„êµ                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   stream_mode='values' (ê¸°ë³¸):                                     â”‚\n",
    "â”‚   â†’ ê° ë…¸ë“œ í›„ ì „ì²´ State ë°˜í™˜                                    â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   stream_mode='updates':                                           â”‚\n",
    "â”‚   â†’ ê° ë…¸ë“œê°€ ë³€ê²½í•œ ë¶€ë¶„ë§Œ ë°˜í™˜                                  â”‚\n",
    "â”‚   â†’ ë” íš¨ìœ¨ì , ë³€ê²½ ì‚¬í•­ ì¶”ì  ìš©ì´                                â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(query: str) -> str:\n",
    "    '''ê³„ì‚°ê¸°. ìˆ˜ì‹ë§Œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.'''\n",
    "    return ast.literal_eval(query)\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1).bind_tools(tools)\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    res = model.invoke(state['messages'])\n",
    "    return {'messages': res}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node('model', model_node)\n",
    "builder.add_node('tools', ToolNode(tools))\n",
    "builder.add_edge(START, 'model')\n",
    "builder.add_conditional_edges('model', tools_condition)\n",
    "builder.add_edge('tools', 'model')\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "print(\"âœ… ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° (values ëª¨ë“œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    'messages': [\n",
    "        HumanMessage(\n",
    "            'ë¯¸êµ­ì˜ ì œ30ëŒ€ ëŒ€í†µë ¹ì´ ì‚¬ë§í–ˆì„ ë•Œ ëª‡ ì‚´ì´ì—ˆë‚˜ìš”?'\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=== stream_mode='values' (ê¸°ë³¸) ===\")\n",
    "print(\"ê° ë…¸ë“œ í›„ ì „ì²´ State ë°˜í™˜\\n\")\n",
    "\n",
    "for i, chunk in enumerate(graph.stream(input_data)):\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    for node_name, node_output in chunk.items():\n",
    "        print(f\"ë…¸ë“œ: {node_name}\")\n",
    "        if 'messages' in node_output:\n",
    "            last_msg = node_output['messages'][-1] if isinstance(node_output['messages'], list) else node_output['messages']\n",
    "            content = str(last_msg.content)[:100] if hasattr(last_msg, 'content') else str(last_msg)[:100]\n",
    "            print(f\"ë‚´ìš©: {content}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. updates ëª¨ë“œ ìŠ¤íŠ¸ë¦¬ë°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== stream_mode='updates' ===\")\n",
    "print(\"ê° ë…¸ë“œê°€ ë³€ê²½í•œ ë¶€ë¶„ë§Œ ë°˜í™˜\\n\")\n",
    "\n",
    "for i, chunk in enumerate(graph.stream(input_data, stream_mode='updates')):\n",
    "    print(f\"--- Update {i+1} ---\")\n",
    "    for node_name, node_output in chunk.items():\n",
    "        print(f\"ë…¸ë“œ: {node_name}\")\n",
    "        print(f\"ë³€ê²½ ë‚´ìš©: {str(node_output)[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=== ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ===\")\n",
    "print(f\"ì§ˆë¬¸: {input_data['messages'][0].content}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for chunk in graph.stream(input_data, stream_mode='updates'):\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    for node_name, node_output in chunk.items():\n",
    "        if node_name == 'model':\n",
    "            msg = node_output.get('messages', [])[-1] if node_output.get('messages') else None\n",
    "            if msg and hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                print(f\"[{elapsed:.1f}s] ğŸ”§ ë„êµ¬ í˜¸ì¶œ ê²°ì •: {[tc['name'] for tc in msg.tool_calls]}\")\n",
    "            elif msg and hasattr(msg, 'content') and msg.content:\n",
    "                print(f\"[{elapsed:.1f}s] ğŸ’¬ ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
    "        elif node_name == 'tools':\n",
    "            print(f\"[{elapsed:.1f}s] âš¡ ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ\")\n",
    "\n",
    "print(f\"\\nì´ ì†Œìš” ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì •ë¦¬: Streaming Output\n",
    "\n",
    "### í•µì‹¬ ì½”ë“œ\n",
    "\n",
    "```python\n",
    "# ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° (ì „ì²´ State)\n",
    "for chunk in graph.stream(input_data):\n",
    "    print(chunk)\n",
    "\n",
    "# updates ëª¨ë“œ (ë³€ê²½ ì‚¬í•­ë§Œ)\n",
    "for chunk in graph.stream(input_data, stream_mode='updates'):\n",
    "    print(chunk)\n",
    "```\n",
    "\n",
    "### stream_mode ë¹„êµ\n",
    "\n",
    "| ëª¨ë“œ | ë°˜í™˜ ë‚´ìš© | ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ |\n",
    "|------|----------|---------------|\n",
    "| **values** | ì „ì²´ State | ì „ì²´ ìƒíƒœ í•„ìš” ì‹œ |\n",
    "| **updates** | ë³€ê²½ëœ ë¶€ë¶„ë§Œ | íš¨ìœ¨ì ì¸ ì²˜ë¦¬, ë³€ê²½ ì¶”ì  |\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0.1).bind_tools(tools)\n",
    "\n",
    "# ë³€ê²½\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1).bind_tools(tools)\n",
    "```\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "**í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°**ìœ¼ë¡œ ë” ì„¸ë°€í•œ ì‹¤ì‹œê°„ ì¶œë ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤. (04ë²ˆ ë…¸íŠ¸ë¶)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## ì½”ë“œ 8-4 Token Streaming: í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Streaming: í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **LLM ì¶œë ¥ì„ í† í° ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## í† í° ìŠ¤íŠ¸ë¦¬ë°ì´ë€?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    í† í° ìŠ¤íŠ¸ë¦¬ë°ì˜ ê°œë…                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   ë…¸ë“œ ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°:                                              â”‚\n",
    "â”‚   ë…¸ë“œ ì™„ë£Œ â†’ ì „ì²´ ì‘ë‹µ ë°˜í™˜                                       â”‚\n",
    "â”‚   \"ë¯¸êµ­ì˜ 30ëŒ€ ëŒ€í†µë ¹ì€ ìº˜ë¹ˆ ì¿¨ë¦¬ì§€ì…ë‹ˆë‹¤...\" (ì „ì²´)              â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°:                                              â”‚\n",
    "â”‚   \"ë¯¸êµ­\" â†’ \"ì˜\" â†’ \"30\" â†’ \"ëŒ€\" â†’ \"ëŒ€í†µë ¹\" â†’ \"ì€\" â†’ ...          â”‚\n",
    "â”‚   (ì‹¤ì‹œê°„ìœ¼ë¡œ ê¸€ìê°€ ë‚˜íƒ€ë‚¨)                                       â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   ì¥ì :                                                            â”‚\n",
    "â”‚   â€¢ ChatGPTì²˜ëŸ¼ ì‹¤ì‹œê°„ íƒ€ì´í•‘ íš¨ê³¼                                 â”‚\n",
    "â”‚   â€¢ ì‚¬ìš©ìê°€ ê¸°ë‹¤ë¦¬ëŠ” ëŠë‚Œ ê°ì†Œ                                    â”‚\n",
    "â”‚   â€¢ ê¸´ ì‘ë‹µë„ ì¦‰ì‹œ ì‹œì‘                                            â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(query: str) -> str:\n",
    "    '''ê³„ì‚°ê¸°. ìˆ˜ì‹ë§Œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.'''\n",
    "    return ast.literal_eval(query)\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1).bind_tools(tools)\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    res = model.invoke(state['messages'])\n",
    "    return {'messages': res}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node('model', model_node)\n",
    "builder.add_node('tools', ToolNode(tools))\n",
    "builder.add_edge(START, 'model')\n",
    "builder.add_conditional_edges('model', tools_condition)\n",
    "builder.add_edge('tools', 'model')\n",
    "\n",
    "# checkpointer í•„ìš” (astream_events ì‚¬ìš© ì‹œ)\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "print(\"âœ… ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. í† í° ìŠ¤íŠ¸ë¦¬ë° (astream_events)\n",
    "\n",
    "ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def stream_tokens():\n",
    "    \"\"\"\n",
    "    astream_eventsë¥¼ ì‚¬ìš©í•œ í† í° ìŠ¤íŠ¸ë¦¬ë°\n",
    "    \n",
    "    event['event'] == 'on_chat_model_stream' ì¼ ë•Œ\n",
    "    í† í° ë‹¨ìœ„ë¡œ contentë¥¼ ë°›ì„ ìˆ˜ ìˆìŒ\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        'messages': [\n",
    "            HumanMessage(\n",
    "                'ë¯¸êµ­ ì œ30ëŒ€ ëŒ€í†µë ¹ì˜ ì‚¬ë§ ë‹¹ì‹œ ë‚˜ì´ëŠ” ëª‡ ì‚´ì´ì—ˆë‚˜ìš”?'\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "    config = {'configurable': {'thread_id': '1'}}\n",
    "    \n",
    "    print(\"=== í† í° ìŠ¤íŠ¸ë¦¬ë° ===\")\n",
    "    print(f\"ì§ˆë¬¸: {input_data['messages'][0].content}\\n\")\n",
    "    print(\"ì‘ë‹µ: \", end='', flush=True)\n",
    "    \n",
    "    async for event in graph.astream_events(input_data, config, version=\"v2\"):\n",
    "        # LLM ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ìº¡ì²˜\n",
    "        if event[\"event\"] == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                print(content, end='', flush=True)  # í† í° ë‹¨ìœ„ ì¶œë ¥\n",
    "    \n",
    "    print(\"\\n\\nâœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ\")\n",
    "\n",
    "# ë¹„ë™ê¸° ì‹¤í–‰\n",
    "await stream_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ì´ë²¤íŠ¸ ìœ í˜• í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def show_events():\n",
    "    \"\"\"\n",
    "    ë‹¤ì–‘í•œ ì´ë²¤íŠ¸ ìœ í˜• í™•ì¸\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        'messages': [\n",
    "            HumanMessage('ì•ˆë…•í•˜ì„¸ìš”, ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”.')\n",
    "        ]\n",
    "    }\n",
    "    config = {'configurable': {'thread_id': '2'}}\n",
    "    \n",
    "    print(\"=== ì´ë²¤íŠ¸ ìœ í˜• ===\")\n",
    "    \n",
    "    event_types = set()\n",
    "    \n",
    "    async for event in graph.astream_events(input_data, config, version=\"v2\"):\n",
    "        event_type = event[\"event\"]\n",
    "        if event_type not in event_types:\n",
    "            event_types.add(event_type)\n",
    "            print(f\"ìƒˆ ì´ë²¤íŠ¸ ìœ í˜•: {event_type}\")\n",
    "    \n",
    "    print(f\"\\në°œê²¬ëœ ì´ë²¤íŠ¸ ìœ í˜•: {len(event_types)}ê°œ\")\n",
    "\n",
    "await show_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ì£¼ìš” ì´ë²¤íŠ¸ í•„í„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def filtered_stream():\n",
    "    \"\"\"\n",
    "    ì£¼ìš” ì´ë²¤íŠ¸ë§Œ í•„í„°ë§í•˜ì—¬ í‘œì‹œ\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        'messages': [\n",
    "            HumanMessage('íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”? í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.')\n",
    "        ]\n",
    "    }\n",
    "    config = {'configurable': {'thread_id': '3'}}\n",
    "    \n",
    "    print(\"=== í•„í„°ë§ëœ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ===\")\n",
    "    \n",
    "    async for event in graph.astream_events(input_data, config, version=\"v2\"):\n",
    "        event_type = event[\"event\"]\n",
    "        \n",
    "        if event_type == \"on_chain_start\":\n",
    "            name = event.get(\"name\", \"unknown\")\n",
    "            print(f\"ğŸš€ ì²´ì¸ ì‹œì‘: {name}\")\n",
    "            \n",
    "        elif event_type == \"on_chat_model_start\":\n",
    "            print(f\"ğŸ¤– LLM í˜¸ì¶œ ì‹œì‘\")\n",
    "            \n",
    "        elif event_type == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                print(f\"ğŸ“ í† í°: '{content}'\")\n",
    "                \n",
    "        elif event_type == \"on_chat_model_end\":\n",
    "            print(f\"âœ… LLM í˜¸ì¶œ ì™„ë£Œ\")\n",
    "            \n",
    "        elif event_type == \"on_chain_end\":\n",
    "            name = event.get(\"name\", \"unknown\")\n",
    "            if name == \"LangGraph\":\n",
    "                print(f\"ğŸ ê·¸ë˜í”„ ì™„ë£Œ\")\n",
    "\n",
    "await filtered_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì •ë¦¬: Token Streaming\n",
    "\n",
    "### í•µì‹¬ ì½”ë“œ\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def stream_tokens():\n",
    "    async for event in graph.astream_events(input, config, version=\"v2\"):\n",
    "        if event[\"event\"] == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                print(content, end='', flush=True)\n",
    "\n",
    "# ì‹¤í–‰\n",
    "await stream_tokens()  # ë˜ëŠ” asyncio.run(stream_tokens())\n",
    "```\n",
    "\n",
    "### ì£¼ìš” ì´ë²¤íŠ¸ ìœ í˜•\n",
    "\n",
    "| ì´ë²¤íŠ¸ | ì„¤ëª… | ìš©ë„ |\n",
    "|--------|------|------|\n",
    "| **on_chat_model_stream** | LLM í† í° ì¶œë ¥ | ì‹¤ì‹œê°„ ì‘ë‹µ í‘œì‹œ |\n",
    "| **on_chat_model_start** | LLM í˜¸ì¶œ ì‹œì‘ | ë¡œë”© í‘œì‹œ |\n",
    "| **on_chat_model_end** | LLM í˜¸ì¶œ ì™„ë£Œ | ë¡œë”© ì¢…ë£Œ |\n",
    "| **on_chain_start/end** | ì²´ì¸ ì‹œì‘/ì™„ë£Œ | ì§„í–‰ ìƒí™© ì¶”ì  |\n",
    "| **on_tool_start/end** | ë„êµ¬ ì‹œì‘/ì™„ë£Œ | ë„êµ¬ ì‹¤í–‰ ì¶”ì  |\n",
    "\n",
    "### ì£¼ì˜ ì‚¬í•­\n",
    "\n",
    "- `astream_events`ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ (`async for` í•„ìš”)\n",
    "- `checkpointer`ê°€ í•„ìš”í•¨ (MemorySaver ë“±)\n",
    "- `version=\"v2\"` ê¶Œì¥\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0.1).bind_tools(tools)\n",
    "\n",
    "# ë³€ê²½\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1).bind_tools(tools)\n",
    "```\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "**Interrupt**ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ê³  ì¬ê°œí•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. (05-06ë²ˆ ë…¸íŠ¸ë¶)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## ì½”ë“œ 8-5~8-6 Interrupt: ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ë‹¨ ë° ì¬ê°œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interrupt: ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ë‹¨ ë° ì¬ê°œ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **ê·¸ë˜í”„ ì‹¤í–‰ì„ íŠ¹ì • ì§€ì ì—ì„œ ì¤‘ë‹¨í•˜ê³  ì¬ê°œ**í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## Interruptë€?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Interruptì˜ ê°œë…                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   ì¼ë°˜ ì‹¤í–‰:                                                       â”‚\n",
    "â”‚   START â†’ ë…¸ë“œ1 â†’ ë…¸ë“œ2 â†’ ë…¸ë“œ3 â†’ END                             â”‚\n",
    "â”‚   (ì¤‘ê°„ì— ë©ˆì¶œ ìˆ˜ ì—†ìŒ)                                            â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   interrupt_before ì‚¬ìš©:                                           â”‚\n",
    "â”‚   START â†’ ë…¸ë“œ1 â†’ [ì¤‘ë‹¨] â†’ ì‚¬ìš©ì ìŠ¹ì¸ â†’ ë…¸ë“œ2 â†’ ë…¸ë“œ3 â†’ END     â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   ì‚¬ìš© ì‚¬ë¡€:                                                       â”‚\n",
    "â”‚   â€¢ ë„êµ¬ ì‹¤í–‰ ì „ ì‚¬ìš©ì ìŠ¹ì¸                                       â”‚\n",
    "â”‚   â€¢ ê²°ì œ/ì‚­ì œ ë“± ìœ„í—˜í•œ ì‘ì—… ì „ í™•ì¸                               â”‚\n",
    "â”‚   â€¢ Human-in-the-loop ì›Œí¬í”Œë¡œìš°                                   â”‚\n",
    "â”‚   â€¢ ë””ë²„ê¹… ë° ê²€í†                                                  â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ì•„í‚¤í…ì²˜\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Interrupt ë™ì‘ íë¦„                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   1ï¸âƒ£ ì²« ë²ˆì§¸ ì‹¤í–‰                                                 â”‚\n",
    "â”‚   graph.stream(input, config, interrupt_before=[\"model\"])         â”‚\n",
    "â”‚   â†’ select_tools ì‹¤í–‰ â†’ model ì „ì— ì¤‘ë‹¨ â†’ ìƒíƒœ ì €ì¥               â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   2ï¸âƒ£ ì‚¬ìš©ì ê²€í† /ìŠ¹ì¸                                             â”‚\n",
    "â”‚   (ì—¬ê¸°ì„œ ìƒíƒœ í™•ì¸, ìˆ˜ì • ê°€ëŠ¥)                                    â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   3ï¸âƒ£ ë‘ ë²ˆì§¸ ì‹¤í–‰ (ì¬ê°œ)                                          â”‚\n",
    "â”‚   graph.stream(None, config)  # None = ì €ì¥ëœ ìƒíƒœì—ì„œ ì¬ê°œ       â”‚\n",
    "â”‚   â†’ model ì‹¤í–‰ â†’ tools ì‹¤í–‰ â†’ ... â†’ ì™„ë£Œ                          â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull llama3.2\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(query: str) -> str:\n",
    "    '''ê³„ì‚°ê¸°. ìˆ˜ì‹ë§Œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.'''\n",
    "    return ast.literal_eval(query)\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1)\n",
    "\n",
    "tools_retriever = InMemoryVectorStore.from_documents(\n",
    "    [Document(tool.description, metadata={'name': tool.name}) for tool in tools],\n",
    "    embeddings,\n",
    ").as_retriever()\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    selected_tools: list[str]\n",
    "\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    selected_tools = [tool for tool in tools if tool.name in state['selected_tools']]\n",
    "    res = model.bind_tools(selected_tools).invoke(state['messages'])\n",
    "    return {'messages': res}\n",
    "\n",
    "\n",
    "def select_tools(state: State) -> State:\n",
    "    query = state['messages'][-1].content\n",
    "    tool_docs = tools_retriever.invoke(query)\n",
    "    return {'selected_tools': [doc.metadata['name'] for doc in tool_docs]}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node('select_tools', select_tools)\n",
    "builder.add_node('model', model_node)\n",
    "builder.add_node('tools', ToolNode(tools))\n",
    "builder.add_edge(START, 'select_tools')\n",
    "builder.add_edge('select_tools', 'model')\n",
    "builder.add_conditional_edges('model', tools_condition)\n",
    "builder.add_edge('tools', 'model')\n",
    "\n",
    "# MemorySaver í•„ìˆ˜ (ìƒíƒœ ì €ì¥ìš©)\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "print(\"âœ… ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. interrupt_before ì‚¬ìš©\n",
    "\n",
    "íŠ¹ì • ë…¸ë“œ ì‹¤í–‰ **ì „ì—** ì¤‘ë‹¨í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    'messages': [\n",
    "        HumanMessage(\n",
    "            'ë¯¸êµ­ ì œ30ëŒ€ ëŒ€í†µë ¹ì˜ ì‚¬ë§ ë‹¹ì‹œ ë‚˜ì´ëŠ” ëª‡ ì‚´ì´ì—ˆë‚˜ìš”?'\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "print(\"=== ì²« ë²ˆì§¸ ì‹¤í–‰ (model ì „ì— ì¤‘ë‹¨) ===\")\n",
    "print(f\"ì§ˆë¬¸: {input_data['messages'][0].content}\\n\")\n",
    "\n",
    "# interrupt_before=[\"model\"]: model ë…¸ë“œ ì‹¤í–‰ ì „ì— ì¤‘ë‹¨\n",
    "output = graph.stream(input_data, config, interrupt_before=[\"model\"])\n",
    "\n",
    "for chunk in output:\n",
    "    for node_name, node_output in chunk.items():\n",
    "        print(f\"ë…¸ë“œ: {node_name}\")\n",
    "        print(f\"  ì¶œë ¥: {node_output}\")\n",
    "    print()\n",
    "\n",
    "print(\"ğŸ›‘ model ë…¸ë“œ ì „ì— ì¤‘ë‹¨ë¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ì¤‘ë‹¨ëœ ìƒíƒœ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜„ì¬ ì €ì¥ëœ ìƒíƒœ í™•ì¸\n",
    "state = graph.get_state(config)\n",
    "\n",
    "print(\"=== ì¤‘ë‹¨ëœ ìƒíƒœ ===\")\n",
    "print(f\"ë©”ì‹œì§€: {state.values.get('messages', [])}\")\n",
    "print(f\"ì„ íƒëœ ë„êµ¬: {state.values.get('selected_tools', [])}\")\n",
    "print(f\"\\në‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ: {state.next}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ì¬ê°œ (Resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ë‘ ë²ˆì§¸ ì‹¤í–‰ (ì¬ê°œ) ===\")\n",
    "print(\"ì‚¬ìš©ìê°€ ìŠ¹ì¸í–ˆë‹¤ê³  ê°€ì •\\n\")\n",
    "\n",
    "# Noneì„ ì „ë‹¬í•˜ë©´ ì €ì¥ëœ ìƒíƒœì—ì„œ ì¬ê°œ\n",
    "output = graph.stream(None, config)\n",
    "\n",
    "for chunk in output:\n",
    "    for node_name, node_output in chunk.items():\n",
    "        print(f\"ë…¸ë“œ: {node_name}\")\n",
    "        if 'messages' in node_output:\n",
    "            last_msg = node_output['messages'][-1] if isinstance(node_output['messages'], list) else node_output['messages']\n",
    "            if hasattr(last_msg, 'content') and last_msg.content:\n",
    "                print(f\"  ì‘ë‹µ: {str(last_msg.content)[:200]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. ìƒˆ ì…ë ¥ìœ¼ë¡œ ì¬ê°œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆë¡œìš´ ìŠ¤ë ˆë“œë¡œ ì‹œì‘\n",
    "config2 = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "print(\"=== ì²« ë²ˆì§¸ ì‹¤í–‰ (ì¤‘ë‹¨) ===\")\n",
    "output = graph.stream(input_data, config2, interrupt_before=[\"model\"])\n",
    "for chunk in output:\n",
    "    pass  # ì¶œë ¥ ìƒëµ\n",
    "print(\"ğŸ›‘ ì¤‘ë‹¨ë¨\\n\")\n",
    "\n",
    "# ìƒˆ ì…ë ¥ìœ¼ë¡œ ì¬ê°œ (ì§ˆë¬¸ ë³€ê²½)\n",
    "new_input = {\n",
    "    'messages': [\n",
    "        HumanMessage(\n",
    "            'ì˜í™” ë³´ì´í›„ë“œì˜ ì´¬ì˜ ê¸°ê°„ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?'\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=== ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìƒˆ ì…ë ¥ìœ¼ë¡œ ì¬ê°œ) ===\")\n",
    "print(f\"ìƒˆ ì§ˆë¬¸: {new_input['messages'][0].content}\\n\")\n",
    "\n",
    "# ìƒˆ ì…ë ¥ ì „ë‹¬ (ê¸°ì¡´ ìƒíƒœì— ì¶”ê°€ë¨)\n",
    "output = graph.stream(new_input, config2)\n",
    "\n",
    "for chunk in output:\n",
    "    for node_name, node_output in chunk.items():\n",
    "        print(f\"ë…¸ë“œ: {node_name}\")\n",
    "        if 'messages' in node_output:\n",
    "            last_msg = node_output['messages'][-1] if isinstance(node_output['messages'], list) else node_output['messages']\n",
    "            if hasattr(last_msg, 'content') and last_msg.content:\n",
    "                print(f\"  ì‘ë‹µ: {str(last_msg.content)[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì •ë¦¬: Interrupt\n",
    "\n",
    "### í•µì‹¬ ì½”ë“œ\n",
    "\n",
    "```python\n",
    "# 1. checkpointer í•„ìˆ˜\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# 2. interrupt_beforeë¡œ ì¤‘ë‹¨ ì§€ì  ì§€ì •\n",
    "output = graph.stream(input, config, interrupt_before=[\"model\"])\n",
    "\n",
    "# 3. ìƒíƒœ í™•ì¸\n",
    "state = graph.get_state(config)\n",
    "print(state.values)  # í˜„ì¬ ìƒíƒœ\n",
    "print(state.next)    # ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ\n",
    "\n",
    "# 4. ì¬ê°œ (ì €ì¥ëœ ìƒíƒœì—ì„œ)\n",
    "output = graph.stream(None, config)\n",
    "\n",
    "# 5. ìƒˆ ì…ë ¥ìœ¼ë¡œ ì¬ê°œ\n",
    "output = graph.stream(new_input, config)\n",
    "```\n",
    "\n",
    "### interrupt_before vs interrupt_after\n",
    "\n",
    "| ì˜µì…˜ | ì¤‘ë‹¨ ì‹œì  | ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ |\n",
    "|------|----------|---------------|\n",
    "| **interrupt_before** | ë…¸ë“œ ì‹¤í–‰ ì „ | ì‹¤í–‰ ì „ ìŠ¹ì¸ |\n",
    "| **interrupt_after** | ë…¸ë“œ ì‹¤í–‰ í›„ | ê²°ê³¼ ê²€í†  í›„ ì§„í–‰ |\n",
    "\n",
    "### ì‚¬ìš© ì‚¬ë¡€\n",
    "\n",
    "| ì‚¬ë¡€ | ì„¤ëª… |\n",
    "|------|------|\n",
    "| ë„êµ¬ ìŠ¹ì¸ | ìœ„í—˜í•œ ë„êµ¬ ì‹¤í–‰ ì „ í™•ì¸ |\n",
    "| ê²°ì œ í™•ì¸ | ê²°ì œ API í˜¸ì¶œ ì „ ìŠ¹ì¸ |\n",
    "| ë°ì´í„° ì‚­ì œ | ì‚­ì œ ì‘ì—… ì „ í™•ì¸ |\n",
    "| Human-in-the-loop | ì‚¬ëŒì˜ ê²€í†  í•„ìš” |\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸\n",
    "embeddings = OpenAIEmbeddings()\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0.1)\n",
    "\n",
    "# ë³€ê²½\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1)\n",
    "```\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "**ìƒíƒœ ìˆ˜ì •** ë° **íˆìŠ¤í† ë¦¬ì—ì„œ Fork**í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. (07-11ë²ˆ ë…¸íŠ¸ë¶)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## ì½”ë“œ 8-7~8-11 ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬: ìˆ˜ì •, ì¬ê°œ, Fork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬: ìˆ˜ì •, ì¬ê°œ, Fork\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **ìƒíƒœ ìˆ˜ì •, ë‹¤ì–‘í•œ ì¬ê°œ ë°©ë²•, íˆìŠ¤í† ë¦¬ì—ì„œ Fork**í•˜ëŠ” ê³ ê¸‰ ê¸°ëŠ¥ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬ ê¸°ëŠ¥\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬                                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   1ï¸âƒ£ get_state: í˜„ì¬ ìƒíƒœ ì¡°íšŒ                                    â”‚\n",
    "â”‚   2ï¸âƒ£ update_state: ìƒíƒœ ìˆ˜ì •                                      â”‚\n",
    "â”‚   3ï¸âƒ£ get_state_history: ê³¼ê±° ìƒíƒœ íˆìŠ¤í† ë¦¬ ì¡°íšŒ                   â”‚\n",
    "â”‚   4ï¸âƒ£ Fork: ê³¼ê±° ì‹œì ì—ì„œ ë¶„ê¸°í•˜ì—¬ ìƒˆë¡œ ì‹¤í–‰                       â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   í™œìš©:                                                            â”‚\n",
    "â”‚   â€¢ ì¤‘ë‹¨ ì‹œì ì—ì„œ ìƒíƒœ ìˆ˜ì • í›„ ì¬ê°œ                                â”‚\n",
    "â”‚   â€¢ ì˜ëª»ëœ ê²°ì • ì‹œ ê³¼ê±°ë¡œ ëŒì•„ê°€ê¸°                                 â”‚\n",
    "â”‚   â€¢ What-if ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸                                        â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull llama3.2\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(query: str) -> str:\n",
    "    '''ê³„ì‚°ê¸°. ìˆ˜ì‹ë§Œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.'''\n",
    "    return ast.literal_eval(query)\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1)\n",
    "\n",
    "tools_retriever = InMemoryVectorStore.from_documents(\n",
    "    [Document(tool.description, metadata={'name': tool.name}) for tool in tools],\n",
    "    embeddings,\n",
    ").as_retriever()\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    selected_tools: list[str]\n",
    "\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    selected_tools = [tool for tool in tools if tool.name in state['selected_tools']]\n",
    "    res = model.bind_tools(selected_tools).invoke(state['messages'])\n",
    "    return {'messages': res}\n",
    "\n",
    "\n",
    "def select_tools(state: State) -> State:\n",
    "    query = state['messages'][-1].content\n",
    "    tool_docs = tools_retriever.invoke(query)\n",
    "    return {'selected_tools': [doc.metadata['name'] for doc in tool_docs]}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node('select_tools', select_tools)\n",
    "builder.add_node('model', model_node)\n",
    "builder.add_node('tools', ToolNode(tools))\n",
    "builder.add_edge(START, 'select_tools')\n",
    "builder.add_edge('select_tools', 'model')\n",
    "builder.add_conditional_edges('model', tools_condition)\n",
    "builder.add_edge('tools', 'model')\n",
    "\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "print(\"âœ… ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ìƒíƒœ ìˆ˜ì • í›„ ì¬ê°œ (update_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    'messages': [\n",
    "        HumanMessage(\n",
    "            'ë¯¸êµ­ ì œ30ëŒ€ ëŒ€í†µë ¹ì˜ ì‚¬ë§ ë‹¹ì‹œ ë‚˜ì´ëŠ” ëª‡ ì‚´ì´ì—ˆë‚˜ìš”?'\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "config = {'configurable': {'thread_id': 'edit-state-1'}}\n",
    "\n",
    "print(\"=== 1ë‹¨ê³„: ì¤‘ë‹¨ ===\")\n",
    "output = graph.stream(input_data, config, interrupt_before=[\"model\"])\n",
    "for chunk in output:\n",
    "    for node_name, node_output in chunk.items():\n",
    "        print(f\"ë…¸ë“œ: {node_name}\")\n",
    "print(\"ğŸ›‘ model ì „ì— ì¤‘ë‹¨ë¨\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜„ì¬ ìƒíƒœ í™•ì¸\n",
    "state = graph.get_state(config)\n",
    "\n",
    "print(\"=== 2ë‹¨ê³„: í˜„ì¬ ìƒíƒœ í™•ì¸ ===\")\n",
    "print(f\"í˜„ì¬ ì§ˆë¬¸: {state.values['messages'][0].content}\")\n",
    "print(f\"ì„ íƒëœ ë„êµ¬: {state.values.get('selected_tools', [])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ ìˆ˜ì •: ì§ˆë¬¸ ë³€ê²½\n",
    "print(\"=== 3ë‹¨ê³„: ìƒíƒœ ìˆ˜ì • ===\")\n",
    "\n",
    "# ë©”ì‹œì§€ ë³µì‚¬ í›„ ì²« ë²ˆì§¸ ë©”ì‹œì§€ ë³€ê²½\n",
    "updated_messages = state.values['messages'].copy()\n",
    "updated_messages[0] = HumanMessage(content=\"ì˜í™” ë³´ì´í›„ë“œì˜ ì´¬ì˜ ê¸°ê°„ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?\")\n",
    "\n",
    "# ìƒíƒœ ì—…ë°ì´íŠ¸ ì ìš©\n",
    "update = {'messages': updated_messages}\n",
    "graph.update_state(config, update)\n",
    "\n",
    "# ìˆ˜ì •ëœ ìƒíƒœ í™•ì¸\n",
    "new_state = graph.get_state(config)\n",
    "print(f\"ë³€ê²½ëœ ì§ˆë¬¸: {new_state.values['messages'][0].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ì •ëœ ìƒíƒœë¡œ ì¬ê°œ\n",
    "print(\"=== 4ë‹¨ê³„: ìˆ˜ì •ëœ ìƒíƒœë¡œ ì¬ê°œ ===\")\n",
    "\n",
    "output = graph.stream(None, config)\n",
    "\n",
    "for chunk in output:\n",
    "    for node_name, node_output in chunk.items():\n",
    "        print(f\"ë…¸ë“œ: {node_name}\")\n",
    "        if 'messages' in node_output:\n",
    "            last_msg = node_output['messages'][-1] if isinstance(node_output['messages'], list) else node_output['messages']\n",
    "            if hasattr(last_msg, 'content') and last_msg.content:\n",
    "                print(f\"  ì‘ë‹µ: {str(last_msg.content)[:200]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… ì™„ë£Œ (ë³€ê²½ëœ ì§ˆë¬¸ìœ¼ë¡œ ì‹¤í–‰ë¨)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. íˆìŠ¤í† ë¦¬ ì¡°íšŒ (get_state_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆ ìŠ¤ë ˆë“œë¡œ ì „ì²´ ì‹¤í–‰\n",
    "config2 = {'configurable': {'thread_id': 'history-1'}}\n",
    "\n",
    "print(\"=== ì „ì²´ ì‹¤í–‰ ===\")\n",
    "output = graph.stream(input_data, config2)\n",
    "for chunk in output:\n",
    "    for node_name in chunk.keys():\n",
    "        print(f\"ë…¸ë“œ ì‹¤í–‰: {node_name}\")\n",
    "\n",
    "print(\"\\nâœ… ì‹¤í–‰ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íˆìŠ¤í† ë¦¬ ì¡°íšŒ\n",
    "print(\"=== ìƒíƒœ íˆìŠ¤í† ë¦¬ ===\")\n",
    "\n",
    "history = list(graph.get_state_history(config2))\n",
    "\n",
    "print(f\"ì´ {len(history)}ê°œì˜ ìƒíƒœ ê¸°ë¡\\n\")\n",
    "\n",
    "for i, state in enumerate(history):\n",
    "    print(f\"[{i}] ì²´í¬í¬ì¸íŠ¸: {state.config['configurable'].get('checkpoint_id', 'N/A')[:20]}...\")\n",
    "    print(f\"    ë‹¤ìŒ ë…¸ë“œ: {state.next}\")\n",
    "    print(f\"    ë©”ì‹œì§€ ìˆ˜: {len(state.values.get('messages', []))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. íˆìŠ¤í† ë¦¬ì—ì„œ Fork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³¼ê±° ì‹œì ì—ì„œ ë‹¤ì‹œ ì‹¤í–‰ (Fork)\n",
    "print(\"=== íˆìŠ¤í† ë¦¬ì—ì„œ Fork ===\")\n",
    "\n",
    "# ì¤‘ê°„ ì‹œì ì˜ ìƒíƒœ ì„ íƒ (ì˜ˆ: select_tools ì™„ë£Œ í›„)\n",
    "if len(history) > 2:\n",
    "    fork_state = history[2]  # 3ë²ˆì§¸ ìƒíƒœ\n",
    "    \n",
    "    print(f\"Fork ì‹œì : {fork_state.next}\")\n",
    "    print(f\"í•´ë‹¹ ì‹œì ì˜ config: {fork_state.config}\")\n",
    "    print()\n",
    "    \n",
    "    # í•´ë‹¹ ì‹œì ì—ì„œ ì¬ì‹¤í–‰\n",
    "    print(\"=== Fork ì§€ì ì—ì„œ ì¬ì‹¤í–‰ ===\")\n",
    "    output = graph.stream(None, fork_state.config)\n",
    "    \n",
    "    for chunk in output:\n",
    "        for node_name in chunk.keys():\n",
    "            print(f\"ë…¸ë“œ ì‹¤í–‰: {node_name}\")\n",
    "    \n",
    "    print(\"\\nâœ… Fork ì™„ë£Œ\")\n",
    "else:\n",
    "    print(\"íˆìŠ¤í† ë¦¬ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì •ë¦¬: ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬\n",
    "\n",
    "### í•µì‹¬ API\n",
    "\n",
    "```python\n",
    "# 1. í˜„ì¬ ìƒíƒœ ì¡°íšŒ\n",
    "state = graph.get_state(config)\n",
    "print(state.values)  # ìƒíƒœ ê°’\n",
    "print(state.next)    # ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ\n",
    "\n",
    "# 2. ìƒíƒœ ìˆ˜ì •\n",
    "update = {'messages': new_messages}\n",
    "graph.update_state(config, update)\n",
    "\n",
    "# 3. íˆìŠ¤í† ë¦¬ ì¡°íšŒ\n",
    "history = list(graph.get_state_history(config))\n",
    "for state in history:\n",
    "    print(state.config, state.next)\n",
    "\n",
    "# 4. Fork (ê³¼ê±° ì‹œì ì—ì„œ ì¬ì‹¤í–‰)\n",
    "output = graph.stream(None, history[2].config)\n",
    "```\n",
    "\n",
    "### ê¸°ëŠ¥ ë¹„êµ\n",
    "\n",
    "| ê¸°ëŠ¥ | ìš©ë„ | API |\n",
    "|------|------|-----|\n",
    "| **get_state** | í˜„ì¬ ìƒíƒœ í™•ì¸ | `graph.get_state(config)` |\n",
    "| **update_state** | ìƒíƒœ ìˆ˜ì • | `graph.update_state(config, update)` |\n",
    "| **get_state_history** | íˆìŠ¤í† ë¦¬ ì¡°íšŒ | `graph.get_state_history(config)` |\n",
    "| **Fork** | ê³¼ê±°ì—ì„œ ë¶„ê¸° | `graph.stream(None, past_config)` |\n",
    "\n",
    "### ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤\n",
    "\n",
    "| ì‹œë‚˜ë¦¬ì˜¤ | ë°©ë²• |\n",
    "|----------|------|\n",
    "| ì§ˆë¬¸ ìˆ˜ì • í›„ ì¬ì‹¤í–‰ | update_state â†’ stream(None) |\n",
    "| ì˜ëª»ëœ ê²°ì • ë¡¤ë°± | get_state_history â†’ Fork |\n",
    "| What-if í…ŒìŠ¤íŠ¸ | Forkë¡œ ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰ |\n",
    "| ë””ë²„ê¹… | get_state_historyë¡œ ê° ë‹¨ê³„ ê²€í†  |\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸\n",
    "embeddings = OpenAIEmbeddings()\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0.1)\n",
    "\n",
    "# ë³€ê²½\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1)\n",
    "```\n",
    "\n",
    "## ch08 ìš”ì•½: ê³ ê¸‰ ê¸°ëŠ¥\n",
    "\n",
    "| ê¸°ëŠ¥ | ìš©ë„ | í•µì‹¬ API |\n",
    "|------|------|----------|\n",
    "| **Structured Output** | êµ¬ì¡°í™”ëœ ì‘ë‹µ | `with_structured_output()` |\n",
    "| **Streaming** | ì‹¤ì‹œê°„ ì¶œë ¥ | `stream()`, `astream_events()` |\n",
    "| **Interrupt** | ì¤‘ë‹¨/ì¬ê°œ | `interrupt_before`, `stream(None)` |\n",
    "| **State Management** | ìƒíƒœ ìˆ˜ì •/Fork | `get_state()`, `update_state()` |\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "**ch10ì—ì„œëŠ”** RAGê³¼ Agentì˜ **í‰ê°€(Evaluation)** ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤."
   ]
  }
 ]
}