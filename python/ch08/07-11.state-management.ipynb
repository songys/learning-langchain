{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬: ìˆ˜ì •, ì¬ê°œ, Fork\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **ìƒíƒœ ìˆ˜ì •, ë‹¤ì–‘í•œ ì¬ê°œ ë°©ë²•, íˆìŠ¤í† ë¦¬ì—ì„œ Fork**í•˜ëŠ” ê³ ê¸‰ ê¸°ëŠ¥ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬ ê¸°ëŠ¥\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬                                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   1ï¸âƒ£ get_state: í˜„ì¬ ìƒíƒœ ì¡°íšŒ                                    â”‚\n",
    "â”‚   2ï¸âƒ£ update_state: ìƒíƒœ ìˆ˜ì •                                      â”‚\n",
    "â”‚   3ï¸âƒ£ get_state_history: ê³¼ê±° ìƒíƒœ íˆìŠ¤í† ë¦¬ ì¡°íšŒ                   â”‚\n",
    "â”‚   4ï¸âƒ£ Fork: ê³¼ê±° ì‹œì ì—ì„œ ë¶„ê¸°í•˜ì—¬ ìƒˆë¡œ ì‹¤í–‰                       â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â”‚   í™œìš©:                                                            â”‚\n",
    "â”‚   â€¢ ì¤‘ë‹¨ ì‹œì ì—ì„œ ìƒíƒœ ìˆ˜ì • í›„ ì¬ê°œ                                â”‚\n",
    "â”‚   â€¢ ì˜ëª»ëœ ê²°ì • ì‹œ ê³¼ê±°ë¡œ ëŒì•„ê°€ê¸°                                 â”‚\n",
    "â”‚   â€¢ What-if ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸                                        â”‚\n",
    "â”‚                                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-ollama langgraph duckduckgo-search langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull llama3.2\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(query: str) -> str:\n",
    "    '''ê³„ì‚°ê¸°. ìˆ˜ì‹ë§Œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.'''\n",
    "    return ast.literal_eval(query)\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search, calculator]\n",
    "\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1)\n",
    "\n",
    "tools_retriever = InMemoryVectorStore.from_documents(\n",
    "    [Document(tool.description, metadata={'name': tool.name}) for tool in tools],\n",
    "    embeddings,\n",
    ").as_retriever()\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    selected_tools: list[str]\n",
    "\n",
    "\n",
    "def model_node(state: State) -> State:\n",
    "    selected_tools = [tool for tool in tools if tool.name in state['selected_tools']]\n",
    "    res = model.bind_tools(selected_tools).invoke(state['messages'])\n",
    "    return {'messages': res}\n",
    "\n",
    "\n",
    "def select_tools(state: State) -> State:\n",
    "    query = state['messages'][-1].content\n",
    "    tool_docs = tools_retriever.invoke(query)\n",
    "    return {'selected_tools': [doc.metadata['name'] for doc in tool_docs]}\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node('select_tools', select_tools)\n",
    "builder.add_node('model', model_node)\n",
    "builder.add_node('tools', ToolNode(tools))\n",
    "builder.add_edge(START, 'select_tools')\n",
    "builder.add_edge('select_tools', 'model')\n",
    "builder.add_conditional_edges('model', tools_condition)\n",
    "builder.add_edge('tools', 'model')\n",
    "\n",
    "graph = builder.compile(checkpointer=MemorySaver())\n",
    "\n",
    "print(\"âœ… ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ìƒíƒœ ìˆ˜ì • í›„ ì¬ê°œ (update_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    'messages': [\n",
    "        HumanMessage(\n",
    "            'ë¯¸êµ­ ì œ30ëŒ€ ëŒ€í†µë ¹ì˜ ì‚¬ë§ ë‹¹ì‹œ ë‚˜ì´ëŠ” ëª‡ ì‚´ì´ì—ˆë‚˜ìš”?'\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "config = {'configurable': {'thread_id': 'edit-state-1'}}\n",
    "\n",
    "print(\"=== 1ë‹¨ê³„: ì¤‘ë‹¨ ===\")\n",
    "output = graph.stream(input_data, config, interrupt_before=[\"model\"])\n",
    "for chunk in output:\n",
    "    for node_name, node_output in chunk.items():\n",
    "        print(f\"ë…¸ë“œ: {node_name}\")\n",
    "print(\"ğŸ›‘ model ì „ì— ì¤‘ë‹¨ë¨\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜„ì¬ ìƒíƒœ í™•ì¸\n",
    "state = graph.get_state(config)\n",
    "\n",
    "print(\"=== 2ë‹¨ê³„: í˜„ì¬ ìƒíƒœ í™•ì¸ ===\")\n",
    "print(f\"í˜„ì¬ ì§ˆë¬¸: {state.values['messages'][0].content}\")\n",
    "print(f\"ì„ íƒëœ ë„êµ¬: {state.values.get('selected_tools', [])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ ìˆ˜ì •: ì§ˆë¬¸ ë³€ê²½\n",
    "print(\"=== 3ë‹¨ê³„: ìƒíƒœ ìˆ˜ì • ===\")\n",
    "\n",
    "# ë©”ì‹œì§€ ë³µì‚¬ í›„ ì²« ë²ˆì§¸ ë©”ì‹œì§€ ë³€ê²½\n",
    "updated_messages = state.values['messages'].copy()\n",
    "updated_messages[0] = HumanMessage(content=\"ì˜í™” ë³´ì´í›„ë“œì˜ ì´¬ì˜ ê¸°ê°„ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?\")\n",
    "\n",
    "# ìƒíƒœ ì—…ë°ì´íŠ¸ ì ìš©\n",
    "update = {'messages': updated_messages}\n",
    "graph.update_state(config, update)\n",
    "\n",
    "# ìˆ˜ì •ëœ ìƒíƒœ í™•ì¸\n",
    "new_state = graph.get_state(config)\n",
    "print(f\"ë³€ê²½ëœ ì§ˆë¬¸: {new_state.values['messages'][0].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ì •ëœ ìƒíƒœë¡œ ì¬ê°œ\n",
    "print(\"=== 4ë‹¨ê³„: ìˆ˜ì •ëœ ìƒíƒœë¡œ ì¬ê°œ ===\")\n",
    "\n",
    "output = graph.stream(None, config)\n",
    "\n",
    "for chunk in output:\n",
    "    for node_name, node_output in chunk.items():\n",
    "        print(f\"ë…¸ë“œ: {node_name}\")\n",
    "        if 'messages' in node_output:\n",
    "            last_msg = node_output['messages'][-1] if isinstance(node_output['messages'], list) else node_output['messages']\n",
    "            if hasattr(last_msg, 'content') and last_msg.content:\n",
    "                print(f\"  ì‘ë‹µ: {str(last_msg.content)[:200]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… ì™„ë£Œ (ë³€ê²½ëœ ì§ˆë¬¸ìœ¼ë¡œ ì‹¤í–‰ë¨)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. íˆìŠ¤í† ë¦¬ ì¡°íšŒ (get_state_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆ ìŠ¤ë ˆë“œë¡œ ì „ì²´ ì‹¤í–‰\n",
    "config2 = {'configurable': {'thread_id': 'history-1'}}\n",
    "\n",
    "print(\"=== ì „ì²´ ì‹¤í–‰ ===\")\n",
    "output = graph.stream(input_data, config2)\n",
    "for chunk in output:\n",
    "    for node_name in chunk.keys():\n",
    "        print(f\"ë…¸ë“œ ì‹¤í–‰: {node_name}\")\n",
    "\n",
    "print(\"\\nâœ… ì‹¤í–‰ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íˆìŠ¤í† ë¦¬ ì¡°íšŒ\n",
    "print(\"=== ìƒíƒœ íˆìŠ¤í† ë¦¬ ===\")\n",
    "\n",
    "history = list(graph.get_state_history(config2))\n",
    "\n",
    "print(f\"ì´ {len(history)}ê°œì˜ ìƒíƒœ ê¸°ë¡\\n\")\n",
    "\n",
    "for i, state in enumerate(history):\n",
    "    print(f\"[{i}] ì²´í¬í¬ì¸íŠ¸: {state.config['configurable'].get('checkpoint_id', 'N/A')[:20]}...\")\n",
    "    print(f\"    ë‹¤ìŒ ë…¸ë“œ: {state.next}\")\n",
    "    print(f\"    ë©”ì‹œì§€ ìˆ˜: {len(state.values.get('messages', []))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. íˆìŠ¤í† ë¦¬ì—ì„œ Fork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³¼ê±° ì‹œì ì—ì„œ ë‹¤ì‹œ ì‹¤í–‰ (Fork)\n",
    "print(\"=== íˆìŠ¤í† ë¦¬ì—ì„œ Fork ===\")\n",
    "\n",
    "# ì¤‘ê°„ ì‹œì ì˜ ìƒíƒœ ì„ íƒ (ì˜ˆ: select_tools ì™„ë£Œ í›„)\n",
    "if len(history) > 2:\n",
    "    fork_state = history[2]  # 3ë²ˆì§¸ ìƒíƒœ\n",
    "    \n",
    "    print(f\"Fork ì‹œì : {fork_state.next}\")\n",
    "    print(f\"í•´ë‹¹ ì‹œì ì˜ config: {fork_state.config}\")\n",
    "    print()\n",
    "    \n",
    "    # í•´ë‹¹ ì‹œì ì—ì„œ ì¬ì‹¤í–‰\n",
    "    print(\"=== Fork ì§€ì ì—ì„œ ì¬ì‹¤í–‰ ===\")\n",
    "    output = graph.stream(None, fork_state.config)\n",
    "    \n",
    "    for chunk in output:\n",
    "        for node_name in chunk.keys():\n",
    "            print(f\"ë…¸ë“œ ì‹¤í–‰: {node_name}\")\n",
    "    \n",
    "    print(\"\\nâœ… Fork ì™„ë£Œ\")\n",
    "else:\n",
    "    print(\"íˆìŠ¤í† ë¦¬ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì •ë¦¬: ê³ ê¸‰ ìƒíƒœ ê´€ë¦¬\n",
    "\n",
    "### í•µì‹¬ API\n",
    "\n",
    "```python\n",
    "# 1. í˜„ì¬ ìƒíƒœ ì¡°íšŒ\n",
    "state = graph.get_state(config)\n",
    "print(state.values)  # ìƒíƒœ ê°’\n",
    "print(state.next)    # ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ\n",
    "\n",
    "# 2. ìƒíƒœ ìˆ˜ì •\n",
    "update = {'messages': new_messages}\n",
    "graph.update_state(config, update)\n",
    "\n",
    "# 3. íˆìŠ¤í† ë¦¬ ì¡°íšŒ\n",
    "history = list(graph.get_state_history(config))\n",
    "for state in history:\n",
    "    print(state.config, state.next)\n",
    "\n",
    "# 4. Fork (ê³¼ê±° ì‹œì ì—ì„œ ì¬ì‹¤í–‰)\n",
    "output = graph.stream(None, history[2].config)\n",
    "```\n",
    "\n",
    "### ê¸°ëŠ¥ ë¹„êµ\n",
    "\n",
    "| ê¸°ëŠ¥ | ìš©ë„ | API |\n",
    "|------|------|-----|\n",
    "| **get_state** | í˜„ì¬ ìƒíƒœ í™•ì¸ | `graph.get_state(config)` |\n",
    "| **update_state** | ìƒíƒœ ìˆ˜ì • | `graph.update_state(config, update)` |\n",
    "| **get_state_history** | íˆìŠ¤í† ë¦¬ ì¡°íšŒ | `graph.get_state_history(config)` |\n",
    "| **Fork** | ê³¼ê±°ì—ì„œ ë¶„ê¸° | `graph.stream(None, past_config)` |\n",
    "\n",
    "### ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤\n",
    "\n",
    "| ì‹œë‚˜ë¦¬ì˜¤ | ë°©ë²• |\n",
    "|----------|------|\n",
    "| ì§ˆë¬¸ ìˆ˜ì • í›„ ì¬ì‹¤í–‰ | update_state â†’ stream(None) |\n",
    "| ì˜ëª»ëœ ê²°ì • ë¡¤ë°± | get_state_history â†’ Fork |\n",
    "| What-if í…ŒìŠ¤íŠ¸ | Forkë¡œ ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰ |\n",
    "| ë””ë²„ê¹… | get_state_historyë¡œ ê° ë‹¨ê³„ ê²€í†  |\n",
    "\n",
    "## ì½”ë“œ ë³€ê²½ì  (OpenAI â†’ Ollama)\n",
    "\n",
    "```python\n",
    "# ì›ë³¸\n",
    "embeddings = OpenAIEmbeddings()\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0.1)\n",
    "\n",
    "# ë³€ê²½\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "model = ChatOllama(model='llama3.2', temperature=0.1)\n",
    "```\n",
    "\n",
    "## ch08 ìš”ì•½: ê³ ê¸‰ ê¸°ëŠ¥\n",
    "\n",
    "| ê¸°ëŠ¥ | ìš©ë„ | í•µì‹¬ API |\n",
    "|------|------|----------|\n",
    "| **Structured Output** | êµ¬ì¡°í™”ëœ ì‘ë‹µ | `with_structured_output()` |\n",
    "| **Streaming** | ì‹¤ì‹œê°„ ì¶œë ¥ | `stream()`, `astream_events()` |\n",
    "| **Interrupt** | ì¤‘ë‹¨/ì¬ê°œ | `interrupt_before`, `stream(None)` |\n",
    "| **State Management** | ìƒíƒœ ìˆ˜ì •/Fork | `get_state()`, `update_state()` |\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "**ch10ì—ì„œëŠ”** RAGê³¼ Agentì˜ **í‰ê°€(Evaluation)** ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
