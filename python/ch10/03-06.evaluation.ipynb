{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith 평가: 에이전트 및 RAG 시스템 평가\n",
    "\n",
    "이 노트북에서는 **LangSmith를 사용한 체계적인 평가 방법**을 개념적으로 배웁니다.\n",
    "\n",
    "## 왜 체계적인 평가가 필요할까요?\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                    LLM 애플리케이션 평가의 어려움                    │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│   전통적인 소프트웨어:                                              │\n",
    "│   입력 → 함수 → 출력                                               │\n",
    "│   결과가 결정적 (항상 같은 출력)                                    │\n",
    "│   테스트: assert output == expected                                │\n",
    "│                                                                    │\n",
    "│   LLM 애플리케이션:                                                 │\n",
    "│   입력 → LLM → 출력                                                │\n",
    "│   결과가 비결정적 (매번 다른 출력)                                  │\n",
    "│   \"정답\"이 명확하지 않음                                           │\n",
    "│                                                                    │\n",
    "│   해결: LLM을 평가자로 사용! (LLM-as-Judge)                        │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## LangSmith 평가 아키텍처\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                    LangSmith 평가 흐름                              │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│   1️⃣ 데이터셋 생성                                                 │\n",
    "│   ┌─────────────────────────────────────────┐                      │\n",
    "│   │ Input (질문)        │ Output (정답)     │                      │\n",
    "│   ├─────────────────────┼───────────────────┤                      │\n",
    "│   │ \"가장 많이 판매된..\" │ \"Hot Girl\"        │                      │\n",
    "│   │ \"Led Zeppelin...\"   │ \"14개의 앨범\"     │                      │\n",
    "│   └─────────────────────┴───────────────────┘                      │\n",
    "│                                                                    │\n",
    "│   2️⃣ 예측 함수 정의                                                │\n",
    "│   def predict(example):                                            │\n",
    "│       return agent.invoke(example[\"input\"])                        │\n",
    "│                                                                    │\n",
    "│   3️⃣ 평가자 정의                                                   │\n",
    "│   def evaluator(run, example):                                     │\n",
    "│       return {\"score\": compare(run.output, example.output)}        │\n",
    "│                                                                    │\n",
    "│   4️⃣ 평가 실행                                                     │\n",
    "│   evaluate(predict, data=dataset, evaluators=[evaluator])          │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. 평가 유형\n",
    "\n",
    "## 1.1 답변 정확도 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의사 코드 - LangSmith 연동 필요\n",
    "\n",
    "# 답변 정확도 평가자\n",
    "def answer_evaluator(run, example):\n",
    "    \"\"\"\n",
    "    RAG 답변 정확도 평가\n",
    "    \n",
    "    LLM을 사용하여 정답과 예측 답변 비교\n",
    "    \"\"\"\n",
    "    # 입력, 정답, 예측값\n",
    "    input_question = example.inputs[\"input\"]\n",
    "    reference = example.outputs[\"output\"]  # 정답\n",
    "    prediction = run.outputs[\"response\"]   # 모델 응답\n",
    "    \n",
    "    # LLM 평가자\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    \n",
    "    # 평가 프롬프트\n",
    "    eval_prompt = f\"\"\"\n",
    "    질문: {input_question}\n",
    "    정답: {reference}\n",
    "    학생 답변: {prediction}\n",
    "    \n",
    "    학생의 답변이 정답과 의미적으로 일치하면 1, 아니면 0을 반환하세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    score = llm.invoke(eval_prompt)\n",
    "    \n",
    "    return {\"key\": \"answer_accuracy\", \"score\": int(score)}\n",
    "\n",
    "print(\"✅ 답변 정확도 평가자 개념 설명 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 도구 호출 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의사 코드 - 도구 호출 평가\n",
    "\n",
    "def check_specific_tool_call(run, example):\n",
    "    \"\"\"\n",
    "    첫 번째 도구 호출이 예상대로인지 확인\n",
    "    \"\"\"\n",
    "    expected_tool = 'sql_db_list_tables'\n",
    "    \n",
    "    response = run.outputs[\"response\"]\n",
    "    \n",
    "    # 도구 호출 가져오기\n",
    "    try:\n",
    "        tool_call = response.tool_calls[0]['name']\n",
    "    except:\n",
    "        tool_call = None\n",
    "    \n",
    "    score = 1 if tool_call == expected_tool else 0\n",
    "    \n",
    "    return {\"key\": \"first_tool_call\", \"score\": score}\n",
    "\n",
    "print(\"✅ 도구 호출 평가자 개념 설명 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 에이전트 경로(Trajectory) 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의사 코드 - 에이전트 경로 평가\n",
    "\n",
    "def contains_all_tool_calls_in_order(run, example):\n",
    "    \"\"\"\n",
    "    에이전트가 예상된 도구들을 올바른 순서로 호출했는지 확인\n",
    "    \"\"\"\n",
    "    expected = [\n",
    "        'sql_db_list_tables',  # 1. 테이블 목록 조회\n",
    "        'sql_db_schema',       # 2. 스키마 조회\n",
    "        'sql_db_query_checker', # 3. 쿼리 검증\n",
    "        'sql_db_query',        # 4. 쿼리 실행\n",
    "        'check_result'         # 5. 결과 확인\n",
    "    ]\n",
    "    \n",
    "    messages = run.outputs[\"response\"]\n",
    "    \n",
    "    # 실제 도구 호출 추출\n",
    "    tool_calls = [\n",
    "        tc['name'] for m in messages \n",
    "        for tc in getattr(m, 'tool_calls', [])\n",
    "    ]\n",
    "    \n",
    "    # 순서대로 포함되어 있는지 확인\n",
    "    it = iter(tool_calls)\n",
    "    score = 1 if all(elem in it for elem in expected) else 0\n",
    "    \n",
    "    return {\"key\": \"trajectory_in_order\", \"score\": score}\n",
    "\n",
    "print(\"✅ 에이전트 경로 평가자 개념 설명 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 평가 데이터셋 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 에이전트 평가용 데이터셋 예시\n",
    "\n",
    "sql_evaluation_examples = [\n",
    "    {\n",
    "        \"input\": \"어느 나라의 고객이 가장 많이 지출했나요? 그리고 얼마를 지출했나요?\",\n",
    "        \"output\": \"가장 많이 지출한 나라는 미국으로, 총 지출액은 $523.06입니다\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"2013년에 가장 많이 판매된 트랙은 무엇인가요?\",\n",
    "        \"output\": \"2013년에 가장 많이 판매된 트랙은 Hot Girl입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Led Zeppelin 아티스트는 몇 개의 앨범을 발매했나요?\",\n",
    "        \"output\": \"Led Zeppelin은 14개의 앨범을 발매했습니다\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"'Big Ones' 앨범의 총 가격은 얼마인가요?\",\n",
    "        \"output\": \"'Big Ones' 앨범의 총 가격은 14.85입니다\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"2009년에 어떤 영업 담당자가 가장 많은 매출을 올렸나요?\",\n",
    "        \"output\": \"Steve Johnson이 2009년에 가장 많은 매출을 올렸습니다\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"=== SQL 에이전트 평가 데이터셋 ===\")\n",
    "for i, ex in enumerate(sql_evaluation_examples):\n",
    "    print(f\"\\n[예제 {i+1}]\")\n",
    "    print(f\"  질문: {ex['input']}\")\n",
    "    print(f\"  정답: {ex['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 평가 실행 (개념)\n",
    "\n",
    "```python\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# 1. LangSmith 클라이언트 생성\n",
    "client = Client()\n",
    "\n",
    "# 2. 데이터셋 생성\n",
    "dataset = client.create_dataset(\"sql-agent-eval\")\n",
    "client.create_examples(\n",
    "    inputs=[{\"input\": ex[\"input\"]} for ex in examples],\n",
    "    outputs=[{\"output\": ex[\"output\"]} for ex in examples],\n",
    "    dataset_id=dataset.id\n",
    ")\n",
    "\n",
    "# 3. 예측 함수\n",
    "def predict(example):\n",
    "    result = agent.invoke({\"messages\": [(\"user\", example[\"input\"])]})\n",
    "    return {\"response\": result[\"messages\"][-1].content}\n",
    "\n",
    "# 4. 평가 실행\n",
    "results = evaluate(\n",
    "    predict,\n",
    "    data=\"sql-agent-eval\",\n",
    "    evaluators=[answer_evaluator, trajectory_evaluator],\n",
    "    num_repetitions=3,  # 3번 반복 실행\n",
    "    experiment_prefix=\"sql-agent-v1\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 정리: LangSmith 평가\n",
    "\n",
    "### 평가 유형\n",
    "\n",
    "| 평가 유형 | 측정 대상 | 평가 방법 |\n",
    "|----------|----------|----------|\n",
    "| **답변 정확도** | 최종 응답 | LLM-as-Judge |\n",
    "| **도구 호출** | 올바른 도구 선택 | 직접 비교 |\n",
    "| **에이전트 경로** | 도구 호출 순서 | 순서 비교 |\n",
    "\n",
    "### 핵심 API\n",
    "\n",
    "```python\n",
    "# LangSmith 평가\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "results = evaluate(\n",
    "    predict_fn,           # 예측 함수\n",
    "    data=dataset_name,    # 데이터셋\n",
    "    evaluators=[...],     # 평가자 목록\n",
    "    num_repetitions=3,    # 반복 횟수\n",
    ")\n",
    "```\n",
    "\n",
    "### 평가자 구조\n",
    "\n",
    "```python\n",
    "def evaluator(run, example) -> dict:\n",
    "    # run.outputs: 모델 출력\n",
    "    # example.inputs: 입력\n",
    "    # example.outputs: 정답\n",
    "    \n",
    "    score = compute_score(...)\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"metric_name\",\n",
    "        \"score\": score  # 0 또는 1\n",
    "    }\n",
    "```\n",
    "\n",
    "## ch10 요약: 평가\n",
    "\n",
    "| 기능 | 용도 |\n",
    "|------|------|\n",
    "| **검색 관련성 평가** | RAG 문서 필터링 |\n",
    "| **답변 정확도** | 최종 응답 품질 |\n",
    "| **도구 호출 평가** | 에이전트 동작 검증 |\n",
    "| **경로 평가** | 에이전트 추론 과정 검증 |\n",
    "\n",
    "## 전체 커리큘럼 요약\n",
    "\n",
    "| 챕터 | 주제 |\n",
    "|------|------|\n",
    "| ch01 | LangChain 기초, 프롬프트, 체인 |\n",
    "| ch02 | RAG 기초, 벡터 DB |\n",
    "| ch03 | 고급 RAG, Agentic RAG |\n",
    "| ch04 | 메모리 관리 |\n",
    "| ch05 | LangGraph 기본 챗봇 |\n",
    "| ch06 | 에이전트와 도구 |\n",
    "| ch07 | 고급 패턴 (Reflection, Subgraph, Supervisor) |\n",
    "| ch08 | 고급 기능 (Streaming, Interrupt, State) |\n",
    "| ch09 | 배포 |\n",
    "| ch10 | 평가 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
