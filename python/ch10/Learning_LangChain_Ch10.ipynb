{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10장. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community langchain-openai langgraph langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\nimport os\n\nos.environ['OPENAI_API_KEY']=userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 코드 10-1~10-2 검색 및 관련성 평가 (Retrieve & Grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검색 및 관련성 평가 (Retrieve & Grade)\n",
    "\n",
    "이 노트북에서는 **RAG 시스템에서 검색 결과의 관련성을 평가**하는 방법을 배웁니다.\n",
    "\n",
    "## RAG 평가의 중요성\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                    RAG 평가가 필요한 이유                           │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│   RAG 시스템:                                                      │\n",
    "│   질문 → 검색 → 문서들 → LLM → 답변                               │\n",
    "│                                                                    │\n",
    "│   문제:                                                            │\n",
    "│   검색된 문서가 질문과 관련 없으면?                                │\n",
    "│   → LLM이 잘못된 정보로 답변                                       │\n",
    "│   → 환각(Hallucination) 발생                                       │\n",
    "│                                                                    │\n",
    "│   해결:                                                            │\n",
    "│   검색 결과의 관련성을 LLM으로 평가!                               │\n",
    "│   → 관련 없는 문서 필터링                                          │\n",
    "│   → 필요시 재검색 또는 웹 검색                                     │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## 아키텍처\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                    검색 + 평가 흐름                                 │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│   질문: \"2024년 LangGraph 에이전트 사례는?\"                        │\n",
    "│     │                                                              │\n",
    "│     ▼                                                              │\n",
    "│   ┌──────────────┐                                                 │\n",
    "│   │   Retriever  │  → 문서 4개 검색                                │\n",
    "│   └──────┬───────┘                                                 │\n",
    "│          │                                                         │\n",
    "│          ▼                                                         │\n",
    "│   ┌──────────────┐                                                 │\n",
    "│   │    Grader    │  → 각 문서 관련성 평가 (yes/no)                │\n",
    "│   │ (LLM 평가기) │                                                 │\n",
    "│   └──────┬───────┘                                                 │\n",
    "│          │                                                         │\n",
    "│   관련 문서만 선택                                                  │\n",
    "│          │                                                         │\n",
    "│          ▼                                                         │\n",
    "│   ┌──────────────┐                                                 │\n",
    "│   │   Generator  │  → 관련 문서로 답변 생성                        │\n",
    "│   └──────────────┘                                                 │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "!apt-get install -y zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "subprocess.Popen(['ollama', 'serve'])\n",
    "time.sleep(3)\n",
    "\n",
    "!ollama pull llama3.2\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 문서 로드 및 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import InMemoryVectorStore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# 블로그 게시물 URL\n",
    "urls = [\n",
    "    \"https://blog.langchain.dev/top-5-langgraph-agents-in-production-2024/\",\n",
    "    \"https://blog.langchain.dev/langchain-state-of-ai-2024/\",\n",
    "    \"https://blog.langchain.dev/introducing-ambient-agents/\",\n",
    "]\n",
    "\n",
    "# 문서 로드\n",
    "print(\"=== 문서 로드 중 ===\")\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "print(f\"로드된 문서: {len(docs_list)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "print(f\"분할된 청크: {len(doc_splits)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 스토어 생성\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"✅ 벡터 스토어 및 Retriever 생성 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 검색 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 테스트\n",
    "question = \"2024년에 프로덕션 환경에서 사용된 LangGraph 에이전트 2개는 무엇인가요?\"\n",
    "\n",
    "results = retriever.invoke(question)\n",
    "\n",
    "print(f\"=== 검색 결과 ({len(results)}개) ===\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n[문서 {i+1}]\")\n",
    "    print(f\"내용: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 관련성 평가 스키마 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"검색된 문서의 관련성 평가 결과\"\"\"\n",
    "    \n",
    "    binary_score: str = Field(\n",
    "        description=\"문서가 질문과 관련이 있으면 'yes', 없으면 'no'\"\n",
    "    )\n",
    "\n",
    "print(\"✅ GradeDocuments 스키마 정의 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 관련성 평가기 (Grader) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# LLM 설정 (Structured Output)\n",
    "llm = ChatOllama(model='llama3.2', temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# 평가 프롬프트\n",
    "system = \"\"\"당신은 사용자 질문에 대한 검색된 문서의 관련성을 평가하는 채점자입니다.\n",
    "문서에 질문과 관련된 키워드나 의미가 포함되어 있다면 관련성이 있다고 평가하세요.\n",
    "문서가 질문과 관련이 있는지 여부를 나타내는 'yes' 또는 'no'로 평가해 주세요.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"검색된 문서: \\n\\n {document} \\n\\n 사용자 질문: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 평가 체인\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "print(\"✅ 관련성 평가기 생성 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 검색 결과 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서 평가\n",
    "print(\"=== 관련성 평가 ===\")\n",
    "print(f\"질문: {question}\\n\")\n",
    "\n",
    "relevant_docs = []\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    result = retrieval_grader.invoke({\n",
    "        \"question\": question, \n",
    "        \"document\": doc.page_content\n",
    "    })\n",
    "    \n",
    "    status = \"✅ 관련\" if result.binary_score == 'yes' else \"❌ 무관\"\n",
    "    print(f\"[문서 {i+1}] {status}\")\n",
    "    print(f\"  내용: {doc.page_content[:100]}...\")\n",
    "    print()\n",
    "    \n",
    "    if result.binary_score == 'yes':\n",
    "        relevant_docs.append(doc)\n",
    "\n",
    "print(f\"\\n관련 문서: {len(relevant_docs)}개 / 전체: {len(results)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 관련 문서로 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 답변 생성 프롬프트\n",
    "answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 질문에 답변하는 어시스턴트입니다. 주어진 문서를 바탕으로 답변하세요.\"),\n",
    "    (\"human\", \"문서:\\n{context}\\n\\n질문: {question}\")\n",
    "])\n",
    "\n",
    "# 컨텍스트 생성\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "# 답변 생성\n",
    "answer_chain = answer_prompt | llm\n",
    "answer = answer_chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "print(\"=== 최종 답변 ===\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 정리: 검색 관련성 평가\n",
    "\n",
    "### 핵심 코드\n",
    "\n",
    "```python\n",
    "# 1. 평가 스키마\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(description=\"'yes' 또는 'no'\")\n",
    "\n",
    "# 2. Structured Output LLM\n",
    "grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# 3. 평가 체인\n",
    "retrieval_grader = grade_prompt | grader\n",
    "\n",
    "# 4. 각 문서 평가\n",
    "for doc in docs:\n",
    "    result = retrieval_grader.invoke({\"question\": q, \"document\": doc})\n",
    "    if result.binary_score == 'yes':\n",
    "        relevant_docs.append(doc)\n",
    "```\n",
    "\n",
    "### 평가 활용\n",
    "\n",
    "| 평가 결과 | 다음 행동 |\n",
    "|----------|----------|\n",
    "| 관련 문서 있음 | 답변 생성 |\n",
    "| 관련 문서 없음 | 재검색 또는 웹 검색 |\n",
    "\n",
    "## 코드 변경점 (OpenAI → Ollama)\n",
    "\n",
    "```python\n",
    "# 원본\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# 변경\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "llm = ChatOllama(model='llama3.2', temperature=0)\n",
    "```\n",
    "\n",
    "## 다음 단계\n",
    "\n",
    "**LangSmith**를 사용한 체계적인 평가 방법을 배웁니다. (03-06번 노트북)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 코드 10-3~10-6 LangSmith 평가: 에이전트 및 RAG 시스템 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith 평가: 에이전트 및 RAG 시스템 평가\n",
    "\n",
    "이 노트북에서는 **LangSmith를 사용한 체계적인 평가 방법**을 개념적으로 배웁니다.\n",
    "\n",
    "## 왜 체계적인 평가가 필요할까요?\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                    LLM 애플리케이션 평가의 어려움                    │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│   전통적인 소프트웨어:                                              │\n",
    "│   입력 → 함수 → 출력                                               │\n",
    "│   결과가 결정적 (항상 같은 출력)                                    │\n",
    "│   테스트: assert output == expected                                │\n",
    "│                                                                    │\n",
    "│   LLM 애플리케이션:                                                 │\n",
    "│   입력 → LLM → 출력                                                │\n",
    "│   결과가 비결정적 (매번 다른 출력)                                  │\n",
    "│   \"정답\"이 명확하지 않음                                           │\n",
    "│                                                                    │\n",
    "│   해결: LLM을 평가자로 사용! (LLM-as-Judge)                        │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## LangSmith 평가 아키텍처\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                    LangSmith 평가 흐름                              │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                    │\n",
    "│   1️⃣ 데이터셋 생성                                                 │\n",
    "│   ┌─────────────────────────────────────────┐                      │\n",
    "│   │ Input (질문)        │ Output (정답)     │                      │\n",
    "│   ├─────────────────────┼───────────────────┤                      │\n",
    "│   │ \"가장 많이 판매된..\" │ \"Hot Girl\"        │                      │\n",
    "│   │ \"Led Zeppelin...\"   │ \"14개의 앨범\"     │                      │\n",
    "│   └─────────────────────┴───────────────────┘                      │\n",
    "│                                                                    │\n",
    "│   2️⃣ 예측 함수 정의                                                │\n",
    "│   def predict(example):                                            │\n",
    "│       return agent.invoke(example[\"input\"])                        │\n",
    "│                                                                    │\n",
    "│   3️⃣ 평가자 정의                                                   │\n",
    "│   def evaluator(run, example):                                     │\n",
    "│       return {\"score\": compare(run.output, example.output)}        │\n",
    "│                                                                    │\n",
    "│   4️⃣ 평가 실행                                                     │\n",
    "│   evaluate(predict, data=dataset, evaluators=[evaluator])          │\n",
    "│                                                                    │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1. 평가 유형\n",
    "\n",
    "## 1.1 답변 정확도 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의사 코드 - LangSmith 연동 필요\n",
    "\n",
    "# 답변 정확도 평가자\n",
    "def answer_evaluator(run, example):\n",
    "    \"\"\"\n",
    "    RAG 답변 정확도 평가\n",
    "    \n",
    "    LLM을 사용하여 정답과 예측 답변 비교\n",
    "    \"\"\"\n",
    "    # 입력, 정답, 예측값\n",
    "    input_question = example.inputs[\"input\"]\n",
    "    reference = example.outputs[\"output\"]  # 정답\n",
    "    prediction = run.outputs[\"response\"]   # 모델 응답\n",
    "    \n",
    "    # LLM 평가자\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    \n",
    "    # 평가 프롬프트\n",
    "    eval_prompt = f\"\"\"\n",
    "    질문: {input_question}\n",
    "    정답: {reference}\n",
    "    학생 답변: {prediction}\n",
    "    \n",
    "    학생의 답변이 정답과 의미적으로 일치하면 1, 아니면 0을 반환하세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    score = llm.invoke(eval_prompt)\n",
    "    \n",
    "    return {\"key\": \"answer_accuracy\", \"score\": int(score)}\n",
    "\n",
    "print(\"✅ 답변 정확도 평가자 개념 설명 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 도구 호출 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의사 코드 - 도구 호출 평가\n",
    "\n",
    "def check_specific_tool_call(run, example):\n",
    "    \"\"\"\n",
    "    첫 번째 도구 호출이 예상대로인지 확인\n",
    "    \"\"\"\n",
    "    expected_tool = 'sql_db_list_tables'\n",
    "    \n",
    "    response = run.outputs[\"response\"]\n",
    "    \n",
    "    # 도구 호출 가져오기\n",
    "    try:\n",
    "        tool_call = response.tool_calls[0]['name']\n",
    "    except:\n",
    "        tool_call = None\n",
    "    \n",
    "    score = 1 if tool_call == expected_tool else 0\n",
    "    \n",
    "    return {\"key\": \"first_tool_call\", \"score\": score}\n",
    "\n",
    "print(\"✅ 도구 호출 평가자 개념 설명 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 에이전트 경로(Trajectory) 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의사 코드 - 에이전트 경로 평가\n",
    "\n",
    "def contains_all_tool_calls_in_order(run, example):\n",
    "    \"\"\"\n",
    "    에이전트가 예상된 도구들을 올바른 순서로 호출했는지 확인\n",
    "    \"\"\"\n",
    "    expected = [\n",
    "        'sql_db_list_tables',  # 1. 테이블 목록 조회\n",
    "        'sql_db_schema',       # 2. 스키마 조회\n",
    "        'sql_db_query_checker', # 3. 쿼리 검증\n",
    "        'sql_db_query',        # 4. 쿼리 실행\n",
    "        'check_result'         # 5. 결과 확인\n",
    "    ]\n",
    "    \n",
    "    messages = run.outputs[\"response\"]\n",
    "    \n",
    "    # 실제 도구 호출 추출\n",
    "    tool_calls = [\n",
    "        tc['name'] for m in messages \n",
    "        for tc in getattr(m, 'tool_calls', [])\n",
    "    ]\n",
    "    \n",
    "    # 순서대로 포함되어 있는지 확인\n",
    "    it = iter(tool_calls)\n",
    "    score = 1 if all(elem in it for elem in expected) else 0\n",
    "    \n",
    "    return {\"key\": \"trajectory_in_order\", \"score\": score}\n",
    "\n",
    "print(\"✅ 에이전트 경로 평가자 개념 설명 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 평가 데이터셋 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 에이전트 평가용 데이터셋 예시\n",
    "\n",
    "sql_evaluation_examples = [\n",
    "    {\n",
    "        \"input\": \"어느 나라의 고객이 가장 많이 지출했나요? 그리고 얼마를 지출했나요?\",\n",
    "        \"output\": \"가장 많이 지출한 나라는 미국으로, 총 지출액은 $523.06입니다\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"2013년에 가장 많이 판매된 트랙은 무엇인가요?\",\n",
    "        \"output\": \"2013년에 가장 많이 판매된 트랙은 Hot Girl입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Led Zeppelin 아티스트는 몇 개의 앨범을 발매했나요?\",\n",
    "        \"output\": \"Led Zeppelin은 14개의 앨범을 발매했습니다\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"'Big Ones' 앨범의 총 가격은 얼마인가요?\",\n",
    "        \"output\": \"'Big Ones' 앨범의 총 가격은 14.85입니다\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"2009년에 어떤 영업 담당자가 가장 많은 매출을 올렸나요?\",\n",
    "        \"output\": \"Steve Johnson이 2009년에 가장 많은 매출을 올렸습니다\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"=== SQL 에이전트 평가 데이터셋 ===\")\n",
    "for i, ex in enumerate(sql_evaluation_examples):\n",
    "    print(f\"\\n[예제 {i+1}]\")\n",
    "    print(f\"  질문: {ex['input']}\")\n",
    "    print(f\"  정답: {ex['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 평가 실행 (개념)\n",
    "\n",
    "```python\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# 1. LangSmith 클라이언트 생성\n",
    "client = Client()\n",
    "\n",
    "# 2. 데이터셋 생성\n",
    "dataset = client.create_dataset(\"sql-agent-eval\")\n",
    "client.create_examples(\n",
    "    inputs=[{\"input\": ex[\"input\"]} for ex in examples],\n",
    "    outputs=[{\"output\": ex[\"output\"]} for ex in examples],\n",
    "    dataset_id=dataset.id\n",
    ")\n",
    "\n",
    "# 3. 예측 함수\n",
    "def predict(example):\n",
    "    result = agent.invoke({\"messages\": [(\"user\", example[\"input\"])]})\n",
    "    return {\"response\": result[\"messages\"][-1].content}\n",
    "\n",
    "# 4. 평가 실행\n",
    "results = evaluate(\n",
    "    predict,\n",
    "    data=\"sql-agent-eval\",\n",
    "    evaluators=[answer_evaluator, trajectory_evaluator],\n",
    "    num_repetitions=3,  # 3번 반복 실행\n",
    "    experiment_prefix=\"sql-agent-v1\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 정리: LangSmith 평가\n",
    "\n",
    "### 평가 유형\n",
    "\n",
    "| 평가 유형 | 측정 대상 | 평가 방법 |\n",
    "|----------|----------|----------|\n",
    "| **답변 정확도** | 최종 응답 | LLM-as-Judge |\n",
    "| **도구 호출** | 올바른 도구 선택 | 직접 비교 |\n",
    "| **에이전트 경로** | 도구 호출 순서 | 순서 비교 |\n",
    "\n",
    "### 핵심 API\n",
    "\n",
    "```python\n",
    "# LangSmith 평가\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "results = evaluate(\n",
    "    predict_fn,           # 예측 함수\n",
    "    data=dataset_name,    # 데이터셋\n",
    "    evaluators=[...],     # 평가자 목록\n",
    "    num_repetitions=3,    # 반복 횟수\n",
    ")\n",
    "```\n",
    "\n",
    "### 평가자 구조\n",
    "\n",
    "```python\n",
    "def evaluator(run, example) -> dict:\n",
    "    # run.outputs: 모델 출력\n",
    "    # example.inputs: 입력\n",
    "    # example.outputs: 정답\n",
    "    \n",
    "    score = compute_score(...)\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"metric_name\",\n",
    "        \"score\": score  # 0 또는 1\n",
    "    }\n",
    "```\n",
    "\n",
    "## ch10 요약: 평가\n",
    "\n",
    "| 기능 | 용도 |\n",
    "|------|------|\n",
    "| **검색 관련성 평가** | RAG 문서 필터링 |\n",
    "| **답변 정확도** | 최종 응답 품질 |\n",
    "| **도구 호출 평가** | 에이전트 동작 검증 |\n",
    "| **경로 평가** | 에이전트 추론 과정 검증 |\n",
    "\n",
    "## 전체 커리큘럼 요약\n",
    "\n",
    "| 챕터 | 주제 |\n",
    "|------|------|\n",
    "| ch01 | LangChain 기초, 프롬프트, 체인 |\n",
    "| ch02 | RAG 기초, 벡터 DB |\n",
    "| ch03 | 고급 RAG, Agentic RAG |\n",
    "| ch04 | 메모리 관리 |\n",
    "| ch05 | LangGraph 기본 챗봇 |\n",
    "| ch06 | 에이전트와 도구 |\n",
    "| ch07 | 고급 패턴 (Reflection, Subgraph, Supervisor) |\n",
    "| ch08 | 고급 기능 (Streaming, Interrupt, State) |\n",
    "| ch09 | 배포 |\n",
    "| ch10 | 평가 |"
   ]
  }
 ]
}